é’ˆå¯¹ä½ çš„ä»»åŠ¡ï¼ˆç”µå•†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼‰ä»¥åŠç¡¬ä»¶é™åˆ¶ï¼ˆ32GB å†…å­˜ + RTX 4070 Ti Superï¼Œ16GB æ˜¾å­˜ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ª è½»é‡ã€é«˜æ•ˆã€å¯è®­ç»ƒ/æ¨ç† çš„ T2Iï¼ˆText-to-Imageï¼‰æ¨¡å‹æ–¹æ¡ˆã€‚ç»“åˆä½ æä¾›çš„ GitHub åŸºçº¿é¡¹ç›®ï¼ˆåŸºäº BART + VQGANï¼‰å’Œå®é™…èµ„æºé™åˆ¶ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ª å¯è¡Œã€å®ç”¨ã€ä¾¿äºéƒ¨ç½²çš„ä»£ç æ–¹æ¡ˆå»ºè®®ã€‚

âœ… æ ¸å¿ƒç›®æ ‡
åœ¨ 16GB æ˜¾å­˜ ä¸Šå®Œæˆ è®­ç»ƒæˆ–å¾®è°ƒ
èƒ½å¤Ÿåœ¨ 32GB å†…å­˜ ä¸‹åŠ è½½æ•°æ®å¹¶è¿è¡Œ
æ”¯æŒ ç”µå•†å•†å“ç±»ç›®ï¼ˆæœè£…ã€é¥°å“ã€åŒ–å¦†å“ï¼‰
è¾“å‡ºå›¾åƒä¸º base64 ç¼–ç ï¼Œç¬¦åˆæäº¤æ ¼å¼

ğŸš« ä¸æ¨èæ–¹æ¡ˆï¼ˆæ˜¾å­˜/ç®—åŠ›ä¸è¶³ï¼‰
Stable Diffusion v1/v2 full fine-tuningï¼ˆ>20GB æ˜¾å­˜ï¼‰
DALLÂ·E Mini / Craiyonï¼ˆæ•ˆæœå·®ï¼Œä¸é€‚ç”¨äºå•†å“å›¾ï¼‰
M6-T / OFA ç­‰å¤§æ¨¡å‹ï¼ˆéœ€å¤šå¡/å¤§æ˜¾å­˜ï¼‰

âœ… æ¨èæ–¹æ¡ˆï¼šè½»é‡åŒ– VQGAN + Transformer Decoderï¼ˆç±»ä¼¼ baselineï¼Œä½†ä¼˜åŒ–æ˜¾å­˜ï¼‰
æ¨¡å‹ç»“æ„æ€è·¯ï¼ˆå‚è€ƒ GitHub baselineï¼Œä½†åšè£å‰ªï¼‰

ç»„ä»¶ è¯´æ˜
------ ------
æ–‡æœ¬ç¼–ç å™¨ ä½¿ç”¨ TinyBERT æˆ– DistilBERTï¼ˆä¸­æ–‡ç‰ˆï¼‰æ›¿ä»£ BARTï¼Œå‚æ•°é‡ < 50M
å›¾åƒ tokenizer ä½¿ç”¨ VQGAN (f=8, codebook=1024)ï¼Œè¾“å‡º 16x16 = 256 tokensï¼ˆ256x256 å›¾åƒï¼‰
å›¾åƒè§£ç å™¨ å›ºå®š VQGAN decoderï¼ˆä¸è®­ç»ƒï¼‰ï¼Œåªè®­ç»ƒ Transformer è§£ç å™¨ï¼ˆé¢„æµ‹å›¾åƒ tokenï¼‰
è®­ç»ƒç›®æ ‡ è‡ªå›å½’é¢„æµ‹ VQGAN token åºåˆ—ï¼ˆç±»ä¼¼ ImageGPT / DALLÂ·Eï¼‰
ğŸ’¡ è¿™ç§â€œå†»ç»“ VQGAN + è®­ç»ƒè½»é‡æ–‡æœ¬â†’token æ¨¡å‹â€çš„æ–¹å¼ï¼Œæ˜¯å½“å‰å°æ˜¾å­˜è®¾å¤‡ä¸Šæœ€å¯è¡Œçš„ T2I æ–¹æ¡ˆã€‚

ğŸ› ï¸ å…·ä½“å®ç°æ­¥éª¤ï¼ˆé€‚é… 16G æ˜¾å­˜ï¼‰
1. æ•°æ®é¢„å¤„ç†ï¼ˆCPU å‹å¥½ï¼‰
å°† T2I_train.img.tsv ä¸­çš„ base64 å›¾åƒè§£ç  â†’ resize åˆ° 256x256
ç”¨é¢„è®­ç»ƒ VQGAN ç¼–ç ä¸º 256 ä¸ªç¦»æ•£ tokenï¼ˆshape: [256]ï¼‰
æ–‡æœ¬ç”¨ Chinese DistilBERT ç¼–ç ä¸º [CLS] + tokensï¼ˆmax_len=64ï¼‰

python
ç¤ºä¾‹ï¼šVQGAN ç¼–ç å›¾åƒ
from taming.models.vqgan import VQModel
import torch

vqgan = VQModel.load_from_checkpoint("logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt")
vqgan.eval().cuda()

def encode_image_to_tokens(img_tensor): # img_tensor: [1, 3, 256, 256]
with torch.no_grad():
_, _, [_, _, indices] = vqgan.encode(img_tensor)
return indices.squeeze(0) # [256]
âš ï¸ æ³¨æ„ï¼šVQGAN æ¨¡å‹æœ¬èº«çº¦ 100MBï¼Œæ¨ç†æ—¶æ˜¾å­˜å ç”¨ < 2GBã€‚

2. æ¨¡å‹è®¾è®¡ï¼ˆè½»é‡ Transformerï¼‰

ä½¿ç”¨ å°å‹ GPT-style decoderï¼š
è¾“å…¥ï¼šæ–‡æœ¬ embeddingï¼ˆæ¥è‡ª DistilBERTï¼‰
è¾“å‡ºï¼š256 ä¸ªå›¾åƒ tokenï¼ˆæ¯ä¸ª âˆˆ [0, 1023]ï¼‰
æ¨¡å‹è§„æ¨¡ï¼š4 å±‚ï¼Œhidden=512ï¼Œhead=8ï¼Œæ€»å‚æ•° ~30M

python
class TextToImageTransformer(nn.Module):
def __init__(self, vocab_size=1024, text_dim=768, hidden=512, n_layers=4):
super().__init__()
self.text_proj = nn.Linear(text_dim, hidden)
self.token_emb = nn.Embedding(vocab_size, hidden)
self.pos_emb = nn.Embedding(256, hidden)
self.transformer = nn.TransformerDecoder(
decoder_layer=nn.TransformerDecoderLayer(d_model=hidden, nhead=8),
num_layers=n_layers,
)
self.head = nn.Linear(hidden, vocab_size)
âœ… æ­¤æ¨¡å‹è®­ç»ƒæ—¶ batch_size=8~16 åœ¨ 16G æ˜¾å­˜ä¸‹å¯è¡Œï¼ˆæ··åˆç²¾åº¦è®­ç»ƒ AMP å¯è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ï¼‰

3. è®­ç»ƒç­–ç•¥ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
ä½¿ç”¨ PyTorch AMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰
Batch size = 8ï¼ˆ256x256 å›¾åƒ + æ–‡æœ¬ï¼‰
Optimizer: AdamW (lr=5e-5, weight_decay=0.01)
æ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚æœ batch_size éœ€è¦æ›´å¤§ï¼‰
æ•°æ®åŠ è½½ç”¨ å¤šè¿›ç¨‹ DataLoaderï¼ˆnum_workers=4ï¼‰ï¼Œé¿å…å†…å­˜ç“¶é¢ˆ

python
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
logits = model(text_emb, image_tokens[:, :-1])
loss = F.cross_entropy(logits.view(-1, vocab_size), image_tokens[:, 1:].reshape(-1))
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

4. æ¨ç† & ç”Ÿæˆï¼ˆæäº¤æ ¼å¼ï¼‰
è‡ªå›å½’ç”Ÿæˆ 256 ä¸ª token
ç”¨ VQGAN decoder é‡å»ºå›¾åƒ
è½¬ä¸º base64 å¹¶å†™å…¥ T2I_test.tsv

python
def generate_image_from_text(text, model, vqgan, tokenizer):
text_ids = tokenizer(text, return_tensors="pt", max_length=64, truncation=True).input_ids.cuda()
with torch.no_grad():
text_emb = distilbert(text_ids).last_hidden_state # [1, L, 768]
image_tokens = autoregressive_generate(model, text_emb, seq_len=256) # [256]
z = vqgan.quantize.embedding(image_tokens).reshape(1, 16, 16, -1).permute(0, 3, 1, 2)
img = vqgan.decode(z)
img = torch.clamp(img[0], -1, 1) * 0.5 + 0.5 # [-1,1] â†’ [0,1]
pil_img = T.ToPILImage()(img.cpu())
# è½¬ base64
buffer = BytesIO()
pil_img.save(buffer, format="PNG")
b64 = base64.b64encode(buffer.getvalue()).decode()
return b64

ğŸ“¦ é¢„è®­ç»ƒæ¨¡å‹å»ºè®®ï¼ˆå…ä»å¤´è®­ç»ƒï¼‰

1. VQGANï¼š
ä½¿ç”¨ [taming-transformers](https://github.com/CompVis/taming-transformers) æä¾›çš„ imagenet_f16_1024 æ¨¡å‹
å·²æ”¯æŒ 256x256 å›¾åƒï¼Œcodebook=1024

2. ä¸­æ–‡æ–‡æœ¬ç¼–ç å™¨ï¼š
uer/chinese_roberta_L-4_H-256ï¼ˆè¶…è½»é‡ï¼‰
æˆ– hfl/chinese-distilbert-base
ğŸ” å¦‚æœæ—¶é—´ç´§å¼ ï¼Œå¯ç›´æ¥ å†»ç»“æ–‡æœ¬ç¼–ç å™¨ + VQGANï¼Œåªè®­ç»ƒä¸­é—´æ˜ å°„å±‚ï¼ˆMLP æˆ–å°å‹ cross-attentionï¼‰

ğŸ“Š é¢„æœŸæ€§èƒ½ï¼ˆåœ¨ä½ çš„ç¡¬ä»¶ä¸Šï¼‰

é˜¶æ®µ æ˜¾å­˜å ç”¨ æ—¶é—´ï¼ˆæ¯ epochï¼‰
------ -------- ----------------
è®­ç»ƒï¼ˆbs=8ï¼‰ ~12â€“14 GB ~2â€“3 å°æ—¶ï¼ˆ90k æ ·æœ¬ï¼‰
æ¨ç†ï¼ˆå•å›¾ï¼‰ ~3 GB ~1â€“2 ç§’/å›¾
æ•°æ®åŠ è½½ CPU å†…å­˜å³°å€¼ ~20 GBï¼ˆéœ€åˆ†æ‰¹åŠ è½½ï¼‰ â€”

âœ… æœ€ç»ˆäº¤ä»˜ç‰©
T2I_test.tsvï¼šimg_id<TAB>base64_str
æ¨¡å‹ checkpointï¼ˆå¯é€‰ï¼‰
æ¨ç†è„šæœ¬ï¼ˆå« base64 ç¼–ç é€»è¾‘ï¼‰

ğŸ”— è¡¥å……èµ„æº
VQGAN é¢„è®­ç»ƒæ¨¡å‹ï¼šhttps://github.com/CompVis/taming-transformers#pretrained-vqgan-models
ä¸­æ–‡ DistilBERTï¼šhttps://huggingface.co/hfl/chinese-distilbert-base
AMP è®­ç»ƒæ•™ç¨‹ï¼šhttps://pytorch.org/docs/stable/amp.html

å¦‚éœ€ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æä¾›ä¸€ä¸ª å®Œæ•´çš„æœ€å°å¯è¿è¡Œä»£ç æ¨¡æ¿ï¼ˆtrain + inferï¼‰ï¼ŒåŒ…å«æ•°æ®åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€base64 è¾“å‡ºç­‰ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ