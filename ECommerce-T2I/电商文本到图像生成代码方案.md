# 电商文本到图像生成代码方案

## 1. 项目概述

本方案基于电商文本到图像生成任务，实现从商品描述文本生成对应商品图片的功能。方案采用扩散模型（Diffusion Model）作为基础架构，并针对电商领域进行定制化微调，以确保生成图片的质量和相关性。

## 2. 项目结构

```
ECommerce-T2I/
├── data/                  # 数据相关目录
│   ├── raw/               # 原始数据（待下载）
│   ├── processed/         # 处理后的数据
│   └── dataloader.py      # 数据加载器
├── models/                # 模型相关目录
│   ├── diffusion/         # 扩散模型核心
│   ├── text_encoder/      # 文本编码器
│   ├── image_encoder/     # 图像编码器（用于评估）
│   └── utils.py           # 模型工具函数
├── train/                 # 训练相关目录
│   ├── trainer.py         # 训练器
│   ├── config.py          # 训练配置
│   └── metrics.py         # 评估指标
├── inference/             # 推理相关目录
│   ├── infer.py           # 推理脚本
│   └── submit.py          # 提交文件生成器
├── tools/                 # 工具脚本
│   ├── data_process.py    # 数据处理脚本
│   └── eval.py            # 评估脚本
├── README.md              # 项目说明
├── requirements.txt       # 依赖库
└── main.py                # 主入口
```

## 3. 核心组件设计

### 3.1 数据处理模块

```python
# data/dataloader.py
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import base64
from io import BytesIO
from PIL import Image
from transformers import CLIPTokenizer

class ECommerceDataset(Dataset):
    def __init__(self, text_file, image_file, tokenizer, image_size=512):
        self.text_data = pd.read_csv(text_file, sep='\t', header=None, names=['img_id', 'description'])
        self.image_data = pd.read_csv(image_file, sep='\t', header=None, names=['img_id', 'image_base64'])
        self.data = pd.merge(self.text_data, self.image_data, on='img_id')
        self.tokenizer = tokenizer
        self.image_size = image_size
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        img_id = row['img_id']
        
        # 处理文本
        text = row['description']
        inputs = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=77,
            return_tensors='pt'
        )
        
        # 处理图像
        image_base64 = row['image_base64']
        image = Image.open(BytesIO(base64.urlsafe_b64decode(image_base64)))
        image = image.resize((self.image_size, self.image_size))
        image = (torch.tensor(image) / 255.0 - 0.5) * 2  # 归一化到[-1, 1]
        image = image.permute(2, 0, 1)  # [H, W, C] -> [C, H, W]
        
        return {
            'img_id': img_id,
            'input_ids': inputs.input_ids.squeeze(),
            'attention_mask': inputs.attention_mask.squeeze(),
            'pixel_values': image
        }

def get_dataloader(text_file, image_file, batch_size, image_size=512):
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    dataset = ECommerceDataset(text_file, image_file, tokenizer, image_size)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)
```

### 3.2 模型架构

采用基于Stable Diffusion的微调方案：

```python
# models/diffusion/pipeline.py
from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline
from transformers import CLIPTextModel, CLIPTokenizer
import torch

class ECommerceStableDiffusionPipeline(StableDiffusionPipeline):
    def __init__(self, vae, text_encoder, tokenizer, unet, scheduler):
        super().__init__(vae, text_encoder, tokenizer, unet, scheduler)
    
    def train_step(self, batch):
        # 实现训练步骤
        pass

def build_model():
    # 加载预训练模型
    pipeline = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        use_safetensors=True
    )
    
    # 转换为电商领域专用模型
    ecommerce_pipeline = ECommerceStableDiffusionPipeline(
        vae=pipeline.vae,
        text_encoder=pipeline.text_encoder,
        tokenizer=pipeline.tokenizer,
        unet=pipeline.unet,
        scheduler=pipeline.scheduler
    )
    
    return ecommerce_pipeline
```

### 3.3 训练器设计

```python
# train/trainer.py
import torch
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm

class Trainer:
    def __init__(self, model, train_dataloader, val_dataloader, config):
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.config = config
        
        # 设备设置
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # 优化器设置
        self.optimizer = AdamW(
            self.model.unet.parameters(),
            lr=config.lr,
            weight_decay=config.weight_decay
        )
        
        # 学习率调度器
        total_steps = len(train_dataloader) * config.epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=total_steps
        )
    
    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        
        for batch in tqdm(self.train_dataloader, desc=f"Epoch {epoch+1}/{self.config.epochs}"):
            # 数据移到设备
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # 前向传播
            loss = self.model.train_step(batch)
            
            # 反向传播
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.unet.parameters(), self.config.clip_grad)
            
            # 更新参数
            self.optimizer.step()
            self.scheduler.step()
            self.optimizer.zero_grad()
            
            total_loss += loss.item()
        
        return total_loss / len(self.train_dataloader)
    
    def train(self):
        for epoch in range(self.config.epochs):
            train_loss = self.train_epoch(epoch)
            print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}")
            
            # 验证
            if (epoch + 1) % self.config.val_interval == 0:
                val_loss = self.validate()
                print(f"Epoch {epoch+1}, Val Loss: {val_loss:.4f}")
                
            # 保存模型
            if (epoch + 1) % self.config.save_interval == 0:
                self.save_model(epoch)
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in self.val_dataloader:
                batch = {k: v.to(self.device) for k, v in batch.items()}
                loss = self.model.train_step(batch)
                total_loss += loss.item()
        
        return total_loss / len(self.val_dataloader)
    
    def save_model(self, epoch):
        save_path = f"{self.config.save_dir}/model_epoch_{epoch+1}.pt"
        torch.save({
            "unet_state_dict": self.model.unet.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "epoch": epoch
        }, save_path)
        print(f"Model saved to {save_path}")
```

### 3.4 评估指标实现

```python
# train/metrics.py
import torch
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.image.inception import InceptionScore
from sentence_transformers import SentenceTransformer
from PIL import Image
import clip

class MetricsCalculator:
    def __init__(self):
        # 加载评估所需的模型
        self.fid = FrechetInceptionDistance(feature=2048)
        self.is_metric = InceptionScore(feature=2048)
        self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device="cuda")
    
    def calculate_fid(self, real_images, generated_images):
        # 计算FID
        self.fid.update(real_images, real=True)
        self.fid.update(generated_images, real=False)
        return self.fid.compute()
    
    def calculate_is(self, generated_images):
        # 计算IS
        self.is_metric.update(generated_images)
        return self.is_metric.compute()
    
    def calculate_r_precision(self, texts, generated_images):
        # 计算R-precision
        # 1. 编码文本
        text_embeddings = self.sentence_model.encode(texts, convert_to_tensor=True)
        
        # 2. 编码生成图像
        image_embeddings = []
        for img in generated_images:
            pil_img = Image.fromarray((img * 255).astype('uint8'))
            processed_img = self.clip_preprocess(pil_img).unsqueeze(0).to("cuda")
            with torch.no_grad():
                img_emb = self.clip_model.encode_image(processed_img)
            image_embeddings.append(img_emb)
        image_embeddings = torch.cat(image_embeddings, dim=0)
        
        # 3. 计算相似度
        similarities = torch.matmul(image_embeddings, text_embeddings.T)
        
        # 4. 计算R-precision
        r_precision = 0
        for i in range(len(texts)):
            top_k = min(5, len(texts))  # 取top-5
            top_indices = torch.topk(similarities[i], top_k).indices
            if i in top_indices:
                r_precision += 1
        r_precision /= len(texts)
        
        return r_precision
```

### 3.5 推理与提交

```python
# inference/submit.py
import pandas as pd
import base64
from io import BytesIO
from PIL import Image
import torch

class SubmitGenerator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
    
    def generate_image(self, text):
        # 使用模型生成图像
        with torch.no_grad():
            image = self.model.generate(
                prompt=text,
                num_inference_steps=50,
                guidance_scale=7.5,
                height=512,
                width=512
            )[0]
        return image
    
    def image_to_base64(self, image):
        # 将图像转换为base64编码
        img_buffer = BytesIO()
        image.save(img_buffer, format='PNG')
        byte_data = img_buffer.getvalue()
        base64_str = base64.b64encode(byte_data).decode('utf-8')
        return base64_str
    
    def generate_submit_file(self, test_file, output_file):
        # 读取测试数据
        test_data = pd.read_csv(test_file, sep='\t', header=None, names=['img_id', 'description'])
        
        # 生成结果
        results = []
        for _, row in test_data.iterrows():
            img_id = row['img_id']
            text = row['description']
            
            # 生成图像
            image = self.generate_image(text)
            
            # 转换为base64
            base64_str = self.image_to_base64(image)
            
            results.append([img_id, base64_str])
        
        # 生成提交文件
        submit_df = pd.DataFrame(results, columns=['img_id', 'image_base64'])
        submit_df.to_csv(output_file, sep='\t', index=False, header=False)
        print(f"Submit file generated: {output_file}")
```

## 4. 训练配置

```python
# train/config.py
class TrainingConfig:
    # 数据配置
    image_size = 512
    batch_size = 8
    num_workers = 4
    
    # 训练配置
    lr = 1e-5
    weight_decay = 0.01
    epochs = 10
    warmup_steps = 1000
    clip_grad = 1.0
    
    # 评估配置
    val_interval = 1
    fid_interval = 5
    
    # 保存配置
    save_dir = "./checkpoints"
    save_interval = 2
    
    # 其他配置
    device = "cuda" if torch.cuda.is_available() else "cpu"
```

## 5. 主流程

```python
# main.py
import argparse
from data.dataloader import get_dataloader
from models.diffusion.pipeline import build_model
from train.trainer import Trainer
from train.config import TrainingConfig

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, choices=["train", "infer"], default="train")
    parser.add_argument("--config", type=str, default="./train/config.py")
    args = parser.parse_args()
    
    if args.mode == "train":
        # 加载配置
        config = TrainingConfig()
        
        # 加载数据
        train_dataloader = get_dataloader(
            text_file="./data/raw/T2I_train.txt.tsv",
            image_file="./data/raw/T2I_train.img.tsv",
            batch_size=config.batch_size,
            image_size=config.image_size
        )
        
        val_dataloader = get_dataloader(
            text_file="./data/raw/T2I_val.txt.tsv",
            image_file="./data/raw/T2I_val.img.tsv",
            batch_size=config.batch_size,
            image_size=config.image_size
        )
        
        # 构建模型
        model = build_model()
        
        # 开始训练
        trainer = Trainer(model, train_dataloader, val_dataloader, config)
        trainer.train()
    
    elif args.mode == "infer":
        # 推理逻辑
        from inference.submit import SubmitGenerator
        from transformers import CLIPTokenizer
        
        # 加载模型和分词器
        model = build_model()
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        
        # 加载训练好的权重
        checkpoint = torch.load("./checkpoints/model_epoch_10.pt")
        model.unet.load_state_dict(checkpoint["unet_state_dict"])
        
        # 生成提交文件
        submit_generator = SubmitGenerator(model, tokenizer)
        submit_generator.generate_submit_file(
            test_file="./data/raw/T2I_test.txt.tsv",
            output_file="./T2I_test.tsv"
        )
```

## 6. 依赖库

```
# requirements.txt
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
diffusers>=0.19.0
sentence-transformers>=2.2.0
clip-by-openai>=1.0
tqdm>=4.65.0
pandas>=2.0.0
pillow>=10.0.0
numpy>=1.24.0
```

## 7. 运行步骤

### 7.1 数据准备

1. 下载数据集：`ECommerce-T2I.zip`
2. 解压到 `data/raw/` 目录
3. 运行数据处理脚本：
   ```bash
   python tools/data_process.py
   ```

### 7.2 训练模型

```bash
python main.py --mode train
```

### 7.3 生成提交文件

```bash
python main.py --mode infer
```

## 8. 优化策略

1. **文本增强**：对商品描述进行同义词替换、语序调整等增强，提高模型的泛化能力
2. **图像增强**：对商品图片进行裁剪、旋转、缩放等增强，丰富训练数据
3. **参数高效微调**：使用LoRA或QLoRA技术，减少微调参数量，加快训练速度
4. **混合精度训练**：使用FP16混合精度训练，减少显存占用，提高训练速度
5. **梯度累积**：在显存不足时使用梯度累积，模拟更大批次的训练效果
6. **多尺度训练**：采用多尺度图像训练策略，提高模型对不同分辨率的适应能力

## 9. 评估与改进

1. **定量评估**：定期计算FID、IS和R-precision指标，监控模型性能
2. **定性评估**：随机抽取生成图片进行人工检查，评估生成质量和相关性
3. **错误分析**：分析生成失败的案例，针对性地改进模型和训练策略
4. **迭代优化**：根据评估结果，调整模型架构、超参数和训练策略

## 10. 注意事项

1. **显存需求**：训练Stable Diffusion模型需要至少24GB显存，建议使用A100或更高级别GPU
2. **数据格式**：确保TSV文件格式正确，base64编码无错误
3. **提交格式**：生成的提交文件必须严格遵循官方格式要求
4. **计算资源**：训练过程耗时较长，建议使用多GPU并行训练或云端GPU资源
5. **模型版权**：确保使用的预训练模型符合版权要求

以上方案为电商文本到图像生成任务提供了一个完整的解决方案，包括数据处理、模型训练、推理生成和提交文件生成等各个环节。该方案基于最新的扩散模型技术，能够生成高质量、高相关性的商品图片，满足电商领域的应用需求。