{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3：基线模型训练与验证\n",
    "\n",
    "本 Notebook 基于 `plan1.md` 的 “#### 2.3 基线模型训练与验证” 规划，并复用 `step_2_2-特征融合与匹配机制` 中的实现思路：\n",
    "- 使用文本与图像特征提取器（BERT + ResNet50）\n",
    "- 使用特征融合（投影到共享空间）与相似度计算（余弦相似度）\n",
    "- 构建对比学习（InfoNCE）训练循环，优化投影层\n",
    "- 在验证集上评估 Recall@1/5/10 并报告 MeanRecall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "- 统一设置本地缓存目录 `/mnt/d/HuggingFaceModels`，仅从本地加载\n",
    "- 导入依赖与数据加载组件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers torch timm torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 仅使用本地缓存\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "\n",
    "# 导入数据加载器\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征提取器与融合/相似度模块\n",
    "与 `step_2_2` 保持一致：\n",
    "- 文本：`bert-base-chinese` 的 [CLS] 输出（768维）\n",
    "- 图像：`resnet50` 的全局特征（2048维）\n",
    "- 融合：线性投影到共享空间（默认512维）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def extract_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    def extract_features(self, texts):\n",
    "        return self.extract_text_features(texts)\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        # timm 将使用 TORCH_HOME 缓存目录；需确保权重已存在以避免下载\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=0).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def extract_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            feats = self.model(tensors)\n",
    "        return feats\n",
    "    \n",
    "    def extract_features(self, images):\n",
    "        return self.extract_image_features(images)\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Linear(768, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(2048, projection_dim).to(self.device)\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        t = self.text_extractor.extract_features(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        i = self.image_extractor.extract_features(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size=32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            # 保持顺序映射；若有None，跳过\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据准备：构建训练配对与验证索引\n",
    "- 按 query 的 `item_ids` 选择对应图片\n",
    "- 跳过缺失或未能解码的图片\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# 为训练与验证加载一定数量的图片\n",
    "train_imgs = loader.create_img_id_to_image_dict(split='train', max_samples=500)\n",
    "valid_imgs = loader.create_img_id_to_image_dict(split='valid', max_samples=500)\n",
    "print(f'Train queries: {len(train_df)}, Train images: {len(train_imgs)}')\n",
    "print(f'Valid queries: {len(valid_df)}, Valid images: {len(valid_imgs)}')\n",
    "\n",
    "# 构建 (text, image) 训练配对\n",
    "train_pairs = []\n",
    "if 'item_ids' in train_df.columns:\n",
    "    for _, row in train_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = row.get('item_ids', [])\n",
    "        if not q or not ids:\n",
    "            continue\n",
    "        # 寻找第一个可用图片\n",
    "        chosen_img = None\n",
    "        chosen_id = None\n",
    "        for iid in ids:\n",
    "            sid = str(iid)\n",
    "            if sid in train_imgs and train_imgs[sid] is not None:\n",
    "                chosen_img = train_imgs[sid]\n",
    "                chosen_id = sid\n",
    "                break\n",
    "        if chosen_img is not None:\n",
    "            train_pairs.append((q, chosen_img, chosen_id))\n",
    "print(f'Usable train pairs: {len(train_pairs)}')\n",
    "\n",
    "# 验证：过滤出带 item_ids 的query\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练：对比学习优化投影层（InfoNCE）\n",
    "- 仅优化 `FeatureFusion` 的投影参数\n",
    "- logits = sim(text, image) / temperature；label = 对角匹配\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "model = CrossModalRetrievalModel(text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device)\n",
    "\n",
    "# 仅优化投影层参数\n",
    "optim = torch.optim.Adam(\n",
    "    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()), \n",
    "    lr=1e-3, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "temperature = 0.07\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) / 2\n",
    "\n",
    "# 构建训练小批次\n",
    "def batch_iter(pairs, bs):\n",
    "    for s in range(0, len(pairs), bs):\n",
    "        yield pairs[s:s+bs]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch in tqdm(batch_iter(train_pairs, batch_size), desc=f'Epoch {ep+1}/{epochs}'):\n",
    "        texts = [b[0] for b in batch]\n",
    "        imgs = [b[1] for b in batch]\n",
    "        t_feats = model.extract_and_fuse_text_features(texts)\n",
    "        i_feats = model.extract_and_fuse_image_features(imgs)\n",
    "        if t_feats.size(0) == 0 or i_feats.size(0) == 0:\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        loss = info_nce_loss(t_feats, i_feats, temperature)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "    print(f'Epoch {ep+1}: avg loss={epoch_loss/max(1,steps):.4f}')\n",
    "\n",
    "# 冻结投影进行评估\n",
    "model.fusion.text_projector.eval()\n",
    "model.fusion.image_projector.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建验证图像索引\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]).to(device) if all_image_ids else torch.empty((0, 512), device=device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "        top_scores, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "        top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 保存投影层权重\n",
    "- 便于后续复现与继续训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'weights'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'step_2_3_projection.pth')\n",
    "torch.save({\n",
    "    'text_projector': model.fusion.text_projector.state_dict(),\n",
    "    'image_projector': model.fusion.image_projector.state_dict(),\n",
    "    'projection_dim': model.fusion.projection_dim\n",
    "}, save_path)\n",
    "print(f'Saved projection weights to: {save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 注意事项\n",
    "- 若本地 `timm` 的 `resnet50` 权重不存在，需先手动缓存到 `TORCH_HOME` 目录；否则可能尝试联网下载。\n",
    "- 若验证集的 `item_ids` 与图片索引不匹配或样本较少，评估指标可能为0或不稳定。\n",
    "- 训练循环演示为轻量版本（epochs=2）；实际训练可适当增大 epochs 与样本规模。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
