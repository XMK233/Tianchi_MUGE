{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2.1：特征提取模块设计 (v2 - 使用 Chinese-CLIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此 notebook 中，我们将使用更先进的 **Chinese-CLIP** 模型来替代之前使用的 BERT 和 ResNet。Chinese-CLIP 是一个为中文多模态任务设计的强大模型，它包含一个基于 ViT 的图像编码器和一个文本编码器，两者都经过了联合训练，以确保生成的特征向量在同一个语义空间中对齐。\n",
    "\n",
    "我们将执行以下步骤：\n",
    "1. **环境准备**：安装并导入所需的库。\n",
    "2. **模型加载**：加载 `OFA-Sys/chinese-clip-vit-base-patch16` 模型和对应的处理器。\n",
    "3. **特征提取器定义**：定义一个统一的特征提取器类，该类可以同时处理文本和图像。\n",
    "4. **数据加载**：使用我们之前定义的 `DataLoader` 加载少量样本数据。\n",
    "5. **功能演示**：在样本数据上运行特征提取器，并检查输出的特征向量的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from plan1.data_loader import DataLoader\n",
    "\n",
    "# 设置 Hugging Face 的缓存目录\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 模型加载与特征提取器定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseCLIPFeatureExtractor:\n",
    "    def __init__(self, model_name=\"OFA-Sys/chinese-clip-vit-base-patch16\", device=None, cache_dir=None):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.model = ChineseCLIPModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(self.device)\n",
    "        self.processor = ChineseCLIPProcessor.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "    def extract_text_features(self, text_list):\n",
    "        inputs = self.processor(text=text_list, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        return text_features\n",
    "\n",
    "    def extract_image_features(self, image_list):\n",
    "        inputs = self.processor(images=image_list, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据加载与功能演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 DataLoader\n",
    "data_loader = DataLoader(data_dir=\"../../../../mnt/d/forCoding_code/Tianchi_MUGE/data/imgs\", \n",
    "                        query_file=\"../../../../mnt/d/forCoding_code/Tianchi_MUGE/data/multimodal_train.jsonl\",\n",
    "                        doc_file=\"../../../../mnt/d/forCoding_code/Tianchi_MUGE/data/multimodal_train.jsonl\")\n",
    "\n",
    "# 加载少量样本数据\n",
    "sample_queries = data_loader.query_data.head(5)\n",
    "sample_docs = data_loader.doc_data.head(5)\n",
    "\n",
    "# 准备文本和图像数据\n",
    "texts_to_process = sample_queries['query_text'].tolist()\n",
    "image_paths_to_process = [f\"../../../../mnt/d/forCoding_code/Tianchi_MUGE/data/imgs/{img_name}.jpg\" for img_name in sample_docs['item_id']]\n",
    "images_to_process = [Image.open(p) for p in image_paths_to_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化特征提取器\n",
    "feature_extractor = ChineseCLIPFeatureExtractor(cache_dir=cache_dir)\n",
    "\n",
    "# 提取文本特征\n",
    "text_features = feature_extractor.extract_text_features(texts_to_process)\n",
    "print(f\"Text features shape: {text_features.shape}\")\n",
    "\n",
    "# 提取图像特征\n",
    "image_features = feature_extractor.extract_image_features(images_to_process)\n",
    "print(f\"Image features shape: {image_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 总结\n",
    "\n",
    "在这个 notebook 中，我们成功地：\n",
    "1. 使用了 `OFA-Sys/chinese-clip-vit-base-patch16` 模型，这是一个更先进的、为中文多模态任务设计的模型。\n",
    "2. 定义了一个统一的特征提取器，可以方便地从文本和图像中提取高质量的特征向量。\n",
    "3. 在样本数据上成功运行了特征提取器，并验证了输出的特征向量具有预期的形状（对于 `base` 模型，通常是 512 维）。\n",
    "\n",
    "这些特征向量现在可以在同一个语义空间中进行比较，为我们下一步构建特征融合和匹配模块奠定了坚实的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
