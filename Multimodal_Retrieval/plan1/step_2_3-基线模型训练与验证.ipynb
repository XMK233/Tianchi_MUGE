{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3：基线模型训练与验证\n",
    "\n",
    "本 Notebook 基于 `plan1.md` 的 “#### 2.3 基线模型训练与验证” 规划，并复用 `step_2_2-特征融合与匹配机制` 中的实现思路：\n",
    "- 使用文本与图像特征提取器（BERT + ResNet50）\n",
    "- 使用特征融合（投影到共享空间）与相似度计算（余弦相似度）\n",
    "- 构建对比学习（InfoNCE）训练循环，优化投影层\n",
    "- 在验证集上评估 Recall@1/5/10 并报告 MeanRecall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "- 统一设置本地缓存目录 `/mnt/d/HuggingFaceModels`，仅从本地加载\n",
    "- 导入依赖与数据加载组件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers torch timm torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # 仅使用本地缓存\n",
    "# cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "# os.environ['TORCH_HOME'] = cache_dir\n",
    "# os.environ['HF_HOME'] = cache_dir\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "\n",
    "# 设置环境变量\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 导入数据加载器\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征提取器与融合/相似度模块\n",
    "与 `step_2_2` 保持一致：\n",
    "- 文本：`bert-base-chinese` 的 [CLS] 输出（768维）\n",
    "- 图像：`resnet50` 的全局特征（2048维）\n",
    "- 融合：线性投影到共享空间（默认512维）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def extract_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    def extract_features(self, texts):\n",
    "        return self.extract_text_features(texts)\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        # timm 将使用 TORCH_HOME 缓存目录；需确保权重已存在以避免下载\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def extract_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            feats = self.model(tensors)\n",
    "        return feats\n",
    "    \n",
    "    def extract_features(self, images):\n",
    "        return self.extract_image_features(images)\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Linear(768, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(2048, projection_dim).to(self.device)\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        t = self.text_extractor.extract_features(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        i = self.image_extractor.extract_features(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size=32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            # 保持顺序映射；若有None，跳过\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据准备：构建训练配对与验证索引\n",
    "- 按 query 的 `item_ids` 选择对应图片\n",
    "- 跳过缺失或未能解码的图片\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 01:11:41,476 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-06 01:11:41,479 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:00, 249972.30it/s]\n",
      "2025-11-06 01:11:42,550 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-06 01:11:42,562 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 311306.20it/s]\n",
      "2025-11-06 01:11:42,583 - INFO - 成功加载valid查询数据，共5008条\n",
      "2025-11-06 01:11:42,584 - INFO - 训练集可能很大，建议指定max_samples参数限制加载数量\n",
      "2025-11-06 01:11:42,586 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "加载train图片数据: 100%|████████████████████████████████████████████████████████▉| 9999/10000 [00:05<00:00, 1870.47it/s]\n",
      "2025-11-06 01:11:58,966 - INFO - 成功创建train图片映射字典，共10000张图片\n",
      "2025-11-06 01:11:58,968 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "加载valid图片数据: 100%|████████████████████████████████████████████████████████| 29806/29806 [00:16<00:00, 1795.12it/s]\n",
      "2025-11-06 01:12:18,464 - INFO - 成功创建valid图片映射字典，共29806张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train queries: 248786, Train images: 10000\n",
      "Valid queries: 5008, Valid images: 29806\n",
      "Usable train pairs: 19673\n",
      "Usable valid queries: 5008\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# 为训练与验证加载一定数量的图片\n",
    "train_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='train', \n",
    "    max_samples=50000\n",
    ")\n",
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=50000\n",
    ")\n",
    "print(f'Train queries: {len(train_df)}, Train images: {len(train_imgs)}')\n",
    "print(f'Valid queries: {len(valid_df)}, Valid images: {len(valid_imgs)}')\n",
    "\n",
    "# 构建 (text, image) 训练配对\n",
    "train_pairs = []\n",
    "if 'item_ids' in train_df.columns:\n",
    "    for _, row in train_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = row.get('item_ids', [])\n",
    "        if not q or not ids:\n",
    "            continue\n",
    "        # 寻找第一个可用图片\n",
    "        chosen_img = None\n",
    "        chosen_id = None\n",
    "        for iid in ids:\n",
    "            sid = str(iid)\n",
    "            if sid in train_imgs and train_imgs[sid] is not None:\n",
    "                chosen_img = train_imgs[sid]\n",
    "                chosen_id = sid\n",
    "                break\n",
    "        if chosen_img is not None:\n",
    "            train_pairs.append((q, chosen_img, chosen_id))\n",
    "print(f'Usable train pairs: {len(train_pairs)}')\n",
    "\n",
    "# 验证：过滤出带 item_ids 的query\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练：对比学习优化投影层（InfoNCE）\n",
    "- 仅优化 `FeatureFusion` 的投影参数\n",
    "- logits = sim(text, image) / temperature；label = 对角匹配\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 01:12:22,436 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-06 01:12:22,846 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 1230it [00:25, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss=0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 1230it [00:25, 48.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss=0.3679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 1230it [00:24, 49.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg loss=0.2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 1230it [00:24, 49.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss=0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 1230it [00:24, 49.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg loss=0.2379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 1230it [00:25, 48.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg loss=0.2230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 1230it [00:24, 49.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg loss=0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 1230it [00:24, 49.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg loss=0.2055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 1230it [00:24, 49.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg loss=0.2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 1230it [00:24, 49.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg loss=0.1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=512, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 仅优化投影层参数\n",
    "optim = torch.optim.Adam(\n",
    "    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()), \n",
    "    lr=1e-3, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "temperature = 0.07\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "\n",
    "# 以代码中使用的 batch_size=16 和 projection_dim=512 为例：\n",
    "\n",
    "# - text_feats 形状: [16, 512]\n",
    "# - image_feats 形状: [16, 512]\n",
    "# - logits 形状: [16, 16] - 每个文本样本与16个图像样本的相似度\n",
    "# - labels 形状: [16] - 值为 [0, 1, 2, ..., 15]\n",
    "# - loss_t 和 loss_i 都是标量\n",
    "# - 最终返回一个标量损失值\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    # text_feats 形状: [N, D] - N个文本样本，每个样本D维特征\n",
    "    # image_feats 形状: [N, D] - N个图像样本，每个样本D维特征\n",
    "    # temp 是标量浮点数，形状: []\n",
    "    ####################################\n",
    "    # 1. 计算相似度矩阵：矩阵乘法计算文本特征和图像特征的点积相似度\n",
    "    # 除以温度参数temp控制分布的平滑程度，小温度值会使分布更尖锐。\n",
    "    # torch.mm执行矩阵乘法，image_feats.t()将图像特征转置\n",
    "    # logits 形状: [N, N] - 每个文本特征与所有图像特征的相似度矩阵\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    \n",
    "    # 2. 创建标签：对角线元素对应正确匹配的样本对\n",
    "    # 假设text_feats和image_feats是一一对应的配对样本。\n",
    "    # 创建长度为N的标签张量，值为0到N-1\n",
    "    # labels 形状: [N] - 一维张量，对角线位置对应正确匹配\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    \n",
    "    # 3. 计算文本到图像的对比损失\n",
    "    # 将相似度矩阵视为分类logits，对角线位置为正确类别。\n",
    "    # 计算文本到图像的交叉熵损失\n",
    "    # loss_t 形状: [] - 标量值，表示当前batch的文本到图像方向损失\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    \n",
    "    # 计算图像到文本的交叉熵损失（交换相似度矩阵维度）\n",
    "    # logits.t() 形状: [N, N] - 转置后的相似度矩阵\n",
    "    # loss_i 形状: [] - 标量值，表示当前batch的图像到文本方向损失\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    \n",
    "    # 返回两个方向损失的平均值\n",
    "    # 返回值形状: [] - 标量值，最终的InfoNCE损失\n",
    "    return (loss_t + loss_i) / 2\n",
    "\n",
    "# 构建训练小批次\n",
    "def batch_iter(pairs, bs):\n",
    "    for s in range(0, len(pairs), bs):\n",
    "        yield pairs[s:s+bs]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch in tqdm(batch_iter(train_pairs, batch_size), desc=f'Epoch {ep+1}/{epochs}'):\n",
    "        texts = [b[0] for b in batch]\n",
    "        imgs = [b[1] for b in batch]\n",
    "        t_feats = model.extract_and_fuse_text_features(texts)\n",
    "        i_feats = model.extract_and_fuse_image_features(imgs)\n",
    "        if t_feats.size(0) == 0 or i_feats.size(0) == 0:\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        loss = info_nce_loss(t_feats, i_feats, temperature)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "    print(f'Epoch {ep+1}: avg loss={epoch_loss/max(1,steps):.4f}')\n",
    "\n",
    "# 冻结投影进行评估\n",
    "model.fusion.text_projector.eval()\n",
    "model.fusion.image_projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 1230it [00:25, 47.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss=0.1925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 1230it [00:25, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss=0.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 1230it [00:25, 48.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg loss=0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 1230it [00:25, 48.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss=0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 1230it [00:26, 47.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg loss=0.1847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 1230it [00:25, 48.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg loss=0.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 1230it [00:25, 48.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg loss=0.1812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 1230it [00:25, 49.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg loss=0.1767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 1230it [00:25, 47.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg loss=0.1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 1230it [00:25, 47.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg loss=0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=512, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for ep in range(epochs):\n",
    "#     model.fusion.text_projector.train()\n",
    "#     model.fusion.image_projector.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     steps = 0\n",
    "#     for batch in tqdm(batch_iter(train_pairs, batch_size), desc=f'Epoch {ep+1}/{epochs}'):\n",
    "#         texts = [b[0] for b in batch]\n",
    "#         imgs = [b[1] for b in batch]\n",
    "#         t_feats = model.extract_and_fuse_text_features(texts)\n",
    "#         i_feats = model.extract_and_fuse_image_features(imgs)\n",
    "#         if t_feats.size(0) == 0 or i_feats.size(0) == 0:\n",
    "#             continue\n",
    "#         optim.zero_grad()\n",
    "#         loss = info_nce_loss(t_feats, i_feats, temperature)\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#         steps += 1\n",
    "#     print(f'Epoch {ep+1}: avg loss={epoch_loss/max(1,steps):.4f}')\n",
    "\n",
    "# # 冻结投影进行评估\n",
    "# model.fusion.text_projector.eval()\n",
    "# model.fusion.image_projector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:24<00:00, 200.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0176, Recall@5=0.0763, Recall@10=0.1266, MeanRecall=0.0735 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建验证图像索引\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]).to(device) if all_image_ids else torch.empty((0, 512), device=device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "        top_scores, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "        top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用5W图训练，\n",
    "\n",
    "## 训练20轮：Recall@1=0.0176, Recall@5=0.0763, Recall@10=0.1266, MeanRecall=0.0735 (N=5008)\n",
    "## 训练10轮：Recall@1=0.0200, Recall@5=0.0731, Recall@10=0.1206, MeanRecall=0.0712 (N=5008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 保存投影层权重\n",
    "- 便于后续复现与继续训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved projection weights to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_2_3_projection.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'step_2_3_projection.pth')\n",
    "torch.save({\n",
    "    'text_projector': model.fusion.text_projector.state_dict(),\n",
    "    'image_projector': model.fusion.image_projector.state_dict(),\n",
    "    'projection_dim': model.fusion.projection_dim\n",
    "}, save_path)\n",
    "print(f'Saved projection weights to: {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 注意事项\n",
    "- 若本地 `timm` 的 `resnet50` 权重不存在，需先手动缓存到 `TORCH_HOME` 目录；否则可能尝试联网下载。\n",
    "- 若验证集的 `item_ids` 与图片索引不匹配或样本较少，评估指标可能为0或不稳定。\n",
    "- 训练循环演示为轻量版本（epochs=2）；实际训练可适当增大 epochs 与样本规模。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}