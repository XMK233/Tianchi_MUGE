{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.1: 特征提取模块设计\n",
    "\n",
    "根据 `plan1.md` 的规划，本 Notebook 将设计并实现用于文本和图像的特征提取模块。我们将：\n",
    "\n",
    "1.  **选择并加载预训练模型**：\n",
    "    *   文本特征提取：使用 `bert-base-chinese`。\n",
    "    *   图像特征提取：使用 `resnet50`。\n",
    "2.  **封装特征提取器**：创建独立的类来处理文本和图像的特征提取。\n",
    "3.  **加载示例数据**：使用 `DataLoader` 加载少量验证集数据。\n",
    "4.  **执行特征提取**：在示例数据上运行特征提取器，并验证输出的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers torch timm\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import timm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# 设置 Hugging Face 和 timm 的缓存目录\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "\n",
    "\n",
    "# 禁用 transformers 冗余日志\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# 将 data_loader.py 的路径添加到系统路径中\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from data_loader import DataLoader\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 在示例数据上测试特征提取器\n",
    "\n",
    "现在，我们加载一些示例数据，并使用我们刚刚定义的特征提取器来提取文本和图像的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 23:58:22,900 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-04 23:58:22,903 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 298270.09it/s]\n",
      "2025-11-04 23:58:22,965 - INFO - 成功加载valid查询数据，共5008条\n",
      "2025-11-04 23:58:22,986 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "加载valid图片数据: 100%|████████████████████████████████████████████████████████| 29806/29806 [00:15<00:00, 1937.97it/s]\n",
      "2025-11-04 23:58:41,270 - INFO - 成功创建valid图片映射字典，共29806张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 3 条查询和 17 张图片作为示例。\n"
     ]
    }
   ],
   "source": [
    "# 初始化 DataLoader\n",
    "loader = DataLoader()\n",
    "\n",
    "# 加载少量验证集数据作为示例\n",
    "valid_queries_df = loader.load_queries(split='valid')\n",
    "sample_queries = valid_queries_df.head(3)  # 取前3个查询作为示例\n",
    "\n",
    "# 加载对应的图片\n",
    "all_item_ids = []\n",
    "for ids in sample_queries['item_ids']:\n",
    "    all_item_ids.extend(ids)\n",
    "\n",
    "# 使用 create_img_id_to_image_dict 加载特定ID的图片\n",
    "# 注意：这里为了演示，我们加载所有验证集图片，然后筛选。在实际应用中，应优化为按需加载。\n",
    "valid_images_dict = loader.create_img_id_to_image_dict(split='valid') # 限制样本量以节省时间\n",
    "\n",
    "sample_images = []\n",
    "sample_image_ids = []\n",
    "for img_id in all_item_ids:\n",
    "    if str(img_id) in valid_images_dict:\n",
    "        sample_images.append(valid_images_dict[str(img_id)])\n",
    "        sample_image_ids.append(str(img_id))\n",
    "\n",
    "print(f\"加载了 {len(sample_queries)} 条查询和 {len(sample_images)} 张图片作为示例。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_text</th>\n",
       "      <th>item_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>248816</td>\n",
       "      <td>圣诞 抱枕</td>\n",
       "      <td>[1006938, 561749, 936929, 286314, 141999, 183846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>248859</td>\n",
       "      <td>德方焕颜美即露</td>\n",
       "      <td>[665851, 157973, 576313, 102370, 104367, 950760]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248871</td>\n",
       "      <td>荷叶边花瓶</td>\n",
       "      <td>[160459, 666622, 637011, 783255, 178679]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id query_text                                           item_ids\n",
       "0    248816      圣诞 抱枕  [1006938, 561749, 936929, 286314, 141999, 183846]\n",
       "1    248859    德方焕颜美即露   [665851, 157973, 576313, 102370, 104367, 950760]\n",
       "2    248871      荷叶边花瓶           [160459, 666622, 637011, 783255, 178679]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本特征提取模块\n",
    "\n",
    "我们定义一个 `TextFeatureExtractor` 类，它使用 `bert-base-chinese` 模型来提取文本的嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本模型 bert-base-chinese 加载完成。\n"
     ]
    }
   ],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, \n",
    "            local_files_only=True ## 如果是第一次加载，千万不要设置这个为True。\n",
    "        )\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            model_name, cache_dir=cache_dir, \n",
    "            local_files_only=True ## 如果是第一次加载，千万不要设置这个为True。\n",
    "        ).to(device)\n",
    "        self.device = device\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        print(f\"文本模型 {model_name} 加载完成。\")\n",
    "\n",
    "    def extract_features(self, texts):\n",
    "        \"\"\"\n",
    "        从一批文本中提取特征。\n",
    "        \n",
    "        Args:\n",
    "            texts (list of str): 文本列表。\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 特征张量。\n",
    "        \"\"\"\n",
    "        # 使用分词器对文本进行编码\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        \n",
    "        # 在无梯度的上下文中进行前向传播\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # 我们使用 [CLS] token 的输出来代表整个句子的特征\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# 初始化文本特征提取器\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 图像特征提取模块\n",
    "\n",
    "接下来，我们定义一个 `ImageFeatureExtractor` 类，它使用预训练的 `resnet50` 模型来提取图像的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 23:58:48,875 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-04 23:58:49,315 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像模型 resnet50 加载完成。\n"
     ]
    }
   ],
   "source": [
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        # timm 会自动使用 TORCH_HOME 环境变量设置的缓存目录，此处 cache_dir 参数仅为保持接口统一\n",
    "        # 当 pretrained=True 时，timm 会首先在 TORCH_HOME 中查找模型，如果找不到则会尝试下载。\n",
    "        # 要实现类似 local_files_only=True 的效果，需要确保模型文件已存在于缓存目录中。\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir, \n",
    "            # local_files_only=True ## 如果是第一次加载，千万不要设置这个为True。\n",
    "        ).to(device)\n",
    "        self.device = device\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        print(f\"图像模型 {model_name} 加载完成。\")\n",
    "\n",
    "        # 定义图像预处理转换\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, images):\n",
    "        \"\"\"\n",
    "        从一批图像中提取特征。\n",
    "        \n",
    "        Args:\n",
    "            images (list of PIL.Image): PIL 图像列表。\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 特征张量。\n",
    "        \"\"\"\n",
    "        # 将图像转换为张量并进行预处理\n",
    "        image_tensors = torch.stack([self.transform(img) for img in images]).to(self.device)\n",
    "        \n",
    "        # 在无梯度的上下文中进行前向传播\n",
    "        with torch.no_grad():\n",
    "            features = self.model(image_tensors)\n",
    "            \n",
    "        return features\n",
    "\n",
    "# 初始化图像特征提取器\n",
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的文本特征维度: torch.Size([3, 768])\n",
      "提取的图像特征维度: torch.Size([17, 2048])\n"
     ]
    }
   ],
   "source": [
    "# 提取文本特征\n",
    "if not sample_queries.empty:\n",
    "    sample_texts = sample_queries['query_text'].tolist()\n",
    "    text_features = text_extractor.extract_features(sample_texts)\n",
    "    print(f\"提取的文本特征维度: {text_features.shape}\")\n",
    "\n",
    "# 提取图像特征\n",
    "if sample_images:\n",
    "    # 确保所有图像都已正确加载\n",
    "    valid_sample_images = [img for img in sample_images if img is not None]\n",
    "    if valid_sample_images:\n",
    "        image_features = image_extractor.extract_features(valid_sample_images)\n",
    "        print(f\"提取的图像特征维度: {image_features.shape}\")\n",
    "    else:\n",
    "        print(\"没有有效的图像可供处理。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 尝试用clip-vit来搞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
    " \n",
    "processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\", cache_dir=cache_dir, \n",
    "                                                      local_files_only=True)\n",
    "model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\", cache_dir=cache_dir, \n",
    "                                                      local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# compute text features\u001b[39;00m\n\u001b[32m     12\u001b[39m inputs = processor(text=texts, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m text_features = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m text_features = text_features / text_features.norm(p=\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# normalize\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# compute image-text similarity scores\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/transformers/utils/generic.py:809\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    800\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    802\u001b[39m     warnings.warn(\n\u001b[32m    803\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    804\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    805\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    806\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    807\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py:1075\u001b[39m, in \u001b[36mChineseCLIPModel.get_text_features\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids)\u001b[39m\n\u001b[32m   1067\u001b[39m text_outputs: BaseModelOutputWithPooling = \u001b[38;5;28mself\u001b[39m.text_model(\n\u001b[32m   1068\u001b[39m     input_ids=input_ids,\n\u001b[32m   1069\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1070\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   1071\u001b[39m     position_ids=position_ids,\n\u001b[32m   1072\u001b[39m )\n\u001b[32m   1074\u001b[39m pooled_output = text_outputs.pooler_output\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m text_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# Squirtle, Bulbasaur, Charmander, Pikachu in English\n",
    "texts = [\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]\n",
    "\n",
    "# compute image feature\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "image_features = model.get_image_features(**inputs)\n",
    "image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# compute text features\n",
    "inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n",
    "text_features = model.get_text_features(**inputs)\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# compute image-text similarity scores\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # probs: [[0.0219, 0.0316, 0.0043, 0.9423]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
    "class ChineseCLIPFeatureExtractor:\n",
    "    def __init__(self, model_name=\"OFA-Sys/chinese-clip-vit-base-patch16\", device=None, cache_dir=None):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        self.model = ChineseCLIPModel.from_pretrained(model_name, cache_dir=cache_dir, \n",
    "                                                      local_files_only=True\n",
    "                                                     ).to(self.device)\n",
    "        self.processor = ChineseCLIPProcessor.from_pretrained(model_name, cache_dir=cache_dir, \n",
    "                                                              local_files_only=True\n",
    "                                                             )\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "\n",
    "    def extract_text_features(self, text_list):\n",
    "        inputs = self.processor(text=text_list, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        return text_features\n",
    "\n",
    "    def extract_image_features(self, image_list):\n",
    "        inputs = self.processor(images=image_list, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# 初始化特征提取器\n",
    "feature_extractor = ChineseCLIPFeatureExtractor(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sample_queries.empty:\n\u001b[32m      3\u001b[39m     sample_texts = sample_queries[\u001b[33m'\u001b[39m\u001b[33mquery_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     text_features = \u001b[43mfeature_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m提取的文本特征维度: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 提取图像特征\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mChineseCLIPFeatureExtractor.extract_text_features\u001b[39m\u001b[34m(self, text_list)\u001b[39m\n\u001b[32m     19\u001b[39m inputs = {k: v.to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     text_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/transformers/utils/generic.py:809\u001b[39m, in \u001b[36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    800\u001b[39m         cls_prefix = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    802\u001b[39m     warnings.warn(\n\u001b[32m    803\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    804\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    805\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    806\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    807\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/transformers/models/chinese_clip/modeling_chinese_clip.py:1075\u001b[39m, in \u001b[36mChineseCLIPModel.get_text_features\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids)\u001b[39m\n\u001b[32m   1067\u001b[39m text_outputs: BaseModelOutputWithPooling = \u001b[38;5;28mself\u001b[39m.text_model(\n\u001b[32m   1068\u001b[39m     input_ids=input_ids,\n\u001b[32m   1069\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1070\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   1071\u001b[39m     position_ids=position_ids,\n\u001b[32m   1072\u001b[39m )\n\u001b[32m   1074\u001b[39m pooled_output = text_outputs.pooler_output\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m text_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text_features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# 提取文本特征\n",
    "if not sample_queries.empty:\n",
    "    sample_texts = sample_queries['query_text'].tolist()\n",
    "    text_features = feature_extractor.extract_text_features(sample_texts)\n",
    "    print(f\"提取的文本特征维度: {text_features.shape}\")\n",
    "\n",
    "# 提取图像特征\n",
    "if sample_images:\n",
    "    # 确保所有图像都已正确加载\n",
    "    valid_sample_images = [img for img in sample_images if img is not None]\n",
    "    if valid_sample_images:\n",
    "        image_features = image_extractor.extract_image_features(valid_sample_images)\n",
    "        print(f\"提取的图像特征维度: {image_features.shape}\")\n",
    "    else:\n",
    "        print(\"没有有效的图像可供处理。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 总结\n",
    "\n",
    "在这个 Notebook 中，我们成功地：\n",
    "\n",
    "1.  **构建了文本特征提取器**：使用 `bert-base-chinese` 模型，能够将文本查询转换为 768 维的特征向量。\n",
    "2.  **构建了图像特征提取器**：使用 `resnet50` 模型，能够将图像转换为 2048 维的特征向量。\n",
    "3.  **验证了特征提取流程**：在示例数据上成功运行了特征提取，并确认了输出张量的形状。\n",
    "\n",
    "这些特征提取模块是构建基线检索模型的核心组件。下一步，我们将使用这些模块来构建一个完整的检索流程，并进行训练和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
