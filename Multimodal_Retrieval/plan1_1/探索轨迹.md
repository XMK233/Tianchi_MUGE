# 3.1

## 实现模型微调策略，适应电商领域数据

* step_3_1-1_乾-基线：~~【折桂】Recall@1=0.0587, Recall@5=0.1979, Recall@10=0.3071, MeanRecall=0.1879 (N=5008)~~ **启发**：`换好的预训练模型，有用。`
* step_3_1-2_坤-使用微调后模型：基于1，但是用预训练过的模型来接着训。没有提升。
* step_3_1-4_蒙：
    * step_3_1-4_蒙_cp3-基于3_更换池化为attentive：~~【折桂】Recall@1=0.0733, Recall@5=0.2113, Recall@10=0.3091, MeanRecall=0.1979 (N=5008)~~ 些微有点提高。
    * step_3_1-4_蒙_cp1-基于3_更换池化为cls：表现没有很大提高。
* step_3_1-5_需：
    * step_3_1-5_需_cp1-基于4_cp2-解冻4层：~~【折桂】Recall@1=0.0681, Recall@5=0.2272, Recall@10=0.3357, MeanRecall=0.2103 (N=5008)~~
    * step_3_1-5_需_cp2-基于4_cp2-解冻8层：~~【折桂】Recall@1=0.0749, Recall@5=0.2410, Recall@10=0.3506, MeanRecall=0.2222 (N=5008)~~ **启发**：`解冻得越多，越好。后续可以考虑进一步解冻，乃至解冻embedding等层。`
* step_3_1-6_讼：
    * step_3_1-6_讼_cp2-基于5_cp2-换图像模型2：【折桂】Recall@1=0.0801, Recall@5=0.2438, Recall@10=0.3580, MeanRecall=0.2273 (N=5008) 只寻3轮，表现就能提升。后续或可考虑增加训练轮数到5轮。Recall@1=0.0779, Recall@5=0.2392, Recall@10=0.3526, MeanRecall=0.2232 (N=5008) 没有有效用的提升。
    * step_3_1-6_讼_cp1-基于5_cp2-换图像模型1：基于convnext-tiny，表现反为不美，后续考虑缩减训练轮数再试。还是不好。会不会还是过拟合了？


# 2.3

## 基线

表现很普通。

## 解冻轻量微调

* step_2_3-2_坤-解冻轻量微调-v2_amp_faiss：表现有明显提升。

## mean_pooling

* step_2_3-3_屯-mean_pooling：训练5轮 ~~【折桂】Recall@1=0.0571, Recall@5=0.1905, Recall@10=0.2927, MeanRecall=0.1801 (N=5008)~~。不确定这里之所以有比较明显的提升是因为随机性还是其他原因。
* step_2_3-3_屯-mean_pooling-v2：
    * 把projector改复杂了一些，训练10轮，才能让loss降到之前的水准。表现反而显著下降了。
* step_2_3-3_屯-mean_pooling-v2_cp1：
    * 恢复了简单的projector，代码更规整。没有实质改进。
    * 解冻2层训练5轮，表现还好点。解冻多了训练轮数多了反而不好。
* step_2_3-3_屯-mean_pooling-v3：增加了学习率schedule。表现似乎倒退了。
* 

## 可学习温度

总体没用。倒回去。

* step_2_3-4_蒙-两层MLP_可学习温度-v4：较为完整的代码。

## 使用clip

* step_2_3-5_需-CLIP双编码器_最小改动：简单地使用clip。效果很差。