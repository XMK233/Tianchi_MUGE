{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3-2 (v2)：解冻编码器顶层进行轻量微调 + 完整保存/加载\n",
    "\n",
    "在 v1 的基础上，增加对已解冻顶层（BERT 的最后2层 + pooler、ResNet50 的 layer4）以及投影层、优化器状态的完整保存与加载能力，支持继续训练或复现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 环境\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型与特征模块（含评估友好方法与索引构建）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Linear(768, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(2048, projection_dim).to(self.device)\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "train_image_batch_size = 500 # 15000\n",
    "max_train_batches = 1 # 2\n",
    "epochs_per_batch = 1 # 2\n",
    "train_step_batch_size = 32 # 64\n",
    "temperature = 0.07\n",
    "\n",
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "last_n_layers = 2\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=last_n_layers)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "optim = build_optimizer(model, text_extractor, image_extractor,\n",
    "                        lr_proj=1e-3, lr_text_top=5e-5, lr_img_top=1e-4, weight_decay=1e-4)\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环（流式）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    text_extractor.model.encoder.layer[-1].train()\n",
    "    text_extractor.model.encoder.layer[-2].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.train()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            t_feats = text_extractor.encode_with_grad(texts)\n",
    "            i_feats = image_extractor.encode_with_grad(imgs)\n",
    "\n",
    "            t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "            i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "\n",
    "            loss = info_nce_loss(t_proj, i_proj, temp=temperature)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                max_norm=5.0\n",
    "            )\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存：投影层 + 已解冻顶层 + 优化器\n",
    "保存 BERT 的最后2层 + pooler、ResNet50 的 layer4、投影层、优化器，便于继续训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    # 文本：最后两层与 pooler\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    # 图像：layer4\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_2_3_unfreeze_checkpoint.pth')\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, last_n_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载：恢复解冻顶层与投影层权重，继续训练\n",
    "加载后会自动再次执行顶层解冻，并恢复优化器状态（如提供）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    # 恢复投影层\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    # 再次解冻指定层数\n",
    "    ln = ckpt.get('last_n_layers', 2)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    # 恢复文本顶层\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    # 恢复图像顶层\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    # 恢复优化器\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 示例：加载以继续训练\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估：Recall@1/5/10（与 v1 一致，可直接复用）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
