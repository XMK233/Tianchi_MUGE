{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-clip-ft",
   "metadata": {},
   "source": [
    "# CLIP 模型直接微调（对比学习）\n",
    "\n",
    "目标：使用电商数据中的图片与文本对，直接在 CLIP 上进行对比学习微调，并保存微调后的权重供后续检索使用。\n",
    "- 使用项目现有 `DataLoader` 逐批加载图片与查询文本，构造图文对。\n",
    "- 采用 CLIP 的对比损失（图像-文本相互对齐），支持 AMP 降内存与加速。\n",
    "- 提供资源友好的默认参数，适配 32GB RAM / 16GB VRAM。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, BertTokenizer\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# 环境与缓存设置\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "clip_save_path = os.path.join(save_dir, 'ft_clip_vit_base_patch32.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04bd628-ea81-4b94-babc-f257f67bbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    if os.path.exists(\"ft_rsn50_brt.finishflag\"):\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dataset-build",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPPairDataset(Dataset):\n",
    "    def __init__(self, items, queries_texts, image_transform=None):\n",
    "        self.items = items\n",
    "        self.texts = queries_texts\n",
    "        self.tf = image_transform\n",
    "        self.n = min(len(self.items), len(self.texts))\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    def __getitem__(self, idx):\n",
    "        it = self.items[idx]\n",
    "        img = it.get('image', None)\n",
    "        text = self.texts[idx]\n",
    "        if img is None:\n",
    "            img = Image.new('RGB', (224, 224), color=(128,128,128))\n",
    "        if self.tf is not None:\n",
    "            img = self.tf(img)\n",
    "        return img, text\n",
    "\n",
    "def build_clip_processor_model():\n",
    "    processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32', cache_dir=cache_dir, local_files_only=True)\n",
    "    model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32', cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "    return processor, model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 此 collate 不再使用，训练循环里会定义基于 CLIPProcessor 的版本\n",
    "    imgs = [b[0] for b in batch]\n",
    "    texts = [b[1] for b in batch]\n",
    "    return {'pixel_values': torch.stack(imgs, dim=0), 'texts': texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip(loader: DataLoader, device, epochs=1, img_batch_size=800, inner_bs=64, lr=5e-5, max_batches=2):\n",
    "    # 使用 CLIPProcessor 进行文本编码，移除 BERT tokenizer\n",
    "    processor, model = build_clip_processor_model()\n",
    "    scaler = GradScaler(enabled=(device.type=='cuda'))\n",
    "    optim = AdamW(model.parameters(), lr=lr)\n",
    "    # 估算总训练步数用于LR调度（近似即可，有助稳定下降）\n",
    "    total_batches_est = max(1, max_batches or 1)\n",
    "    steps_per_batch_est = max(1, img_batch_size // inner_bs)\n",
    "    total_steps_est = total_batches_est * steps_per_batch_est\n",
    "    warmup_steps = max(1, int(0.2 * total_steps_est))\n",
    "    scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps_est)\n",
    "    image_tf = None\n",
    "\n",
    "    model.train()\n",
    "    # 仅加载一次训练查询，并在各批次循环中重复利用\n",
    "    train_df = loader.load_queries(split='train')\n",
    "    train_texts = [t for t in train_df.get('query_text', []) if isinstance(t, str)]\n",
    "    if len(train_texts) == 0:\n",
    "        return model\n",
    "    text_offset = 0\n",
    "\n",
    "    outer = loader.load_images_batch(split='train', batch_size=img_batch_size, max_batches=max_batches)\n",
    "    for batch_idx, img_batch in enumerate(outer):\n",
    "        # 基于 item_ids 构造当前批次的真实图文正样本对\n",
    "        # 统一键为字符串，避免类型不一致导致无法匹配\n",
    "        img_map = {str(it['img_id']): it['image'] for it in img_batch if it.get('image', None) is not None}\n",
    "        paired_items = []\n",
    "        paired_texts = []\n",
    "        if 'item_ids' in train_df.columns:\n",
    "            for _, row in train_df.iterrows():\n",
    "                q = row.get('query_text', None)\n",
    "                ids = row.get('item_ids', [])\n",
    "                # 兼容字符串形式的列表，如 '[123, 456]' 或 '123,456'\n",
    "                if isinstance(ids, str):\n",
    "                    parsed = None\n",
    "                    try:\n",
    "                        parsed = json.loads(ids)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            parsed = [s.strip() for s in ids.split(',') if s.strip()]\n",
    "                        except Exception:\n",
    "                            parsed = None\n",
    "                    ids = parsed if isinstance(parsed, list) else []\n",
    "                if not q or not isinstance(ids, list) or not ids:\n",
    "                    continue\n",
    "                chosen_img = None\n",
    "                for iid in ids:\n",
    "                    sid = str(iid)\n",
    "                    if sid in img_map:\n",
    "                        chosen_img = img_map[sid]\n",
    "                        break\n",
    "                if chosen_img is not None:\n",
    "                    paired_items.append({'image': chosen_img})\n",
    "                    paired_texts.append(q)\n",
    "        if len(paired_items) == 0:\n",
    "            continue\n",
    "        ds = CLIPPairDataset(paired_items, paired_texts, image_tf)\n",
    "        def collate_clip(batch):\n",
    "            imgs = [b[0] for b in batch]\n",
    "            texts = [b[1] for b in batch]\n",
    "            inputs = processor(text=texts, images=imgs, return_tensors='pt', padding=True, truncation=True)\n",
    "            return {\n",
    "                'pixel_values': inputs['pixel_values'],\n",
    "                'input_ids': inputs['input_ids'],\n",
    "                'attention_mask': inputs.get('attention_mask', None)\n",
    "            }\n",
    "        dl = TorchDataLoader(ds, batch_size=inner_bs, shuffle=True, num_workers=2, pin_memory=(device.type=='cuda'), collate_fn=collate_clip)\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        # pbar = tqdm(dl, desc=f'Train batch {batch_idx+1}', leave=False)\n",
    "        for i, batch in enumerate(dl, 1):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            with autocast(enabled=(device.type=='cuda')):\n",
    "                outputs = model(**batch)\n",
    "                # 直接使用 logits 计算对比损失\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                logits_per_text = outputs.logits_per_text\n",
    "                # 跳过过小微批（对比学习至少需要 >1 的批量用于负样本）\n",
    "                bs = logits_per_image.size(0)\n",
    "                if bs < 2:\n",
    "                    continue\n",
    "                targets = torch.arange(bs, device=device)\n",
    "                # 标签平滑可缓解偶发的损失尖峰\n",
    "                ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "                loss_i = ce(logits_per_image, targets)\n",
    "                loss_t = ce(logits_per_text, targets)\n",
    "                loss = (loss_i + loss_t) / 2.0\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            # 累计与显示损失\n",
    "            running_loss += float(loss.detach().item())\n",
    "            steps += 1\n",
    "            avg = running_loss / max(1, steps)\n",
    "            # pbar.set_postfix({'loss': f'{loss.item():.4f}', 'avg': f'{avg:.4f}'})\n",
    "        current_lr = optim.param_groups[0]['lr']\n",
    "        print(f'Batch {batch_idx+1}: avg loss={running_loss/max(1, steps):.4f} | lr={current_lr:.6f}')\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run-and-save",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:53:09,376 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/tmp/ipykernel_225180/2793667927.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type=='cuda'))\n",
      "2025-11-09 21:53:14,020 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:00, 263029.12it/s]\n",
      "2025-11-09 21:53:15,027 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-09 21:53:15,047 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:   2%|▊                                                   | 1953/129380 [00:00<00:47, 2656.98it/s]/tmp/ipykernel_225180/2793667927.py:73: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(device.type=='cuda')):\n",
      "实际加载train图片数据:   2%|▊                                                   | 2000/129380 [00:24<1:19:29, 26.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: avg loss=2.4462 | lr=0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:   3%|█▌                                                  | 4000/129380 [00:44<1:07:32, 30.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: avg loss=1.9444 | lr=0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:   5%|██▌                                                   | 6000/129380 [01:06<57:54, 35.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: avg loss=1.9106 | lr=0.000029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:   6%|███▎                                                  | 8000/129380 [01:29<57:22, 35.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: avg loss=1.9356 | lr=0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:   8%|████                                                 | 10000/129380 [01:49<52:36, 37.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: avg loss=2.0269 | lr=0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:   9%|████▉                                                | 12000/129380 [02:10<54:01, 36.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: avg loss=2.1035 | lr=0.000059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  11%|█████▋                                               | 14000/129380 [02:30<50:30, 38.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7: avg loss=2.3169 | lr=0.000069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  12%|██████▌                                              | 16000/129380 [02:50<46:48, 40.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: avg loss=2.3365 | lr=0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  14%|███████▎                                             | 18000/129380 [03:12<48:54, 37.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9: avg loss=2.3229 | lr=0.000088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  15%|████████▏                                            | 20000/129380 [03:34<40:43, 44.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: avg loss=2.1658 | lr=0.000097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  17%|█████████                                            | 22000/129380 [03:54<47:22, 37.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: avg loss=2.1579 | lr=0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  19%|█████████▊                                           | 24000/129380 [04:14<44:30, 39.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12: avg loss=2.1948 | lr=0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  20%|██████████▋                                          | 26000/129380 [04:36<48:26, 35.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13: avg loss=2.2168 | lr=0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  22%|███████████▍                                         | 28000/129380 [04:55<41:59, 40.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14: avg loss=2.2234 | lr=0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|████████████▎                                        | 30000/129380 [05:19<50:18, 32.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15: avg loss=2.2096 | lr=0.000097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  25%|█████████████                                        | 32000/129380 [05:42<47:33, 34.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16: avg loss=2.1719 | lr=0.000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  26%|█████████████▉                                       | 34000/129380 [06:04<47:03, 33.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17: avg loss=2.1394 | lr=0.000094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  28%|██████████████▋                                      | 36000/129380 [06:27<46:12, 33.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18: avg loss=2.0526 | lr=0.000092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  29%|███████████████▌                                     | 38000/129380 [06:47<40:00, 38.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19: avg loss=2.0629 | lr=0.000090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  31%|████████████████▍                                    | 40000/129380 [07:08<40:12, 37.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: avg loss=2.0088 | lr=0.000087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  32%|█████████████████▏                                   | 42000/129380 [07:30<40:21, 36.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21: avg loss=1.9842 | lr=0.000085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  34%|██████████████████                                   | 44000/129380 [07:50<38:39, 36.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22: avg loss=1.9719 | lr=0.000082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  36%|██████████████████▊                                  | 46000/129380 [08:11<31:23, 44.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23: avg loss=1.9370 | lr=0.000079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  37%|███████████████████▋                                 | 48000/129380 [08:33<38:20, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24: avg loss=1.9204 | lr=0.000075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  39%|████████████████████▍                                | 50000/129380 [08:53<33:32, 39.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25: avg loss=1.9596 | lr=0.000072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  40%|█████████████████████▎                               | 52000/129380 [09:14<36:09, 35.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 26: avg loss=1.8317 | lr=0.000068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  42%|██████████████████████                               | 54000/129380 [09:35<33:32, 37.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27: avg loss=1.8901 | lr=0.000065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  43%|██████████████████████▉                              | 56000/129380 [09:56<32:48, 37.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28: avg loss=1.9172 | lr=0.000061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  45%|███████████████████████▊                             | 58000/129380 [10:16<30:40, 38.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29: avg loss=1.8597 | lr=0.000058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|████████████████████████▌                            | 60000/129380 [10:35<28:59, 39.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30: avg loss=1.8542 | lr=0.000054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  48%|█████████████████████████▍                           | 62000/129380 [10:56<31:03, 36.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31: avg loss=1.8326 | lr=0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  49%|██████████████████████████▏                          | 64000/129380 [11:20<34:02, 32.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32: avg loss=1.8425 | lr=0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  51%|███████████████████████████                          | 66000/129380 [11:42<30:41, 34.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33: avg loss=1.7960 | lr=0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  53%|███████████████████████████▊                         | 68000/129380 [12:05<29:48, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34: avg loss=1.7470 | lr=0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  54%|████████████████████████████▋                        | 70000/129380 [12:29<31:24, 31.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35: avg loss=1.7178 | lr=0.000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  56%|█████████████████████████████▍                       | 72000/129380 [12:50<23:46, 40.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36: avg loss=1.7270 | lr=0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  57%|██████████████████████████████▎                      | 74000/129380 [13:11<26:27, 34.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37: avg loss=1.7048 | lr=0.000028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  59%|███████████████████████████████▏                     | 76000/129380 [13:34<25:56, 34.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38: avg loss=1.6888 | lr=0.000024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  60%|███████████████████████████████▉                     | 78000/129380 [13:53<22:18, 38.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39: avg loss=1.7002 | lr=0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  62%|████████████████████████████████▊                    | 80000/129380 [14:14<21:31, 38.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: avg loss=1.6862 | lr=0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  63%|█████████████████████████████████▌                   | 82000/129380 [14:34<16:54, 46.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41: avg loss=1.6354 | lr=0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  65%|██████████████████████████████████▍                  | 84000/129380 [14:56<20:31, 36.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42: avg loss=1.6409 | lr=0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  66%|███████████████████████████████████▏                 | 86000/129380 [15:18<21:50, 33.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43: avg loss=1.6673 | lr=0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  68%|████████████████████████████████████                 | 88000/129380 [15:38<20:09, 34.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44: avg loss=1.6074 | lr=0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  69%|████████████████████████████████████                | 89879/129380 [15:39<01:53, 348.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45: avg loss=1.7069 | lr=0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  71%|█████████████████████████████████████▋               | 92000/129380 [16:19<14:00, 44.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 46: avg loss=1.7099 | lr=0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  73%|██████████████████████████████████████▌              | 94149/129380 [16:38<11:58, 49.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 47: avg loss=1.7001 | lr=0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  74%|███████████████████████████████████████▎             | 96000/129380 [16:59<16:38, 33.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 48: avg loss=1.6542 | lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  76%|████████████████████████████████████████▏            | 98000/129380 [17:24<17:26, 29.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49: avg loss=1.6896 | lr=0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  77%|████████████████████████████████████████▏           | 100000/129380 [17:45<13:58, 35.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50: avg loss=1.6758 | lr=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  79%|████████████████████████████████████████▉           | 102000/129380 [18:08<13:39, 33.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51: avg loss=1.7446 | lr=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  80%|█████████████████████████████████████████▊          | 104000/129380 [18:32<10:15, 41.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52: avg loss=1.7369 | lr=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  82%|██████████████████████████████████████████▌         | 106000/129380 [18:52<10:13, 38.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53: avg loss=1.7845 | lr=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  83%|███████████████████████████████████████████▍        | 108000/129380 [19:14<10:26, 34.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54: avg loss=1.7020 | lr=0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  85%|████████████████████████████████████████████▏       | 110000/129380 [19:36<09:34, 33.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55: avg loss=1.7022 | lr=0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  87%|█████████████████████████████████████████████       | 112000/129380 [19:56<07:57, 36.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56: avg loss=1.6692 | lr=0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  88%|█████████████████████████████████████████████▊      | 114000/129380 [20:18<07:10, 35.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 57: avg loss=1.6478 | lr=0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  90%|██████████████████████████████████████████████▌     | 116000/129380 [20:38<06:01, 36.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58: avg loss=1.6099 | lr=0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  91%|███████████████████████████████████████████████▍    | 118000/129380 [20:58<04:46, 39.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 59: avg loss=1.6193 | lr=0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|████████████████████████████████████████████████▏   | 120000/129380 [21:20<04:27, 35.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: avg loss=1.6082 | lr=0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  94%|█████████████████████████████████████████████████   | 122000/129380 [21:41<03:28, 35.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 61: avg loss=1.5765 | lr=0.000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  96%|█████████████████████████████████████████████████▊  | 124000/129380 [22:01<02:24, 37.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 62: avg loss=1.6109 | lr=0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  97%|██████████████████████████████████████████████████▋ | 126000/129380 [22:23<01:32, 36.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63: avg loss=1.5576 | lr=0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  99%|███████████████████████████████████████████████████▍| 128000/129380 [22:43<00:35, 38.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64: avg loss=1.5802 | lr=0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据: 100%|████████████████████████████████████████████████████| 129380/129380 [22:43<00:00, 94.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65: avg loss=1.5692 | lr=0.000023\n",
      "CLIP fine-tuned and saved to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/ft_clip_vit_base_patch32.pth\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "clip_model = train_clip(loader, device=device, img_batch_size=2000, max_batches=100, inner_bs=16, epochs=3, lr=1e-4)\n",
    "torch.save(clip_model.state_dict(), clip_save_path)\n",
    "print(f'CLIP fine-tuned and saved to: {clip_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d57ebb-f4f0-4967-b055-40c1d843f788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eval-recall",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eval-code",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 22:16:26,338 - INFO - Loading faiss with AVX2 support.\n",
      "2025-11-09 22:16:26,408 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-11-09 22:16:26,413 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 349031.66it/s]\n",
      "2025-11-09 22:16:26,433 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 22:16:26,511 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据: 100%|████████████████████████████████████████████████████| 29806/29806 [00:14<00:00, 2053.30it/s]\n",
      "2025-11-09 22:16:43,723 - INFO - 成功创建valid图片映射字典，共29806张图片\n",
      "Build image index: 100%|██████████████████████████████████████████████████████████████| 466/466 [00:42<00:00, 11.08it/s]\n",
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:43<00:00, 115.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0132, Recall@5=0.0537, Recall@10=0.0931, MeanRecall=0.0533 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib, numpy as np\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = importlib.import_module('faiss')\n",
    "        HAS_FAISS = True\n",
    "except BaseException as e:\n",
    "    print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 加载验证集查询\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "# 构建验证集图像字典（可选限制最大样本数）\n",
    "valid_imgs_max_samples = None  # 可改为例如 30000 以降低内存\n",
    "valid_imgs = loader.create_img_id_to_image_dict(split='valid', max_samples=valid_imgs_max_samples)\n",
    "\n",
    "# 使用 CLIP 计算图像特征并构建索引\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32', cache_dir=cache_dir, local_files_only=True)\n",
    "clip_model.eval()\n",
    "def l2_normalize(x):\n",
    "    return torch.nn.functional.normalize(x, p=2, dim=-1)\n",
    "\n",
    "image_index = {}\n",
    "all_ids = list(valid_imgs.keys())\n",
    "batch_size = 64\n",
    "for s in tqdm(range(0, len(all_ids), batch_size), desc='Build image index'):\n",
    "    batch_ids = all_ids[s:s+batch_size]\n",
    "    batch_imgs = [valid_imgs[i] for i in batch_ids if valid_imgs[i] is not None]\n",
    "    valid_batch_ids = [i for i in batch_ids if valid_imgs[i] is not None]\n",
    "    if not batch_imgs:\n",
    "        continue\n",
    "    inputs = processor(images=batch_imgs, return_tensors='pt')\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = clip_model.get_image_features(pixel_values)\n",
    "        feats = l2_normalize(feats).detach().cpu()\n",
    "    for j, img_id in enumerate(valid_batch_ids):\n",
    "        image_index[img_id] = feats[j]\n",
    "\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, clip_model.visual_projection.out_features if hasattr(clip_model, 'visual_projection') else 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    faiss_index.add(all_image_feats.numpy().astype('float32'))\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        inputs = processor(text=[q_text], return_tensors='pt', padding=True)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            q_feat = clip_model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            q_feat = l2_normalize(q_feat)\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = (q_feat @ all_image_feats.t()).squeeze(0)\n",
    "            _, top_idx = torch.topk(sims, k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / max(1, total)) for k in k_values}, total\n",
    "\n",
    "k_values = [1, 5, 10]\n",
    "recall_scores, N = compute_recall_at_k(k_values, valid_queries)\n",
    "mean_recall = sum(recall_scores.values()) / len(k_values)\n",
    "print(f\"Recall@1={recall_scores[1]:.4f}, Recall@5={recall_scores[5]:.4f}, Recall@10={recall_scores[10]:.4f}, MeanRecall={mean_recall:.4f} (N={N})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885be54-5235-447e-b903-884b362c9a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c837b6-3719-43b5-b572-114d22c5343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb66396f-eb4f-488a-a2fe-60cab99113f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ft_clip.finishflag\", \"w\") as f:\n",
    "    f.write(\"finish\")\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "kill_current_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08e588-4c8c-46aa-82c9-91f8b33c4692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899a945-72b4-4ff1-9f7c-376968fc809c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51489676-baef-404d-b1a3-36da0a4306da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}