{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我已在目标 notebook 的文本特征提取器中加入可选池化，并在实例化时切换为 CLS 池化。\n",
    "\n",
    "修改要点\n",
    "- TextFeatureExtractor 支持三种池化：\n",
    "  - `mean`：基于 `attention_mask` 的均值池化（原基线）。\n",
    "  - `cls`：直接取第一个 token 向量作为句向量。\n",
    "  - `attentive`：增加一个线性层生成注意力分数，掩蔽 padding 后做加权求和。\n",
    "- 新增参数 `pooling`，默认为 `mean`，并在 `encode_with_grad` 中根据该参数分支执行。\n",
    "- 在模型初始化处将 `TextFeatureExtractor(..., pooling='cls')`，以便直接对比不同池化。\n",
    "\n",
    "使用建议\n",
    "- 你可以在实例化处改 `pooling` 为 `mean` 或 `attentive` 来做对比。\n",
    "- 若使用 `attentive`，注意该线性层会参与训练，可以在优化器分组时纳入投影头组或单独设置学习率。\n",
    "- 评估时建议保持相同的训练步数、调度器配置与数据划分，便于公平对比。\n",
    "\n",
    "需要我把 `attentive` 的权重也纳入优化器并在日志中标注当前池化方式吗？我可以顺手加上相应的打印与评估摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import importlib\n",
    "\n",
    "# 可选：Faiss 加速检索（安全导入，避免 NumPy 版本问题导致报错中断）\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = importlib.import_module('faiss')\n",
    "        HAS_FAISS = True\n",
    "except BaseException as e:\n",
    "    print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 设置环境变量（与基线一致，按需修改为本地镜像/缓存）\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器（使用 plan1_1/data_loader.py）\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_3_1_1__.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    if os.path.exists(\"step_3_1-6_讼_cp1-基于5_cp2-换图像模型1.finishflag\"):\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型与特征模块\n",
    "文本特征使用基于 `attention_mask` 的 mean-pooling（排除 padding），相比仅用 [CLS] 更稳健。训练时允许梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "# 1. 优化版两层MLP投影头（核心组件）\n",
    "class OptimizedMLPProjector(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim) if use_bn else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "        # Kaiming初始化\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.zeros_(self.layer2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None, pooling='mean'):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)#\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)#, local_files_only=True\n",
    "        self.pooling = pooling\n",
    "        if self.pooling == 'attentive':\n",
    "            self.attn = nn.Linear(768, 1).to(self.device)\n",
    "        # 默认 eval，训练时将对子模块单独切换 train\n",
    "        self.model.eval()\n",
    "        \n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # [B, L, 768]\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if self.pooling == 'cls':\n",
    "            return token_embeddings[:, 0, :]\n",
    "        if self.pooling == 'attentive':\n",
    "            # 运行时再确保注意力层与当前张量设备一致，增强健壮性\n",
    "            self.attn = self.attn.to(token_embeddings.device)\n",
    "            scores = self.attn(token_embeddings).squeeze(-1)  # [B, L]\n",
    "            if attention_mask is not None:\n",
    "                mask_pad = (attention_mask == 0)\n",
    "                scores = scores.masked_fill(mask_pad, torch.finfo(scores.dtype).min)\n",
    "            # 在 FP16 下用 FP32 计算 softmax 更稳健\n",
    "            weights = torch.softmax(scores.to(torch.float32), dim=1).to(scores.dtype).unsqueeze(-1)  # [B, L, 1]\n",
    "            return (token_embeddings * weights).sum(dim=1)\n",
    "        if attention_mask is None:\n",
    "            # 兜底：无 mask 时退化为 CLS\n",
    "            return token_embeddings[:, 0, :]\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(token_embeddings)  # [B, L, 1]\n",
    "        summed = (token_embeddings * mask).sum(dim=1)  # [B, 768]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)        # [B, 1]\n",
    "        mean_pooled = summed / lengths\n",
    "        return mean_pooled\n",
    "\n",
    "# class ImageFeatureExtractor:\n",
    "#     def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "#         self.device = device\n",
    "#         self.model = timm.create_model(\n",
    "#             model_name, pretrained=True, num_classes=0,\n",
    "#             cache_dir=cache_dir\n",
    "#         ).to(device)\n",
    "#         self.model.eval()\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "        \n",
    "#     def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "#         if not images:\n",
    "#             return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "#         tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "#         feats = self.model(tensors)\n",
    "#         return feats\n",
    "# image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "class ImageFeatureExtractor:\n",
    "    '''\n",
    "    改进版，使得timm不要每次都去连huggingface。\n",
    "    '''\n",
    "    def __init__(self, model_name='resnet101', device='cpu', weights_path=None, cache_dir=None):\n",
    "        self.device = device\n",
    "        if weights_path is None:\n",
    "            self.model = timm.create_model(model_name, pretrained=True, num_classes=0, cache_dir=cache_dir)\n",
    "        else:\n",
    "            self.model = timm.create_model(\n",
    "                model_name, pretrained=False, num_classes=0, cache_dir=cache_dir,\n",
    "                pretrained_cfg_overlay={'file': weights_path}\n",
    "            )\n",
    "\n",
    "        if weights_path is not None:\n",
    "            if weights_path.endswith('.safetensors'):\n",
    "                state_dict = load_file(weights_path)\n",
    "            else:\n",
    "                state_dict = torch.load(weights_path, map_location='cpu')\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            in_dim = getattr(self.model, 'num_features', 2048)\n",
    "            return torch.empty((0, in_dim), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "        \n",
    "class FeatureFusion:\n",
    "    # 类作用：将原始文本/图像特征投影到共同的子空间（projection_dim）\n",
    "    # 参数:\n",
    "    # - fusion_method: 融合方式，当前支持 'projection'\n",
    "    # - projection_dim: 目标投影维度\n",
    "    # - device: 设备\n",
    "    # - hidden_dim: 两层 MLP 的中间隐藏层维度\n",
    "    # - dropout: Dropout 概率\n",
    "    # - text_in_dim/image_in_dim: 输入维度（默认文本768/图像2048）\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None, hidden_dim=1024, dropout=0.1, text_in_dim=768, image_in_dim=2048):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout\n",
    "        if fusion_method == 'projection':\n",
    "            # self.text_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(text_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            # self.image_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(image_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            \n",
    "            # self.text_projector = OptimizedMLPProjector(text_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "            # self.image_projector = OptimizedMLPProjector(image_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "\n",
    "            self.text_projector = torch.nn.Linear(text_in_dim, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(image_in_dim, projection_dim).to(self.device)\n",
    "            \n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        # 评估阶段禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.text_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        # 评估阶段禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.image_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n",
    "仅解冻顶层，降低学习率，控制训练稳定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n",
    "\n",
    "# LLRD（Layer-wise LR Decay）优化器构建：为BERT顶层设置逐层衰减学习率\n",
    "def build_llrd_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                         lr_proj: float = 1e-3, lr_text_max: float = 5e-5, lr_img_top: float = 1e-4, decay: float = 0.9,\n",
    "                         last_n_layers: int = 2, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    # 投影层（文本/图像）\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    # 文本顶层：逐层衰减（最顶层lr=lr_text_max，其次乘以decay）\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    # 从顶层到次顶层设置lr\n",
    "    order = 0\n",
    "    for i in range(total_layers - 1, start_idx - 1, -1):\n",
    "        group_lr = lr_text_max * (decay ** order)\n",
    "        params.append({\n",
    "            'params': [p for p in enc.layer[i].parameters() if p.requires_grad],\n",
    "            'lr': group_lr,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "        order += 1\n",
    "    # pooler（若存在），使用与最顶层一致的lr\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        params.append({\n",
    "            'params': [p for p in text_extractor.model.pooler.parameters() if p.requires_grad],\n",
    "            'lr': lr_text_max,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "    # 将注意力池化权重也纳入文本顶层优化\n",
    "    if hasattr(text_extractor, 'attn'):\n",
    "        params.append({\n",
    "            'params': [p for p in text_extractor.attn.parameters() if p.requires_grad],\n",
    "            'lr': lr_text_max,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "    # 图像顶层（layer4）\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        params.append({\n",
    "            'params': [p for p in image_extractor.model.layer4.parameters() if p.requires_grad],\n",
    "            'lr': lr_img_top,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n",
    "\n",
    "# Warmup + Cosine 学习率调度器\n",
    "def build_warmup_cosine_scheduler(optimizer: torch.optim.Optimizer, warmup_ratio: float, min_lr_ratio: float, total_steps: int):\n",
    "    warmup_steps = max(1, int(total_steps * max(0.0, min(warmup_ratio, 0.5))))\n",
    "    min_ratio = max(0.0, min(min_lr_ratio, 1.0))\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # Cosine decay from 1.0 -> min_ratio\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return min_ratio + (1.0 - min_ratio) * cosine\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数\n",
    "保持与基线一致的查询数据加载；图片按批次流式以控制显存。默认使用较小批次以便顺利运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 21:39:30,012 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-10 21:39:30,015 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:00, 253081.56it/s]\n",
      "2025-11-10 21:39:31,072 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-10 21:39:31,085 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 287182.12it/s]\n",
      "2025-11-10 21:39:31,107 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# # 训练与流式参数（默认较小，确保顺利运行；可按需增大）\n",
    "# train_image_batch_size = 500\n",
    "# max_train_batches = 1\n",
    "# epochs_per_batch = 1\n",
    "# train_step_batch_size = 32\n",
    "# valid_imgs_max_samples = 100\n",
    "\n",
    "# 训练与流式参数（按需调整）：实际用\n",
    "train_image_batch_size = 15000 ## 一个大batch有这么多图片样本。\n",
    "max_train_batches = 10 ## 总共加载多少个大batch。\n",
    "epochs_per_batch = 5 ## 每个大batch训练几个epoch。\n",
    "train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "valid_imgs_max_samples = 30000\n",
    "\n",
    "use_amp = True\n",
    "temperature = 0.07\n",
    "\n",
    "# 微调与调度参数\n",
    "last_n_layers = 8  # 顶层解冻层数\n",
    "warmup_ratio = 0.1  # 预热比例\n",
    "min_lr_ratio = 0.1  # 余弦最低学习率相对比例\n",
    "use_grad_checkpoint = False  # 可选：启用BERT梯度检查点以降低显存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型并执行顶层解冻\n",
    "解冻文本最后2层与池化；解冻图像 `layer4`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 21:39:31,355 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet101.a1h_in1k)\n",
      "2025-11-10 21:39:31,761 - INFO - [timm/resnet101.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350046/1299514826.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(\n",
    "    device=device\n",
    ")\n",
    "text_extractor = TextFeatureExtractor(\n",
    "    model_name = \"hfl/chinese-roberta-wwm-ext\", \n",
    "    device=device, \n",
    "    cache_dir=cache_dir,\n",
    "    pooling='attentive'\n",
    ")\n",
    "\n",
    "\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "# 可选：启用BERT梯度检查点以降低显存\n",
    "if use_grad_checkpoint and hasattr(text_extractor.model, 'gradient_checkpointing_enable'):\n",
    "    text_extractor.model.gradient_checkpointing_enable()\n",
    "\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=last_n_layers)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "# 使用LLRD优化器：为文本顶层设置逐层衰减的学习率\n",
    "optim = build_llrd_optimizer(model, text_extractor, image_extractor,\n",
    "                             lr_proj=1e-3, lr_text_max=5e-5, lr_img_top=1e-4, decay=0.9,\n",
    "                             last_n_layers=last_n_layers, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环：按批次流式构建配对并微调顶层\n",
    "仅使用配对中的第一张可用图片；文本与图像编码器顶层参与反向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 21:39:34,527 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:  12%|█████▉                                             | 14957/129380 [00:06<00:47, 2417.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images=15000, usable_pairs=29127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350046/3641466815.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "实际加载train图片数据:  12%|█████▉                                             | 14957/129380 [00:16<00:47, 2417.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=1.6310\n",
      "Epoch 2/5: avg loss=0.6379\n",
      "Epoch 3/5: avg loss=0.1736\n",
      "Epoch 4/5: avg loss=0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  12%|█████▊                                            | 15000/129380 [04:28<13:34:43,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|███████████▊                                       | 29869/129380 [04:34<00:41, 2376.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: images=15000, usable_pairs=28909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|███████████▊                                       | 29869/129380 [04:46<00:41, 2376.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=1.1298\n",
      "Epoch 2/5: avg loss=0.3387\n",
      "Epoch 3/5: avg loss=0.0758\n",
      "Epoch 4/5: avg loss=0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|███████████▊                                       | 30000/129380 [08:33<9:23:31,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 44897/129380 [08:38<00:28, 2971.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: images=15000, usable_pairs=28595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 44897/129380 [08:56<00:28, 2971.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=1.0503\n",
      "Epoch 2/5: avg loss=0.2910\n",
      "Epoch 3/5: avg loss=0.0624\n",
      "Epoch 4/5: avg loss=0.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 45000/129380 [12:36<6:42:24,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▌                           | 59781/129380 [12:41<00:22, 3088.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: images=15000, usable_pairs=29155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▌                           | 59781/129380 [12:56<00:22, 3088.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=1.0133\n",
      "Epoch 2/5: avg loss=0.2643\n",
      "Epoch 3/5: avg loss=0.0546\n",
      "Epoch 4/5: avg loss=0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▋                           | 60000/129380 [16:43<4:57:58,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▍                     | 74824/129380 [16:49<00:18, 2914.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: images=15000, usable_pairs=29347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▍                     | 74824/129380 [17:06<00:18, 2914.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=0.9958\n",
      "Epoch 2/5: avg loss=0.2571\n",
      "Epoch 3/5: avg loss=0.0512\n",
      "Epoch 4/5: avg loss=0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▌                     | 75000/129380 [20:55<4:16:39,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  69%|███████████████████████████████████▍               | 89880/129380 [21:00<00:12, 3088.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: images=15000, usable_pairs=29191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  69%|███████████████████████████████████▍               | 89880/129380 [21:17<00:12, 3088.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=0.9564\n",
      "Epoch 2/5: avg loss=0.2370\n",
      "Epoch 3/5: avg loss=0.0516\n",
      "Epoch 4/5: avg loss=0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  70%|███████████████████████████████████▍               | 90000/129380 [25:00<3:03:02,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 104910/129380 [25:04<00:07, 3341.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7: images=15000, usable_pairs=28504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 104910/129380 [25:17<00:07, 3341.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=0.9384\n",
      "Epoch 2/5: avg loss=0.2257\n",
      "Epoch 3/5: avg loss=0.0435\n",
      "Epoch 4/5: avg loss=0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 105000/129380 [29:05<1:50:10,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|██████████████████████████████████████████████▎   | 119982/129380 [29:09<00:02, 3406.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: images=15000, usable_pairs=28894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|██████████████████████████████████████████████▎   | 119982/129380 [29:27<00:02, 3406.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: avg loss=0.9161\n",
      "Epoch 2/5: avg loss=0.2158\n",
      "Epoch 3/5: avg loss=0.0436\n",
      "Epoch 4/5: avg loss=0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|████████████████████████████████████████████████▏   | 120000/129380 [33:09<44:23,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据: 100%|████████████████████████████████████████████████████| 129380/129380 [33:12<00:00, 64.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9: images=9380, usable_pairs=18432\n",
      "Epoch 1/5: avg loss=0.9062\n",
      "Epoch 2/5: avg loss=0.1715\n",
      "Epoch 3/5: avg loss=0.0358\n",
      "Epoch 4/5: avg loss=0.0211\n",
      "Epoch 5/5: avg loss=0.0175\n"
     ]
    }
   ],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    text_extractor.model.encoder.layer[-1].train()\n",
    "    text_extractor.model.encoder.layer[-2].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.train()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.train()\n",
    "\n",
    "    # 为当前大batch构建 Warmup+Cosine 学习率调度器（按总steps）\n",
    "    steps_per_epoch = math.ceil(len(pairs) / max(1, step_bs))\n",
    "    total_steps = epochs * max(1, steps_per_epoch)\n",
    "    scheduler = build_warmup_cosine_scheduler(optim, warmup_ratio=warmup_ratio, min_lr_ratio=min_lr_ratio, total_steps=total_steps)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            optim.zero_grad()\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    loss = info_nce_loss(t_proj, i_proj, temp=temperature)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                loss = info_nce_loss(t_proj, i_proj, temp=temperature)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "            # if (steps % 100) == 0:\n",
    "            #     print('Current LRs:', [pg['lr'] for pg in optim.param_groups])\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载图片与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存：投影层 + 已解冻顶层 + 优化器\n",
    "保存 BERT 的最后2层 + pooler、ResNet50 的 layer4、投影层、优化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_3_1_1__.pth\n"
     ]
    }
   ],
   "source": [
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "# 保存一次\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, last_n_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载：恢复解冻顶层与投影层权重，继续训练\n",
    "加载后会自动再次执行顶层解冻，并恢复优化器状态（如提供）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    ln = ckpt.get('last_n_layers', last_n_layers)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 测试加载\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 22:15:35,142 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据: 100%|████████████████████████████████████████████████████| 29806/29806 [00:15<00:00, 1985.10it/s]\n",
      "2025-11-10 22:15:53,009 - INFO - 成功创建valid图片映射字典，共29806张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:34<00:00, 147.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0779, Recall@5=0.2392, Recall@10=0.3526, MeanRecall=0.2232 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=valid_imgs_max_samples,\n",
    "    # batch_size=3000,\n",
    "    # max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"step_3_1-6_讼_cp2-基于5_cp2-换图像模型2.finishflag\", \"w\") as f:\n",
    "    f.write(\"finish\")\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "kill_current_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
