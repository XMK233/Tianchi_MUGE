{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3-4：两层MLP投影头 + 可学习温度 + AMP/FAISS (蒙)\n",
    "\n",
    "依据 `2.3的改进方案.md` 的第三优先：\n",
    "- 将投影头升级为两层 MLP（Linear → GELU → Dropout → Linear）\n",
    "- 引入可学习温度（logit_scale），替代固定常数 temperature\n",
    "- 保留 AMP 训练加速与 FAISS 检索回退逻辑，保证可运行且稳定\n",
    "\n",
    "在 `step_2_3-3_屯-mean_pooling.ipynb` 基础上实现，文本特征使用 attention_mask 的 mean-pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS available: using accelerated index.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AMP\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 安全导入 FAISS（不可用则回退，并打印清晰日志）\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    import importlib.util\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = __import__('faiss')\n",
    "        HAS_FAISS = True\n",
    "        print('FAISS available: using accelerated index.')\n",
    "    else:\n",
    "        print('FAISS not found: falling back to torch similarity.')\n",
    "except Exception as e:\n",
    "    print(f'FAISS import failed: {e.__class__.__name__}. Fallback to torch.')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 环境与缓存\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install numpy==1.25.2\n",
    "## 只有用1.25.2的numpy才能用得了faiss。\n",
    "## 如果出了什么问题，我们就换回2.3.4版本的numpy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型模块\n",
    "- 文本特征：attention_mask mean-pooling\n",
    "- 投影头：两层 MLP（含 GELU + Dropout）\n",
    "- 可学习温度：logit_scale 参数，训练中联合优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        # 优先本地加载，失败则远程镜像加载\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "            self.model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        except Exception:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=False)\n",
    "            self.model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=False).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        inputs = self.tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=32, return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # [B, L, 768]\n",
    "        attention_mask = inputs['attention_mask']     # [B, L]\n",
    "        # mean-pooling with attention mask\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(token_embeddings)  # [B, L, 1]\n",
    "        summed = (token_embeddings * mask).sum(dim=1)  # [B, 768]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)        # [B, 1]\n",
    "        mean_pooled = summed / lengths\n",
    "        return mean_pooled\n",
    "\n",
    "# from safetensors.torch import load_file\n",
    "# class ImageFeatureExtractor:\n",
    "#     '''\n",
    "#     改进版，使得timm不要每次都去连huggingface。\n",
    "#     '''\n",
    "#     def __init__(self, model_name='resnet50', device='cpu', weights_path=None, cache_dir=None):\n",
    "#         self.device = device\n",
    "#         self.model = timm.create_model(model_name, pretrained=False, num_classes=0, cache_dir=cache_dir)\n",
    "\n",
    "#         if weights_path is not None:\n",
    "#             if weights_path.endswith('.safetensors'):\n",
    "#                 state_dict = load_file(weights_path)\n",
    "#             else:\n",
    "#                 state_dict = torch.load(weights_path, map_location='cpu')\n",
    "#             self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "#         self.model = self.model.to(device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "\n",
    "#     def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "#         if not images:\n",
    "#             in_dim = getattr(self.model, 'num_features', 2048)\n",
    "#             return torch.empty((0, in_dim), dtype=torch.float32, device=self.device)\n",
    "#         tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "#         feats = self.model(tensors)\n",
    "#         return feats\n",
    "# image_extractor = ImageFeatureExtractor(\n",
    "#     device=device, \n",
    "#     weights_path=\"/mnt/d/HuggingFaceModels/models--timm--resnet50.a1_in1k/snapshots/767268603ca0cb0bfe326fa87277f19c419566ef/model.safetensors\"\n",
    "# )\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None, hidden_dim=1024, dropout=0.1, text_in_dim=768, image_in_dim=2048):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Sequential(\n",
    "                torch.nn.Linear(text_in_dim, hidden_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Dropout(p=dropout),\n",
    "                torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            ).to(self.device)\n",
    "            self.image_projector = torch.nn.Sequential(\n",
    "                torch.nn.Linear(image_in_dim, hidden_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Dropout(p=dropout),\n",
    "                torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            ).to(self.device)\n",
    "\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # 数值稳健化：去除 NaN/Inf 并在归一化中使用 eps\n",
    "        f = torch.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(f, p=2, dim=1, eps=1e-6)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        # 动态获取输入维度，避免环境差异导致维度不符\n",
    "        text_in_dim = getattr(text_extractor.model.config, 'hidden_size', 768)\n",
    "        image_in_dim = getattr(image_extractor.model, 'num_features', 2048)\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device, text_in_dim=text_in_dim, image_in_dim=image_in_dim)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # 可学习温度：logit_scale，取值经 exp 后再取倒数作为 temperature\n",
    "        init_temp = 0.07\n",
    "        self.logit_scale = torch.nn.Parameter(torch.tensor(math.log(1.0 / init_temp), dtype=torch.float32))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.normalize_features:\n",
    "            return x\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "    def current_temperature(self) -> torch.Tensor:\n",
    "        # 将 logit_scale 转换为正的温度，并进行合理范围裁剪\n",
    "        temp = 1.0 / torch.exp(self.logit_scale)\n",
    "        return torch.clamp(temp, min=1e-3, max=10.0)\n",
    "\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temperature: torch.Tensor) -> torch.Tensor:\n",
    "    # 数值防护：去除 NaN/Inf 并在半精度下转 float32 防溢出\n",
    "    t = torch.nan_to_num(text_feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    i = torch.nan_to_num(image_feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    logits = torch.mm(t, i.t()).float() / temperature.float()\n",
    "    # 限幅，避免极端值导致 softmax 溢出或 NaN\n",
    "    logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    logits = torch.clamp(logits, -100.0, 100.0)\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n",
    "\n",
    "class MemoryBank:\n",
    "    def __init__(self, dim: int, size: int = 65536, device: str = 'cpu'):\n",
    "        self.size = size\n",
    "        self.ptr = 0\n",
    "        self.device = device\n",
    "        self.bank = torch.zeros((size, dim), dtype=torch.float32, device=device)\n",
    "    def add(self, feats: torch.Tensor):\n",
    "        if feats is None or feats.numel() == 0:\n",
    "            return\n",
    "        b = feats.shape[0]\n",
    "        feats = torch.nn.functional.normalize(torch.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0), p=2, dim=1, eps=1e-6)\n",
    "        if b >= self.size:\n",
    "            self.bank = feats[-self.size:].detach().to(self.device)\n",
    "            self.ptr = 0\n",
    "            return\n",
    "        end = (self.ptr + b) % self.size\n",
    "        if self.ptr + b <= self.size:\n",
    "            self.bank[self.ptr:self.ptr+b] = feats.detach()\n",
    "        else:\n",
    "            part1 = self.size - self.ptr\n",
    "            self.bank[self.ptr:] = feats[:part1].detach()\n",
    "            self.bank[:end] = feats[part1:].detach()\n",
    "        self.ptr = end\n",
    "    def get(self, k: int) -> torch.Tensor:\n",
    "        if self.bank.numel() == 0:\n",
    "            return torch.empty((0, 0), device=self.device)\n",
    "        valid = min(self.size, k)\n",
    "        return self.bank[:valid]\n",
    "\n",
    "def info_nce_with_memory(text_feats: torch.Tensor, image_feats: torch.Tensor, temperature: torch.Tensor, img_memory: MemoryBank = None, txt_memory: MemoryBank = None, neg_k: int = 2048) -> torch.Tensor:\n",
    "    t = torch.nn.functional.normalize(torch.nan_to_num(text_feats, nan=0.0, posinf=0.0, neginf=0.0), p=2, dim=1, eps=1e-6)\n",
    "    i = torch.nn.functional.normalize(torch.nan_to_num(image_feats, nan=0.0, posinf=0.0, neginf=0.0), p=2, dim=1, eps=1e-6)\n",
    "    logits = torch.mm(t, i.t()).float() / temperature.float()\n",
    "    logits = torch.clamp(torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4), -100.0, 100.0)\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_main = 0.5 * (torch.nn.functional.cross_entropy(logits, labels) + torch.nn.functional.cross_entropy(logits.t(), labels))\n",
    "    loss_mem = 0.0\n",
    "    if img_memory is not None:\n",
    "        i_mem = img_memory.get(neg_k)\n",
    "        if i_mem.numel() > 0:\n",
    "            i_mem = torch.nn.functional.normalize(i_mem, p=2, dim=1, eps=1e-6)\n",
    "            logits_t_mem = torch.mm(t, i_mem.t()).float() / temperature.float()\n",
    "            logits_t_mem = torch.clamp(torch.nan_to_num(logits_t_mem, nan=0.0, posinf=1e4, neginf=-1e4), -100.0, 100.0)\n",
    "            labels_t = torch.arange(logits_t_mem.size(0), device=logits_t_mem.device)\n",
    "            loss_mem += torch.nn.functional.cross_entropy(logits_t_mem, labels_t)\n",
    "    if txt_memory is not None:\n",
    "        t_mem = txt_memory.get(neg_k)\n",
    "        if t_mem.numel() > 0:\n",
    "            t_mem = torch.nn.functional.normalize(t_mem, p=2, dim=1, eps=1e-6)\n",
    "            logits_i_mem = torch.mm(i, t_mem.t()).float() / temperature.float()\n",
    "            logits_i_mem = torch.clamp(torch.nan_to_num(logits_i_mem, nan=0.0, posinf=1e4, neginf=-1e4), -100.0, 100.0)\n",
    "            labels_i = torch.arange(logits_i_mem.size(0), device=logits_i_mem.device)\n",
    "            loss_mem += torch.nn.functional.cross_entropy(logits_i_mem, labels_i)\n",
    "    return loss_main + 0.5 * loss_mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n",
    "- 解冻 BERT 顶层（最后2层 + pooler）与 ResNet layer4\n",
    "- 优化器包含：两层MLP投影头参数、已解冻的顶层参数、logit_scale 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, lr_logit_scale: float = 1e-3, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    # 两层MLP投影头\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    # 文本顶层\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    # 图像顶层\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    # 可学习温度参数\n",
    "    params.append({\n",
    "        'params': [model.logit_scale],\n",
    "        'lr': lr_logit_scale,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 14:34:51,053 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-08 14:34:51,054 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:01, 244943.29it/s]\n",
      "2025-11-08 14:34:52,142 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-08 14:34:52,155 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 261637.89it/s]\n",
      "2025-11-08 14:34:52,178 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# 训练与流式参数（默认较小，确保顺利运行；可按需增大）\n",
    "train_image_batch_size = 500\n",
    "max_train_batches = 1\n",
    "epochs_per_batch = 1\n",
    "train_step_batch_size = 32\n",
    "valid_imgs_max_samples = 100\n",
    "\n",
    "# # 训练与流式参数（按需调整）：实际用\n",
    "# train_image_batch_size = 15000 ## 一个大batch有这么多图片样本。\n",
    "# max_train_batches = 10 ## 总共加载多少个大batch。\n",
    "# epochs_per_batch = 10 ## 每个大batch训练几个epoch。\n",
    "# train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "# valid_imgs_max_samples = 30000\n",
    "\n",
    "use_amp = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型与优化器，并进行顶层解冻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 14:34:52,345 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-08 14:34:53,172 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_726243/4183273530.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=2)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "optim = build_optimizer(model, text_extractor, image_extractor,\n",
    "                        lr_proj=1e-3, lr_text_top=5e-5, lr_img_top=1e-4, lr_logit_scale=1e-3, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环（流式）\n",
    "- 构建 (query, image) 配对\n",
    "- AMP 加速与梯度裁剪\n",
    "- 使用可学习温度 model.current_temperature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulation steps: 1, use_memory_loss: False, negatives_k: 4096, memory_size: 65536, proj_dim: 512\n"
     ]
    }
   ],
   "source": [
    "# 记忆队列与梯度累积参数（仅当前笔记本修改，不影响其他文件）\n",
    "accum_steps = 1\n",
    "use_memory_loss = False\n",
    "negatives_k = 4096\n",
    "memory_size = 65536\n",
    "proj_dim = model.fusion.projection_dim\n",
    "img_memory = MemoryBank(dim=proj_dim, size=memory_size, device=device) if use_memory_loss else None\n",
    "txt_memory = MemoryBank(dim=proj_dim, size=memory_size, device=device) if use_memory_loss else None\n",
    "print(f'Accumulation steps: {accum_steps}, use_memory_loss: {use_memory_loss}, negatives_k: {negatives_k}, memory_size: {memory_size}, proj_dim: {proj_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 14:34:56,186 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:   0%|                                                     | 301/129380 [00:00<00:42, 3005.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images=500, usable_pairs=997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_726243/755022051.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "实际加载train图片数据:   0%|▏                                                      | 499/129380 [00:06<29:19, 73.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: avg loss=2.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    text_extractor.model.encoder.layer[-1].train()\n",
    "    text_extractor.model.encoder.layer[-2].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.train()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.train()\n",
    "\n",
    "    # 本地容错参数，防止未执行初始化单元时出错\n",
    "    accum_steps_local = globals().get('accum_steps', 1)\n",
    "    negatives_k_local = globals().get('negatives_k', 0)\n",
    "    img_memory_local = globals().get('img_memory', None)\n",
    "    txt_memory_local = globals().get('txt_memory', None)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            # 梯度累积：仅在每 accum_steps_local 次才清零\n",
    "            if steps % accum_steps_local == 0:\n",
    "                optim.zero_grad()\n",
    "            temp = model.current_temperature()\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    # 先计算未缩放的原始损失，再用于统计\n",
    "                    if globals().get('use_memory_loss', False) and 'info_nce_with_memory' in globals():\n",
    "                        loss_raw = info_nce_with_memory(t_proj, i_proj, temperature=temp, img_memory=img_memory_local, txt_memory=txt_memory_local, neg_k=negatives_k_local)\n",
    "                    else:\n",
    "                        loss_raw = info_nce_loss(t_proj, i_proj, temperature=temp)\n",
    "                    # 累积梯度时按步数缩放反传损失，避免有效学习率膨胀\n",
    "                    loss = loss_raw / max(1, accum_steps_local)\n",
    "                scaler.scale(loss).backward()\n",
    "                # 梯度累积：仅在每 accum_steps_local 次执行优化器步进\n",
    "                if (steps + 1) % accum_steps_local == 0:\n",
    "                    scaler.unscale_(optim)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                        max_norm=5.0\n",
    "                    )\n",
    "                    scaler.step(optim)\n",
    "                    scaler.update()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                # 先计算未缩放的原始损失，再用于统计\n",
    "                if globals().get('use_memory_loss', False) and 'info_nce_with_memory' in globals():\n",
    "                    loss_raw = info_nce_with_memory(t_proj, i_proj, temperature=temp, img_memory=img_memory_local, txt_memory=txt_memory_local, neg_k=negatives_k_local)\n",
    "                else:\n",
    "                    loss_raw = info_nce_loss(t_proj, i_proj, temperature=temp)\n",
    "                # 累积梯度时按步数缩放反传损失，避免有效学习率膨胀\n",
    "                loss = loss_raw / max(1, accum_steps_local)\n",
    "                loss.backward()\n",
    "                # 梯度累积：仅在每 accum_steps_local 次执行优化器步进\n",
    "                if (steps + 1) % accum_steps_local == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                        max_norm=5.0\n",
    "                    )\n",
    "                    optim.step()\n",
    "            # 更新记忆队列（使用当前 batch 的投影特征），仅在启用记忆损失时\n",
    "            if globals().get('use_memory_loss', False):\n",
    "                try:\n",
    "                    if img_memory_local is not None:\n",
    "                        img_memory_local.add(i_proj.detach())\n",
    "                    if txt_memory_local is not None:\n",
    "                        txt_memory_local.add(t_proj.detach())\n",
    "                except Exception as _:\n",
    "                    pass\n",
    "            # 统计使用未缩放的原始损失，便于观察下降趋势\n",
    "            running_loss += loss_raw.item()\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载检查点\n",
    "保存两层MLP投影头、解冻顶层与优化器，并记录 logit_scale（可学习温度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_2_3_4_mlp_temp_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_2_3_4_mlp_temp_checkpoint.pth')\n",
    "\n",
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'logit_scale': model.logit_scale.detach().cpu(),\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    if 'logit_scale' in ckpt:\n",
    "        model.logit_scale.data = ckpt['logit_scale'].to(model.logit_scale.device)\n",
    "    ln = ckpt.get('last_n_layers', 2)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 保存一次并测试加载\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, 2)\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "优先使用 FAISS；不可用则回退到 Torch 相似度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 14:36:29,315 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据:  99%|████████████████████████████████████████████████████████▍| 99/100 [00:00<00:00, 3316.42it/s]\n",
      "2025-11-08 14:36:33,367 - INFO - 成功创建valid图片映射字典，共100张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:21<00:00, 237.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0004, Recall@5=0.0018, Recall@10=0.0038, MeanRecall=0.0020 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=valid_imgs_max_samples, \n",
    "    batch_size=3000, max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
