{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import importlib\n",
    "\n",
    "# 可选：Faiss 加速检索（安全导入，避免 NumPy 版本问题导致报错中断）\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = importlib.import_module('faiss')\n",
    "        HAS_FAISS = True\n",
    "except BaseException as e:\n",
    "    print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 设置环境变量（与基线一致，按需修改为本地镜像/缓存）\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器（使用 plan1_1/data_loader.py）\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_3_1_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    if os.path.exists(\"step_2_3-3_屯-mean_pooling-v2_cp1.finishflag\"):\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型与特征模块\n",
    "文本特征使用基于 `attention_mask` 的 mean-pooling（排除 padding），相比仅用 [CLS] 更稳健。训练时允许梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "# 1. 优化版两层MLP投影头（核心组件）\n",
    "class OptimizedMLPProjector(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim) if use_bn else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "        # Kaiming初始化\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.zeros_(self.layer2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        # 默认 eval，训练时将对子模块单独切换 train\n",
    "        self.model.eval()\n",
    "        \n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # [B, L, 768]\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is None:\n",
    "            # 兜底：无 mask 时退化为 CLS\n",
    "            return token_embeddings[:, 0, :]\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(token_embeddings)  # [B, L, 1]\n",
    "        summed = (token_embeddings * mask).sum(dim=1)  # [B, 768]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)        # [B, 1]\n",
    "        mean_pooled = summed / lengths\n",
    "        return mean_pooled\n",
    "\n",
    "# class ImageFeatureExtractor:\n",
    "#     def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "#         self.device = device\n",
    "#         self.model = timm.create_model(\n",
    "#             model_name, pretrained=True, num_classes=0,\n",
    "#             cache_dir=cache_dir\n",
    "#         ).to(device)\n",
    "#         self.model.eval()\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "        \n",
    "#     def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "#         if not images:\n",
    "#             return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "#         tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "#         feats = self.model(tensors)\n",
    "#         return feats\n",
    "# image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "class ImageFeatureExtractor:\n",
    "    '''\n",
    "    改进版，使得timm不要每次都去连huggingface。\n",
    "    '''\n",
    "    def __init__(self, model_name='resnet50', device='cpu', weights_path=None, cache_dir=None):\n",
    "        self.device = device\n",
    "        self.model = timm.create_model(model_name, pretrained=False, num_classes=0, cache_dir=cache_dir)\n",
    "\n",
    "        if weights_path is not None:\n",
    "            if weights_path.endswith('.safetensors'):\n",
    "                state_dict = load_file(weights_path)\n",
    "            else:\n",
    "                state_dict = torch.load(weights_path, map_location='cpu')\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            in_dim = getattr(self.model, 'num_features', 2048)\n",
    "            return torch.empty((0, in_dim), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "        \n",
    "class FeatureFusion:\n",
    "    # 类作用：将原始文本/图像特征投影到共同的子空间（projection_dim）\n",
    "    # 参数:\n",
    "    # - fusion_method: 融合方式，当前支持 'projection'\n",
    "    # - projection_dim: 目标投影维度\n",
    "    # - device: 设备\n",
    "    # - hidden_dim: 两层 MLP 的中间隐藏层维度\n",
    "    # - dropout: Dropout 概率\n",
    "    # - text_in_dim/image_in_dim: 输入维度（默认文本768/图像2048）\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None, hidden_dim=1024, dropout=0.1, text_in_dim=768, image_in_dim=2048):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout\n",
    "        if fusion_method == 'projection':\n",
    "            # self.text_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(text_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            # self.image_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(image_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            \n",
    "            # self.text_projector = OptimizedMLPProjector(text_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "            # self.image_projector = OptimizedMLPProjector(image_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "\n",
    "            self.text_projector = torch.nn.Linear(text_in_dim, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(image_in_dim, projection_dim).to(self.device)\n",
    "            \n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        # 评估阶段禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.text_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        # 评估阶段禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.image_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n",
    "仅解冻顶层，降低学习率，控制训练稳定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n",
    "\n",
    "# LLRD（Layer-wise LR Decay）优化器构建：为BERT顶层设置逐层衰减学习率\n",
    "def build_llrd_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                         lr_proj: float = 1e-3, lr_text_max: float = 5e-5, lr_img_top: float = 1e-4, decay: float = 0.9,\n",
    "                         last_n_layers: int = 2, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    # 投影层（文本/图像）\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    # 文本顶层：逐层衰减（最顶层lr=lr_text_max，其次乘以decay）\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    # 从顶层到次顶层设置lr\n",
    "    order = 0\n",
    "    for i in range(total_layers - 1, start_idx - 1, -1):\n",
    "        group_lr = lr_text_max * (decay ** order)\n",
    "        params.append({\n",
    "            'params': [p for p in enc.layer[i].parameters() if p.requires_grad],\n",
    "            'lr': group_lr,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "        order += 1\n",
    "    # pooler（若存在），使用与最顶层一致的lr\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        params.append({\n",
    "            'params': [p for p in text_extractor.model.pooler.parameters() if p.requires_grad],\n",
    "            'lr': lr_text_max,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "    # 图像顶层（layer4）\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        params.append({\n",
    "            'params': [p for p in image_extractor.model.layer4.parameters() if p.requires_grad],\n",
    "            'lr': lr_img_top,\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n",
    "\n",
    "# Warmup + Cosine 学习率调度器\n",
    "def build_warmup_cosine_scheduler(optimizer: torch.optim.Optimizer, warmup_ratio: float, min_lr_ratio: float, total_steps: int):\n",
    "    warmup_steps = max(1, int(total_steps * max(0.0, min(warmup_ratio, 0.5))))\n",
    "    min_ratio = max(0.0, min(min_lr_ratio, 1.0))\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # Cosine decay from 1.0 -> min_ratio\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return min_ratio + (1.0 - min_ratio) * cosine\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数\n",
    "保持与基线一致的查询数据加载；图片按批次流式以控制显存。默认使用较小批次以便顺利运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:47:23,214 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-09 16:47:23,216 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:00, 285464.12it/s]\n",
      "2025-11-09 16:47:24,159 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-09 16:47:24,170 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 329729.29it/s]\n",
      "2025-11-09 16:47:24,190 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# # 训练与流式参数（默认较小，确保顺利运行；可按需增大）\n",
    "# train_image_batch_size = 500\n",
    "# max_train_batches = 1\n",
    "# epochs_per_batch = 1\n",
    "# train_step_batch_size = 32\n",
    "# valid_imgs_max_samples = 100\n",
    "\n",
    "# 训练与流式参数（按需调整）：实际用\n",
    "train_image_batch_size = 15000 ## 一个大batch有这么多图片样本。\n",
    "max_train_batches = 10 ## 总共加载多少个大batch。\n",
    "epochs_per_batch = 5 ## 每个大batch训练几个epoch。\n",
    "train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "valid_imgs_max_samples = 30000\n",
    "\n",
    "use_amp = True\n",
    "temperature = 0.07\n",
    "\n",
    "# 微调与调度参数\n",
    "last_n_layers = 2  # 顶层解冻层数\n",
    "warmup_ratio = 0.1  # 预热比例\n",
    "min_lr_ratio = 0.1  # 余弦最低学习率相对比例\n",
    "use_grad_checkpoint = False  # 可选：启用BERT梯度检查点以降低显存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型并执行顶层解冻\n",
    "解冻文本最后2层与池化；解冻图像 `layer4`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25321/3153992814.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(\n",
    "    device=device, \n",
    "    weights_path=\"/mnt/d/HuggingFaceModels/models--timm--resnet50.a1_in1k/snapshots/767268603ca0cb0bfe326fa87277f19c419566ef/model.safetensors\"\n",
    ")\n",
    "\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "# 可选：启用BERT梯度检查点以降低显存\n",
    "if use_grad_checkpoint and hasattr(text_extractor.model, 'gradient_checkpointing_enable'):\n",
    "    text_extractor.model.gradient_checkpointing_enable()\n",
    "\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=last_n_layers)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "# 使用LLRD优化器：为文本顶层设置逐层衰减的学习率\n",
    "optim = build_llrd_optimizer(model, text_extractor, image_extractor,\n",
    "                             lr_proj=1e-3, lr_text_max=5e-5, lr_img_top=1e-4, decay=0.9,\n",
    "                             last_n_layers=last_n_layers, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环：按批次流式构建配对并微调顶层\n",
    "仅使用配对中的第一张可用图片；文本与图像编码器顶层参与反向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:47:25,827 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:  11%|█████▊                                             | 14809/129380 [00:06<00:42, 2723.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images=15000, usable_pairs=29127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25321/2067923214.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.00021978021978021978, 1.0989010989010989e-05, 9.89010989010989e-06, 1.0989010989010989e-05, 2.1978021978021977e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  11%|█████▊                                             | 14809/129380 [00:17<00:42, 2723.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.00043956043956043956, 2.1978021978021977e-05, 1.978021978021978e-05, 2.1978021978021977e-05, 4.3956043956043955e-05]\n",
      "Current LRs: [0.0006593406593406593, 3.296703296703297e-05, 2.9670329670329673e-05, 3.296703296703297e-05, 6.593406593406594e-05]\n",
      "Current LRs: [0.0008791208791208791, 4.3956043956043955e-05, 3.956043956043956e-05, 4.3956043956043955e-05, 8.791208791208791e-05]\n",
      "Current LRs: [0.0009997325167765264, 4.9986625838826325e-05, 4.498796325494369e-05, 4.9986625838826325e-05, 9.997325167765265e-05]\n",
      "Current LRs: [0.0009972253784704896, 4.986126892352448e-05, 4.4875142031172034e-05, 4.986126892352448e-05, 9.972253784704895e-05]\n",
      "Current LRs: [0.000992093743812234, 4.96046871906117e-05, 4.464421847155053e-05, 4.96046871906117e-05, 9.92093743812234e-05]\n",
      "Current LRs: [0.0009843677272744393, 4.921838636372196e-05, 4.429654772734977e-05, 4.921838636372196e-05, 9.843677272744393e-05]\n",
      "Current LRs: [0.0009740926681942644, 4.870463340971322e-05, 4.38341700687419e-05, 4.870463340971322e-05, 9.740926681942645e-05]\n",
      "Epoch 1/5: avg loss=1.6763\n",
      "Current LRs: [0.0009597759876209447, 4.798879938104724e-05, 4.3189919442942514e-05, 4.798879938104724e-05, 9.597759876209448e-05]\n",
      "Current LRs: [0.0009443380060197386, 4.7216900300986936e-05, 4.249521027088824e-05, 4.7216900300986936e-05, 9.443380060197387e-05]\n",
      "Current LRs: [0.0009265858921975941, 4.63292946098797e-05, 4.1696365148891736e-05, 4.63292946098797e-05, 9.26585892197594e-05]\n",
      "Current LRs: [0.000906623822617237, 4.533119113086185e-05, 4.0798072017775666e-05, 4.533119113086185e-05, 9.06623822617237e-05]\n",
      "Current LRs: [0.0008845689426405331, 4.422844713202666e-05, 3.9805602418823995e-05, 4.422844713202666e-05, 8.845689426405332e-05]\n",
      "Current LRs: [0.0008605506790729231, 4.3027533953646156e-05, 3.872478055828154e-05, 4.3027533953646156e-05, 8.605506790729231e-05]\n",
      "Current LRs: [0.0008347099806354643, 4.173549903177322e-05, 3.7561949128595896e-05, 4.173549903177322e-05, 8.347099806354644e-05]\n",
      "Current LRs: [0.0008071984908216898, 4.0359924541084496e-05, 3.632393208697605e-05, 4.0359924541084496e-05, 8.071984908216899e-05]\n",
      "Current LRs: [0.0007781776579932898, 3.890888289966449e-05, 3.501799460969804e-05, 3.890888289966449e-05, 7.781776579932898e-05]\n",
      "Epoch 2/5: avg loss=0.6766\n",
      "Current LRs: [0.0007444040351533181, 3.7220201757665905e-05, 3.3498181581899314e-05, 3.7220201757665905e-05, 7.444040351533181e-05]\n",
      "Current LRs: [0.0007127667752349837, 3.563833876174918e-05, 3.2074504885574265e-05, 3.563833876174918e-05, 7.127667752349836e-05]\n",
      "Current LRs: [0.0006801743351582076, 3.400871675791038e-05, 3.060784508211934e-05, 3.400871675791038e-05, 6.801743351582076e-05]\n",
      "Current LRs: [0.0006468179803211741, 3.2340899016058706e-05, 2.9106809114452835e-05, 3.2340899016058706e-05, 6.468179803211741e-05]\n",
      "Current LRs: [0.0006128934590776387, 3.064467295388194e-05, 2.7580205658493745e-05, 3.064467295388194e-05, 6.128934590776387e-05]\n",
      "Current LRs: [0.0005785998540077513, 2.8929992700387564e-05, 2.603699343034881e-05, 2.8929992700387564e-05, 5.785998540077513e-05]\n",
      "Current LRs: [0.0005441384136223142, 2.720692068111571e-05, 2.448622861300414e-05, 2.720692068111571e-05, 5.441384136223142e-05]\n",
      "Current LRs: [0.0005097113713564987, 2.5485568567824936e-05, 2.2937011711042442e-05, 2.5485568567824936e-05, 5.097113713564987e-05]\n",
      "Current LRs: [0.00047552075878363223, 2.3776037939181613e-05, 2.139843414526345e-05, 2.3776037939181613e-05, 4.7552075878363225e-05]\n",
      "Epoch 3/5: avg loss=0.2269\n",
      "Current LRs: [0.0004380895507758155, 2.1904477538790774e-05, 1.9714029784911697e-05, 2.1904477538790774e-05, 4.380895507758155e-05]\n",
      "Current LRs: [0.0004050529755009726, 2.0252648775048633e-05, 1.822738389754377e-05, 2.0252648775048633e-05, 4.0505297550097265e-05]\n",
      "Current LRs: [0.00037286700700154055, 1.8643350350077028e-05, 1.6779015315069325e-05, 1.8643350350077028e-05, 3.7286700700154056e-05]\n",
      "Current LRs: [0.00034172052533886166, 1.7086026266943084e-05, 1.5377423640248775e-05, 1.7086026266943084e-05, 3.417205253388617e-05]\n",
      "Current LRs: [0.0003117963104521865, 1.5589815522609325e-05, 1.4030833970348393e-05, 1.5589815522609325e-05, 3.117963104521865e-05]\n",
      "Current LRs: [0.00028326996953331106, 1.4163498476665554e-05, 1.2747148628999e-05, 1.4163498476665554e-05, 2.8326996953331107e-05]\n",
      "Current LRs: [0.000256308906493748, 1.2815445324687403e-05, 1.1533900792218663e-05, 1.2815445324687403e-05, 2.5630890649374805e-05]\n",
      "Current LRs: [0.00023107133957200925, 1.1553566978600462e-05, 1.0398210280740416e-05, 1.1553566978600462e-05, 2.3107133957200923e-05]\n",
      "Current LRs: [0.0002077053728460683, 1.0385268642303415e-05, 9.346741778073073e-06, 1.0385268642303415e-05, 2.077053728460683e-05]\n",
      "Epoch 4/5: avg loss=0.0871\n",
      "Current LRs: [0.00018412696121156835, 9.206348060578417e-06, 8.285713254520576e-06, 9.206348060578417e-06, 1.8412696121156835e-05]\n",
      "Current LRs: [0.0001651456355986643, 8.257281779933216e-06, 7.431553601939894e-06, 8.257281779933216e-06, 1.651456355986643e-05]\n",
      "Current LRs: [0.0001484227884266776, 7.42113942133388e-06, 6.6790254792004916e-06, 7.42113942133388e-06, 1.484227884266776e-05]\n",
      "Current LRs: [0.00013405655601236136, 6.702827800618068e-06, 6.0325450205562615e-06, 6.702827800618068e-06, 1.3405655601236135e-05]\n",
      "Current LRs: [0.00012213124512000395, 6.106562256000198e-06, 5.495906030400178e-06, 6.106562256000198e-06, 1.2213124512000396e-05]\n",
      "Current LRs: [0.00011271683821581324, 5.635841910790662e-06, 5.072257719711596e-06, 5.635841910790662e-06, 1.1271683821581324e-05]\n",
      "Current LRs: [0.00010586858278297735, 5.293429139148868e-06, 4.764086225233981e-06, 5.293429139148868e-06, 1.0586858278297735e-05]\n",
      "Current LRs: [0.00010162666710746518, 5.081333355373259e-06, 4.573200019835933e-06, 5.081333355373259e-06, 1.0162666710746519e-05]\n",
      "Current LRs: [0.00010001598443718422, 5.000799221859212e-06, 4.50071929967329e-06, 5.000799221859212e-06, 1.0001598443718423e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  12%|█████▉                                             | 15000/129380 [03:08<6:41:39,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|███████████▊                                       | 29830/129380 [03:13<00:36, 2734.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: images=15000, usable_pairs=28909\n",
      "Current LRs: [0.00022123893805309737, 1.1061946902654869e-05, 9.955752212389382e-06, 1.1061946902654869e-05, 2.2123893805309738e-05]\n",
      "Current LRs: [0.00044247787610619474, 2.2123893805309738e-05, 1.9911504424778764e-05, 2.2123893805309738e-05, 4.4247787610619477e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  23%|███████████▊                                       | 29830/129380 [03:27<00:36, 2734.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0006637168141592921, 3.3185840707964604e-05, 2.9867256637168145e-05, 3.3185840707964604e-05, 6.637168141592921e-05]\n",
      "Current LRs: [0.0008849557522123895, 4.4247787610619477e-05, 3.982300884955753e-05, 4.4247787610619477e-05, 8.849557522123895e-05]\n",
      "Current LRs: [0.000999690861483414, 4.9984543074170704e-05, 4.498608876675363e-05, 4.9984543074170704e-05, 9.996908614834141e-05]\n",
      "Current LRs: [0.0009970638991512201, 4.9853194957561005e-05, 4.486787546180491e-05, 4.9853194957561005e-05, 9.970638991512201e-05]\n",
      "Current LRs: [0.0009917719712445245, 4.958859856222623e-05, 4.462973870600361e-05, 4.958859856222623e-05, 9.917719712445247e-05]\n",
      "Current LRs: [0.00098384662315476, 4.9192331157738004e-05, 4.4273098041964204e-05, 4.9192331157738004e-05, 9.838466231547601e-05]\n",
      "Current LRs: [0.0009733350981951647, 4.866675490975823e-05, 4.380007941878241e-05, 4.866675490975823e-05, 9.733350981951646e-05]\n",
      "Epoch 1/5: avg loss=1.2562\n",
      "Current LRs: [0.0009597272139897383, 4.798636069948692e-05, 4.318772462953822e-05, 4.798636069948692e-05, 9.597272139897384e-05]\n",
      "Current LRs: [0.0009441503485074182, 4.720751742537091e-05, 4.248676568283382e-05, 4.720751742537091e-05, 9.441503485074182e-05]\n",
      "Current LRs: [0.0009262239372175652, 4.631119686087826e-05, 4.168007717479043e-05, 4.631119686087826e-05, 9.262239372175651e-05]\n",
      "Current LRs: [0.0009060548401664778, 4.53027420083239e-05, 4.0772467807491504e-05, 4.53027420083239e-05, 9.06054840166478e-05]\n",
      "Current LRs: [0.0008837632861389274, 4.418816430694637e-05, 3.9769347876251736e-05, 4.418816430694637e-05, 8.837632861389274e-05]\n",
      "Current LRs: [0.0008594821559696185, 4.297410779848092e-05, 3.867669701863283e-05, 4.297410779848092e-05, 8.594821559696184e-05]\n",
      "Current LRs: [0.0008333561904352813, 4.166780952176407e-05, 3.750102856958766e-05, 4.166780952176407e-05, 8.333561904352814e-05]\n",
      "Current LRs: [0.0008055411274491803, 4.0277056372459015e-05, 3.6249350735213115e-05, 4.0277056372459015e-05, 8.055411274491803e-05]\n",
      "Current LRs: [0.0007762027737012567, 3.881013868506283e-05, 3.4929124816556556e-05, 3.881013868506283e-05, 7.762027737012567e-05]\n",
      "Epoch 2/5: avg loss=0.4621\n",
      "Current LRs: [0.0007442630614109561, 3.7213153070547805e-05, 3.3491837763493027e-05, 3.7213153070547805e-05, 7.442630614109561e-05]\n",
      "Current LRs: [0.0007123681130339595, 3.561840565169797e-05, 3.2056565086528175e-05, 3.561840565169797e-05, 7.123681130339595e-05]\n",
      "Current LRs: [0.000679505281917037, 3.397526409585185e-05, 3.057773768626667e-05, 3.397526409585185e-05, 6.79505281917037e-05]\n",
      "Current LRs: [0.0006458704646954879, 3.22935232347744e-05, 2.9064170911296957e-05, 3.22935232347744e-05, 6.45870464695488e-05]\n",
      "Current LRs: [0.0006116641598442956, 3.058320799221478e-05, 2.7524887192993302e-05, 3.058320799221478e-05, 6.116641598442956e-05]\n",
      "Current LRs: [0.0005770902724987942, 2.885451362493971e-05, 2.596906226244574e-05, 2.885451362493971e-05, 5.770902724987942e-05]\n",
      "Current LRs: [0.0005423548989680957, 2.711774494840479e-05, 2.440597045356431e-05, 2.711774494840479e-05, 5.423548989680958e-05]\n",
      "Current LRs: [0.0005076650981868463, 2.5383254909342313e-05, 2.2844929418408084e-05, 2.5383254909342313e-05, 5.0766509818684625e-05]\n",
      "Current LRs: [0.00047322765742873, 2.36613828714365e-05, 2.129524458429285e-05, 2.36613828714365e-05, 4.7322765742873e-05]\n",
      "Epoch 3/5: avg loss=0.1340\n",
      "Current LRs: [0.0004379010634045493, 2.1895053170227464e-05, 1.970554785320472e-05, 2.1895053170227464e-05, 4.379010634045493e-05]\n",
      "Current LRs: [0.0004046120329513393, 2.0230601647566966e-05, 1.820754148281027e-05, 2.0230601647566966e-05, 4.046120329513393e-05]\n",
      "Current LRs: [0.0003721896659166407, 1.8609483295832036e-05, 1.6748534966248834e-05, 1.8609483295832036e-05, 3.721896659166407e-05]\n",
      "Current LRs: [0.00034082723331194743, 1.704136166559737e-05, 1.5337225499037634e-05, 1.704136166559737e-05, 3.408272333119474e-05]\n",
      "Current LRs: [0.0003107116878377187, 1.553558439188594e-05, 1.3982025952697344e-05, 1.553558439188594e-05, 3.107116878377188e-05]\n",
      "Current LRs: [0.0002820225494509417, 1.4101127472547084e-05, 1.2691014725292377e-05, 1.4101127472547084e-05, 2.820225494509417e-05]\n",
      "Current LRs: [0.00025493083523957184, 1.2746541761978592e-05, 1.1471887585780733e-05, 1.2746541761978592e-05, 2.5493083523957185e-05]\n",
      "Current LRs: [0.00022959803998291164, 1.1479901999145583e-05, 1.0331911799231024e-05, 1.1479901999145583e-05, 2.2959803998291166e-05]\n",
      "Current LRs: [0.0002061751734748476, 1.030875867374238e-05, 9.277882806368143e-06, 1.030875867374238e-05, 2.061751734748476e-05]\n",
      "Epoch 4/5: avg loss=0.0598\n",
      "Current LRs: [0.0001839914036720604, 9.199570183603021e-06, 8.27961316524272e-06, 9.199570183603021e-06, 1.8399140367206043e-05]\n",
      "Current LRs: [0.0001648845984580573, 8.244229922902865e-06, 7.419806930612578e-06, 8.244229922902865e-06, 1.648845984580573e-05]\n",
      "Current LRs: [0.0001480734813755901, 7.403674068779505e-06, 6.6633066619015545e-06, 7.403674068779505e-06, 1.480734813755901e-05]\n",
      "Current LRs: [0.0001336582641581019, 6.682913207905095e-06, 6.014621887114585e-06, 6.682913207905095e-06, 1.336582641581019e-05]\n",
      "Current LRs: [0.00012172487648523453, 6.086243824261726e-06, 5.477619441835554e-06, 6.086243824261726e-06, 1.2172487648523453e-05]\n",
      "Current LRs: [0.00011234445375261222, 5.617222687630611e-06, 5.05550041886755e-06, 5.617222687630611e-06, 1.1234445375261222e-05]\n",
      "Current LRs: [0.00010557291303093594, 5.278645651546797e-06, 4.7507810863921175e-06, 5.278645651546797e-06, 1.0557291303093595e-05]\n",
      "Current LRs: [0.00010145061974211483, 5.072530987105742e-06, 4.5652778883951675e-06, 5.072530987105742e-06, 1.0145061974211484e-05]\n",
      "Current LRs: [0.00010000214703939072, 5.000107351969537e-06, 4.500096616772583e-06, 5.000107351969537e-06, 1.0000214703939073e-05]\n",
      "Epoch 5/5: avg loss=0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 44964/129380 [06:08<00:23, 3587.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: images=15000, usable_pairs=28595\n",
      "Current LRs: [0.00022371364653243848, 1.1185682326621925e-05, 1.0067114093959732e-05, 1.1185682326621925e-05, 2.237136465324385e-05]\n",
      "Current LRs: [0.00044742729306487697, 2.237136465324385e-05, 2.0134228187919465e-05, 2.237136465324385e-05, 4.47427293064877e-05]\n",
      "Current LRs: [0.0006711409395973155, 3.3557046979865775e-05, 3.02013422818792e-05, 3.3557046979865775e-05, 6.711409395973155e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 44964/129380 [06:27<00:23, 3587.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0008948545861297539, 4.47427293064877e-05, 4.026845637583893e-05, 4.47427293064877e-05, 8.94854586129754e-05]\n",
      "Current LRs: [0.0009996146352894524, 4.998073176447262e-05, 4.498265858802535e-05, 4.998073176447262e-05, 9.996146352894524e-05]\n",
      "Current LRs: [0.0009967918965714344, 4.983959482857172e-05, 4.485563534571455e-05, 4.983959482857172e-05, 9.967918965714344e-05]\n",
      "Current LRs: [0.0009912459282684686, 4.9562296413423426e-05, 4.460606677208108e-05, 4.9562296413423426e-05, 9.912459282684685e-05]\n",
      "Current LRs: [0.0009830105334666796, 4.9150526673333975e-05, 4.423547400600058e-05, 4.9150526673333975e-05, 9.830105334666795e-05]\n",
      "Epoch 1/5: avg loss=1.1742\n",
      "Current LRs: [0.0009595663234991816, 4.797831617495908e-05, 4.3180484557463175e-05, 4.797831617495908e-05, 9.595663234991816e-05]\n",
      "Current LRs: [0.0009437746306750542, 4.718873153375271e-05, 4.246985838037744e-05, 4.718873153375271e-05, 9.437746306750542e-05]\n",
      "Current LRs: [0.0009255828524145411, 4.627914262072706e-05, 4.165122835865435e-05, 4.627914262072706e-05, 9.255828524145412e-05]\n",
      "Current LRs: [0.0009051018689487787, 4.5255093447438934e-05, 4.072958410269504e-05, 4.5255093447438934e-05, 9.051018689487787e-05]\n",
      "Current LRs: [0.000882456513383584, 4.41228256691792e-05, 3.971054310226128e-05, 4.41228256691792e-05, 8.82456513383584e-05]\n",
      "Current LRs: [0.0008577848108324605, 4.2889240541623026e-05, 3.8600316487460726e-05, 4.2889240541623026e-05, 8.577848108324605e-05]\n",
      "Current LRs: [0.0008312371371433455, 4.1561856857167275e-05, 3.740567117145055e-05, 4.1561856857167275e-05, 8.312371371433455e-05]\n",
      "Current LRs: [0.0008029753023467171, 4.014876511733586e-05, 3.613388860560227e-05, 4.014876511733586e-05, 8.029753023467171e-05]\n",
      "Epoch 2/5: avg loss=0.4182\n",
      "Current LRs: [0.0007439123503471427, 3.719561751735713e-05, 3.347605576562142e-05, 3.719561751735713e-05, 7.439123503471427e-05]\n",
      "Current LRs: [0.0007116427906072752, 3.5582139530363764e-05, 3.202392557732739e-05, 3.5582139530363764e-05, 7.116427906072753e-05]\n",
      "Current LRs: [0.0006783880061286576, 3.391940030643288e-05, 3.052746027578959e-05, 3.391940030643288e-05, 6.783880061286575e-05]\n",
      "Current LRs: [0.0006443506872770464, 3.221753436385232e-05, 2.8995780927467087e-05, 3.221753436385232e-05, 6.443506872770464e-05]\n",
      "Current LRs: [0.0006097382940229569, 3.0486914701147846e-05, 2.743822323103306e-05, 3.0486914701147846e-05, 6.097382940229569e-05]\n",
      "Current LRs: [0.0005747617914578301, 2.87380895728915e-05, 2.586428061560235e-05, 2.87380895728915e-05, 5.7476179145783e-05]\n",
      "Current LRs: [0.0005396343639462311, 2.6981718197311557e-05, 2.42835463775804e-05, 2.6981718197311557e-05, 5.3963436394623113e-05]\n",
      "Current LRs: [0.0005045701157514124, 2.5228505787570622e-05, 2.270565520881356e-05, 2.5228505787570622e-05, 5.0457011575141244e-05]\n",
      "Epoch 3/5: avg loss=0.1201\n",
      "Current LRs: [0.00043752463337195645, 2.1876231668597824e-05, 1.9688608501738044e-05, 2.1876231668597824e-05, 4.375246333719565e-05]\n",
      "Current LRs: [0.00040387649235484126, 2.0193824617742062e-05, 1.8174442155967858e-05, 2.0193824617742062e-05, 4.0387649235484124e-05]\n",
      "Current LRs: [0.0003711189849014257, 1.8555949245071284e-05, 1.6700354320564157e-05, 1.8555949245071284e-05, 3.711189849014257e-05]\n",
      "Current LRs: [0.00033945177043729697, 1.697258852186485e-05, 1.5275329669678365e-05, 1.697258852186485e-05, 3.39451770437297e-05]\n",
      "Current LRs: [0.00030906786297180366, 1.5453393148590183e-05, 1.3908053833731165e-05, 1.5453393148590183e-05, 3.0906786297180366e-05]\n",
      "Current LRs: [0.0002801524546634229, 1.4007622733171144e-05, 1.260686045985403e-05, 1.4007622733171144e-05, 2.801524546634229e-05]\n",
      "Current LRs: [0.0002528817870598877, 1.2644089352994384e-05, 1.1379680417694946e-05, 1.2644089352994384e-05, 2.5288178705988768e-05]\n",
      "Current LRs: [0.00022742207689294526, 1.1371103844647262e-05, 1.0233993460182537e-05, 1.1371103844647262e-05, 2.2742207689294524e-05]\n",
      "Epoch 4/5: avg loss=0.0547\n",
      "Current LRs: [0.00018376539046649249, 9.188269523324624e-06, 8.269442570992163e-06, 9.188269523324624e-06, 1.8376539046649248e-05]\n",
      "Current LRs: [0.00016448300731146997, 8.224150365573499e-06, 7.401735329016149e-06, 8.224150365573499e-06, 1.6448300731146998e-05]\n",
      "Current LRs: [0.00014755037867985286, 7.377518933992643e-06, 6.639767040593379e-06, 7.377518933992643e-06, 1.4755037867985286e-05]\n",
      "Current LRs: [0.0001330707101928586, 6.65353550964293e-06, 5.988181958678637e-06, 6.65353550964293e-06, 1.330707101928586e-05]\n",
      "Current LRs: [0.0001211322564985831, 6.056612824929154e-06, 5.4509515424362395e-06, 6.056612824929154e-06, 1.2113225649858308e-05]\n",
      "Current LRs: [0.00011180778335340707, 5.590389167670353e-06, 5.031350250903318e-06, 5.590389167670353e-06, 1.1180778335340706e-05]\n",
      "Current LRs: [0.00010515412410934036, 5.2577062054670184e-06, 4.731935584920316e-06, 5.2577062054670184e-06, 1.0515412410934037e-05]\n",
      "Current LRs: [0.00010121183331054691, 5.060591665527346e-06, 4.5545324989746115e-06, 5.060591665527346e-06, 1.0121183331054692e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  35%|█████████████████▋                                 | 45000/129380 [08:56<4:27:02,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▌                           | 59714/129380 [09:00<00:19, 3592.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: images=15000, usable_pairs=29155\n",
      "Current LRs: [0.0002192982456140351, 1.0964912280701754e-05, 9.868421052631579e-06, 1.0964912280701754e-05, 2.1929824561403507e-05]\n",
      "Current LRs: [0.0004385964912280702, 2.1929824561403507e-05, 1.9736842105263158e-05, 2.1929824561403507e-05, 4.3859649122807014e-05]\n",
      "Current LRs: [0.0006578947368421054, 3.289473684210527e-05, 2.9605263157894742e-05, 3.289473684210527e-05, 6.578947368421054e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▌                           | 59714/129380 [09:19<00:19, 3592.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0008771929824561404, 4.3859649122807014e-05, 3.9473684210526316e-05, 4.3859649122807014e-05, 8.771929824561403e-05]\n",
      "Current LRs: [0.000999744769921361, 4.998723849606805e-05, 4.498851464646125e-05, 4.998723849606805e-05, 9.99744769921361e-05]\n",
      "Current LRs: [0.0009972688047930772, 4.986344023965386e-05, 4.4877096215688477e-05, 4.986344023965386e-05, 9.972688047930772e-05]\n",
      "Current LRs: [0.0009921731999528022, 4.9608659997640114e-05, 4.46477939978761e-05, 4.9608659997640114e-05, 9.921731999528023e-05]\n",
      "Current LRs: [0.0009844878002022593, 4.9224390010112965e-05, 4.430195100910167e-05, 4.9224390010112965e-05, 9.844878002022593e-05]\n",
      "Current LRs: [0.0009742576186927644, 4.871288093463822e-05, 4.38415928411744e-05, 4.871288093463822e-05, 9.742576186927644e-05]\n",
      "Epoch 1/5: avg loss=1.1431\n",
      "Current LRs: [0.0009598531721142239, 4.799265860571119e-05, 4.319339274514008e-05, 4.799265860571119e-05, 9.598531721142238e-05]\n",
      "Current LRs: [0.0009444441673792107, 4.722220836896054e-05, 4.2499987532064485e-05, 4.722220836896054e-05, 9.444441673792107e-05]\n",
      "Current LRs: [0.0009267249152181378, 4.6336245760906896e-05, 4.1702621184816206e-05, 4.6336245760906896e-05, 9.267249152181379e-05]\n",
      "Current LRs: [0.0009067991967490502, 4.5339959837452514e-05, 4.080596385370726e-05, 4.5339959837452514e-05, 9.067991967490503e-05]\n",
      "Current LRs: [0.0008847837162956836, 4.423918581478418e-05, 3.981526723330576e-05, 4.423918581478418e-05, 8.847837162956836e-05]\n",
      "Current LRs: [0.00086080741785381, 4.30403708926905e-05, 3.873633380342145e-05, 4.30403708926905e-05, 8.6080741785381e-05]\n",
      "Current LRs: [0.0008350107298701978, 4.175053649350989e-05, 3.75754828441589e-05, 4.175053649350989e-05, 8.350107298701977e-05]\n",
      "Current LRs: [0.0008075447427574923, 4.037723713787462e-05, 3.633951342408716e-05, 4.037723713787462e-05, 8.075447427574924e-05]\n",
      "Current LRs: [0.0007785703239622904, 3.8928516198114524e-05, 3.503566457830307e-05, 3.8928516198114524e-05, 7.785703239622905e-05]\n",
      "Epoch 2/5: avg loss=0.3951\n",
      "Current LRs: [0.0007445379913171545, 3.722689956585772e-05, 3.350420960927195e-05, 3.722689956585772e-05, 7.445379913171545e-05]\n",
      "Current LRs: [0.0007129365795682238, 3.564682897841119e-05, 3.2082146080570074e-05, 3.564682897841119e-05, 7.129365795682238e-05]\n",
      "Current LRs: [0.0006803808532554362, 3.401904266277181e-05, 3.061713839649463e-05, 3.401904266277181e-05, 6.803808532554362e-05]\n",
      "Current LRs: [0.0006470614902722366, 3.2353074513611834e-05, 2.9117767062250648e-05, 3.2353074513611834e-05, 6.470614902722367e-05]\n",
      "Current LRs: [0.0006131736411086968, 3.065868205543484e-05, 2.7592813849891355e-05, 3.065868205543484e-05, 6.131736411086968e-05]\n",
      "Current LRs: [0.0005789157858610616, 2.8945789293053084e-05, 2.6051210363747775e-05, 2.8945789293053084e-05, 5.789157858610617e-05]\n",
      "Current LRs: [0.000544488571739896, 2.7224428586994798e-05, 2.450198572829532e-05, 2.7224428586994798e-05, 5.4448857173989595e-05]\n",
      "Current LRs: [0.0005100936378855079, 2.5504681894275395e-05, 2.2954213704847856e-05, 2.5504681894275395e-05, 5.100936378855079e-05]\n",
      "Current LRs: [0.0004759324343736697, 2.3796621718683487e-05, 2.141695954681514e-05, 2.3796621718683487e-05, 4.7593243437366975e-05]\n",
      "Epoch 3/5: avg loss=0.1075\n",
      "Current LRs: [0.0004381963223338444, 2.190981611669222e-05, 1.9718834505023e-05, 2.190981611669222e-05, 4.381963223338444e-05]\n",
      "Current LRs: [0.00040518915533889157, 2.025945776694458e-05, 1.823351199025012e-05, 2.025945776694458e-05, 4.051891553388916e-05]\n",
      "Current LRs: [0.00037303014103273757, 1.865150705163688e-05, 1.678635634647319e-05, 1.865150705163688e-05, 3.730301410327376e-05]\n",
      "Current LRs: [0.0003419076337787371, 1.7095381688936853e-05, 1.5385843520043168e-05, 1.7095381688936853e-05, 3.419076337787371e-05]\n",
      "Current LRs: [0.0003120039171501359, 1.5600195857506794e-05, 1.4040176271756116e-05, 1.5600195857506794e-05, 3.120039171501359e-05]\n",
      "Current LRs: [0.00028349413630075906, 1.4174706815037953e-05, 1.2757236133534157e-05, 1.4174706815037953e-05, 2.8349413630075906e-05]\n",
      "Current LRs: [0.0002565452721451991, 1.2827263607259957e-05, 1.1544537246533961e-05, 1.2827263607259957e-05, 2.5654527214519913e-05]\n",
      "Current LRs: [0.00023131516335670704, 1.1565758167835352e-05, 1.0409182351051818e-05, 1.1565758167835352e-05, 2.3131516335670704e-05]\n",
      "Current LRs: [0.0002079515819109086, 1.0397579095545431e-05, 9.357821185990888e-06, 1.0397579095545431e-05, 2.0795158191090862e-05]\n",
      "Epoch 4/5: avg loss=0.0504\n",
      "Current LRs: [0.0001841688366899979, 9.208441834499896e-06, 8.287597651049906e-06, 9.208441834499896e-06, 1.841688366899979e-05]\n",
      "Current LRs: [0.00016520033471976076, 8.260016735988038e-06, 7.434015062389235e-06, 8.260016735988038e-06, 1.6520033471976077e-05]\n",
      "Current LRs: [0.0001484855926217227, 7.424279631086135e-06, 6.681851667977522e-06, 7.424279631086135e-06, 1.484855926217227e-05]\n",
      "Current LRs: [0.00013412250812920768, 6.706125406460384e-06, 6.0355128658143456e-06, 6.706125406460384e-06, 1.3412250812920767e-05]\n",
      "Current LRs: [0.00012219520538885874, 6.109760269442938e-06, 5.498784242498644e-06, 6.109760269442938e-06, 1.2219520538885876e-05]\n",
      "Current LRs: [0.00011277354224808378, 5.63867711240419e-06, 5.07480940116377e-06, 5.63867711240419e-06, 1.127735422480838e-05]\n",
      "Current LRs: [0.0001059127010997797, 5.295635054988985e-06, 4.7660715494900865e-06, 5.295635054988985e-06, 1.059127010997797e-05]\n",
      "Current LRs: [0.00010165286568074437, 5.082643284037219e-06, 4.574378955633497e-06, 5.082643284037219e-06, 1.0165286568074438e-05]\n",
      "Current LRs: [0.00010001898571675944, 5.000949285837972e-06, 4.500854357254175e-06, 5.000949285837972e-06, 1.0001898571675944e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  46%|███████████████████████▋                           | 60000/129380 [11:50<2:54:03,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▌                     | 74937/129380 [11:54<00:15, 3577.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: images=15000, usable_pairs=29347\n",
      "Current LRs: [0.0002178649237472767, 1.0893246187363835e-05, 9.803921568627453e-06, 1.0893246187363835e-05, 2.178649237472767e-05]\n",
      "Current LRs: [0.0004357298474945534, 2.178649237472767e-05, 1.9607843137254906e-05, 2.178649237472767e-05, 4.357298474945534e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▌                     | 74937/129380 [12:10<00:15, 3577.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0006535947712418301, 3.2679738562091506e-05, 2.9411764705882354e-05, 3.2679738562091506e-05, 6.535947712418301e-05]\n",
      "Current LRs: [0.0008714596949891068, 4.357298474945534e-05, 3.921568627450981e-05, 4.357298474945534e-05, 8.714596949891068e-05]\n",
      "Current LRs: [0.0009997812719901233, 4.998906359950617e-05, 4.499015723955555e-05, 4.998906359950617e-05, 9.997812719901233e-05]\n",
      "Current LRs: [0.0009974153965040112, 4.9870769825200563e-05, 4.488369284268051e-05, 4.9870769825200563e-05, 9.974153965040113e-05]\n",
      "Current LRs: [0.0009924631492012948, 4.962315746006474e-05, 4.4660841714058265e-05, 4.962315746006474e-05, 9.924631492012948e-05]\n",
      "Current LRs: [0.0009849531575117686, 4.924765787558843e-05, 4.4322892088029587e-05, 4.924765787558843e-05, 9.849531575117686e-05]\n",
      "Current LRs: [0.0009749288344046789, 4.874644172023395e-05, 4.387179754821055e-05, 4.874644172023395e-05, 9.74928834404679e-05]\n",
      "Epoch 1/5: avg loss=1.1183\n",
      "Current LRs: [0.0009599460792587228, 4.799730396293614e-05, 4.3197573566642525e-05, 4.799730396293614e-05, 9.599460792587228e-05]\n",
      "Current LRs: [0.0009446607060312414, 4.7233035301562075e-05, 4.250973177140586e-05, 4.7233035301562075e-05, 9.446607060312415e-05]\n",
      "Current LRs: [0.0009270939197474009, 4.635469598737004e-05, 4.1719226388633044e-05, 4.635469598737004e-05, 9.270939197474008e-05]\n",
      "Current LRs: [0.0009073472686357773, 4.536736343178887e-05, 4.083062708860998e-05, 4.536736343178887e-05, 9.073472686357774e-05]\n",
      "Current LRs: [0.0008855349020580303, 4.427674510290151e-05, 3.9849070592611364e-05, 4.427674510290151e-05, 8.855349020580302e-05]\n",
      "Current LRs: [0.00086178291064629, 4.30891455323145e-05, 3.878023097908305e-05, 4.30891455323145e-05, 8.6178291064629e-05]\n",
      "Current LRs: [0.0008362285974117082, 4.1811429870585413e-05, 3.7630286883526873e-05, 4.1811429870585413e-05, 8.362285974117083e-05]\n",
      "Current LRs: [0.0008090196840376668, 4.045098420188334e-05, 3.640588578169501e-05, 4.045098420188334e-05, 8.090196840376669e-05]\n",
      "Current LRs: [0.0007803134569458244, 3.901567284729122e-05, 3.51141055625621e-05, 3.901567284729122e-05, 7.803134569458244e-05]\n",
      "Epoch 2/5: avg loss=0.3753\n",
      "Current LRs: [0.0007447409868142341, 3.72370493407117e-05, 3.351334440664053e-05, 3.72370493407117e-05, 7.44740986814234e-05]\n",
      "Current LRs: [0.0007133562354357958, 3.566781177178979e-05, 3.210103059461081e-05, 3.566781177178979e-05, 7.133562354357958e-05]\n",
      "Current LRs: [0.0006810271715310142, 3.4051358576550705e-05, 3.064622271889564e-05, 3.4051358576550705e-05, 6.810271715310141e-05]\n",
      "Current LRs: [0.0006479406795486445, 3.239703397743223e-05, 2.9157330579689005e-05, 3.239703397743223e-05, 6.479406795486446e-05]\n",
      "Current LRs: [0.0006142880223979258, 3.071440111989629e-05, 2.7642961007906662e-05, 3.071440111989629e-05, 6.142880223979258e-05]\n",
      "Current LRs: [0.0005802637358160913, 2.901318679080457e-05, 2.6111868111724112e-05, 2.901318679080457e-05, 5.802637358160914e-05]\n",
      "Current LRs: [0.0005460645038166618, 2.7303225190833086e-05, 2.4572902671749777e-05, 2.7303225190833086e-05, 5.460645038166617e-05]\n",
      "Current LRs: [0.0005118880217192077, 2.5594401085960386e-05, 2.3034960977364348e-05, 2.5594401085960386e-05, 5.118880217192077e-05]\n",
      "Current LRs: [0.00047793185333306265, 2.3896592666653133e-05, 2.1506933399987822e-05, 2.3896592666653133e-05, 4.7793185333306266e-05]\n",
      "Epoch 3/5: avg loss=0.1007\n",
      "Current LRs: [0.0004384144223343777, 2.1920721116718883e-05, 1.9728649005046995e-05, 2.1920721116718883e-05, 4.3841442233437766e-05]\n",
      "Current LRs: [0.0004056155667776125, 2.0280778338880624e-05, 1.825270050499256e-05, 2.0280778338880624e-05, 4.056155667776125e-05]\n",
      "Current LRs: [0.0003736513535504603, 1.8682567677523016e-05, 1.6814310909770716e-05, 1.8682567677523016e-05, 3.736513535504603e-05]\n",
      "Current LRs: [0.00034270655801129415, 1.7135327900564707e-05, 1.5421795110508236e-05, 1.7135327900564707e-05, 3.4270655801129414e-05]\n",
      "Current LRs: [0.0003129600625760489, 1.5648003128802444e-05, 1.40832028159222e-05, 1.5648003128802444e-05, 3.129600625760489e-05]\n",
      "Current LRs: [0.0002845838226535852, 1.4229191132679262e-05, 1.2806272019411335e-05, 1.4229191132679262e-05, 2.8458382265358525e-05]\n",
      "Current LRs: [0.0002577418726239672, 1.288709363119836e-05, 1.1598384268078525e-05, 1.288709363119836e-05, 2.577418726239672e-05]\n",
      "Current LRs: [0.000232589377605789, 1.1629468880289451e-05, 1.0466521992260506e-05, 1.1629468880289451e-05, 2.3258937760578903e-05]\n",
      "Current LRs: [0.00020927173649399383, 1.0463586824699691e-05, 9.417228142229724e-06, 1.0463586824699691e-05, 2.0927173649399383e-05]\n",
      "Epoch 4/5: avg loss=0.0477\n",
      "Current LRs: [0.0001842999897798236, 9.21499948899118e-06, 8.293499540092061e-06, 9.21499948899118e-06, 1.842999897798236e-05]\n",
      "Current LRs: [0.0001654339790784178, 8.27169895392089e-06, 7.444529058528802e-06, 8.27169895392089e-06, 1.654339790784178e-05]\n",
      "Current LRs: [0.00014879102714053952, 7.439551357026976e-06, 6.6955962213242786e-06, 7.439551357026976e-06, 1.4879102714053952e-05]\n",
      "Current LRs: [0.00013446734179044665, 6.723367089522332e-06, 6.0510303805701e-06, 6.723367089522332e-06, 1.3446734179044665e-05]\n",
      "Current LRs: [0.00012254572388029793, 6.127286194014897e-06, 5.514557574613407e-06, 6.127286194014897e-06, 1.2254572388029794e-05]\n",
      "Current LRs: [0.00011309508864370409, 5.654754432185205e-06, 5.089278988966684e-06, 5.654754432185205e-06, 1.130950886437041e-05]\n",
      "Current LRs: [0.00010617006731780154, 5.308503365890077e-06, 4.77765302930107e-06, 5.308503365890077e-06, 1.0617006731780154e-05]\n",
      "Current LRs: [0.00010181069133674907, 5.090534566837454e-06, 4.581481110153708e-06, 5.090534566837454e-06, 1.0181069133674908e-05]\n",
      "Current LRs: [0.00010004216092222748, 5.002108046111374e-06, 4.501897241500237e-06, 5.002108046111374e-06, 1.0004216092222749e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  58%|█████████████████████████████▌                     | 75000/129380 [14:47<2:51:33,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  70%|███████████████████████████████████▍               | 89939/129380 [14:51<00:11, 3459.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: images=15000, usable_pairs=29191\n",
      "Current LRs: [0.0002192982456140351, 1.0964912280701754e-05, 9.868421052631579e-06, 1.0964912280701754e-05, 2.1929824561403507e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  70%|███████████████████████████████████▍               | 89939/129380 [15:01<00:11, 3459.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0004385964912280702, 2.1929824561403507e-05, 1.9736842105263158e-05, 2.1929824561403507e-05, 4.3859649122807014e-05]\n",
      "Current LRs: [0.0006578947368421054, 3.289473684210527e-05, 2.9605263157894742e-05, 3.289473684210527e-05, 6.578947368421054e-05]\n",
      "Current LRs: [0.0008771929824561404, 4.3859649122807014e-05, 3.9473684210526316e-05, 4.3859649122807014e-05, 8.771929824561403e-05]\n",
      "Current LRs: [0.0009997453906337855, 4.998726953168927e-05, 4.4988542578520345e-05, 4.998726953168927e-05, 9.997453906337855e-05]\n",
      "Current LRs: [0.0009972754408986632, 4.986377204493316e-05, 4.487739484043985e-05, 4.986377204493316e-05, 9.972754408986632e-05]\n",
      "Current LRs: [0.00099219218106604, 4.9609609053302006e-05, 4.46486481479718e-05, 4.9609609053302006e-05, 9.921921810660401e-05]\n",
      "Current LRs: [0.0009845253112558735, 4.922626556279368e-05, 4.430363900651431e-05, 4.922626556279368e-05, 9.845253112558736e-05]\n",
      "Current LRs: [0.0009743196269263065, 4.871598134631532e-05, 4.3844383211683796e-05, 4.871598134631532e-05, 9.743196269263065e-05]\n",
      "Epoch 1/5: avg loss=1.0828\n",
      "Current LRs: [0.0009598073315174957, 4.7990366575874784e-05, 4.3191329918287304e-05, 4.7990366575874784e-05, 9.598073315174957e-05]\n",
      "Current LRs: [0.0009444109133257636, 4.722054566628818e-05, 4.2498491099659364e-05, 4.722054566628818e-05, 9.444109133257636e-05]\n",
      "Current LRs: [0.0009267100582521676, 4.6335502912608384e-05, 4.170195262134754e-05, 4.6335502912608384e-05, 9.267100582521677e-05]\n",
      "Current LRs: [0.0009068081876319039, 4.5340409381595195e-05, 4.0806368443435675e-05, 4.5340409381595195e-05, 9.068081876319039e-05]\n",
      "Current LRs: [0.0008848215827420009, 4.424107913710004e-05, 3.981697122339004e-05, 4.424107913710004e-05, 8.848215827420009e-05]\n",
      "Current LRs: [0.0008608787054010934, 4.304393527005467e-05, 3.87395417430492e-05, 4.304393527005467e-05, 8.608787054010934e-05]\n",
      "Current LRs: [0.0008351194474015688, 4.175597237007844e-05, 3.758037513307059e-05, 4.175597237007844e-05, 8.351194474015688e-05]\n",
      "Current LRs: [0.0008076943131594511, 4.038471565797256e-05, 3.6346244092175306e-05, 4.038471565797256e-05, 8.076943131594512e-05]\n",
      "Current LRs: [0.0007787635403575791, 3.893817701787896e-05, 3.504435931609106e-05, 3.893817701787896e-05, 7.787635403575792e-05]\n",
      "Epoch 2/5: avg loss=0.3578\n",
      "Current LRs: [0.0007444723722036858, 3.722361861018429e-05, 3.350125674916586e-05, 3.722361861018429e-05, 7.444723722036858e-05]\n",
      "Current LRs: [0.0007129078217007372, 3.564539108503686e-05, 3.208085197653318e-05, 3.564539108503686e-05, 7.129078217007372e-05]\n",
      "Current LRs: [0.000680391444612906, 3.40195722306453e-05, 3.061761500758077e-05, 3.40195722306453e-05, 6.80391444612906e-05]\n",
      "Current LRs: [0.0006471132253855869, 3.235566126927935e-05, 2.9120095142351414e-05, 3.235566126927935e-05, 6.47113225385587e-05]\n",
      "Current LRs: [0.0006132675997028829, 3.066337998514414e-05, 2.759704198662973e-05, 3.066337998514414e-05, 6.132675997028828e-05]\n",
      "Current LRs: [0.0005790523184522002, 2.895261592261001e-05, 2.605735433034901e-05, 2.895261592261001e-05, 5.790523184522002e-05]\n",
      "Current LRs: [0.0005446672923190032, 2.723336461595016e-05, 2.4510028154355143e-05, 2.723336461595016e-05, 5.446672923190032e-05]\n",
      "Current LRs: [0.0005103134237624481, 2.5515671188122403e-05, 2.2964104069310162e-05, 2.5515671188122403e-05, 5.103134237624481e-05]\n",
      "Current LRs: [0.00047619143319635086, 2.380957165981754e-05, 2.142861449383579e-05, 2.380957165981754e-05, 4.761914331963508e-05]\n",
      "Epoch 3/5: avg loss=0.0981\n",
      "Current LRs: [0.00043816286617718073, 2.1908143308859037e-05, 1.9717328977973133e-05, 2.1908143308859037e-05, 4.381628661771807e-05]\n",
      "Current LRs: [0.00040519614030412853, 2.0259807015206428e-05, 1.8233826313685787e-05, 2.0259807015206428e-05, 4.0519614030412856e-05]\n",
      "Current LRs: [0.00037307546443538696, 1.865377322176935e-05, 1.6788395899592415e-05, 1.865377322176935e-05, 3.73075464435387e-05]\n",
      "Current LRs: [0.0003419885110405595, 1.7099425552027976e-05, 1.5389482996825177e-05, 1.7099425552027976e-05, 3.419885110405595e-05]\n",
      "Current LRs: [0.00031211691282687407, 1.5605845641343704e-05, 1.4045261077209333e-05, 1.5605845641343704e-05, 3.121169128268741e-05]\n",
      "Current LRs: [0.0002836352015081304, 1.418176007540652e-05, 1.2763584067865868e-05, 1.418176007540652e-05, 2.836352015081304e-05]\n",
      "Current LRs: [0.00025670978806283806, 1.2835489403141903e-05, 1.1551940462827713e-05, 1.2835489403141903e-05, 2.5670978806283807e-05]\n",
      "Current LRs: [0.00023149799043962994, 1.1574899521981496e-05, 1.0417409569783347e-05, 1.1574899521981496e-05, 2.3149799043962992e-05]\n",
      "Current LRs: [0.0002081471143908014, 1.040735571954007e-05, 9.366620147586064e-06, 1.040735571954007e-05, 2.081471143908014e-05]\n",
      "Epoch 4/5: avg loss=0.0464\n",
      "Current LRs: [0.00018417098470518894, 9.208549235259448e-06, 8.287694311733502e-06, 9.208549235259448e-06, 1.8417098470518896e-05]\n",
      "Current LRs: [0.00016522398071621702, 8.261199035810851e-06, 7.4350791322297665e-06, 8.261199035810851e-06, 1.6522398071621702e-05]\n",
      "Current LRs: [0.00014852511951471902, 7.426255975735952e-06, 6.6836303781623564e-06, 7.426255975735952e-06, 1.4852511951471904e-05]\n",
      "Current LRs: [0.00013417196805520285, 6.708598402760143e-06, 6.037738562484128e-06, 6.708598402760143e-06, 1.3417196805520286e-05]\n",
      "Current LRs: [0.00012224838794107945, 6.112419397053972e-06, 5.501177457348575e-06, 6.112419397053972e-06, 1.2224838794107944e-05]\n",
      "Current LRs: [0.00011282404544387046, 5.6412022721935225e-06, 5.077082044974171e-06, 5.6412022721935225e-06, 1.1282404544387045e-05]\n",
      "Current LRs: [0.00010595400446192173, 5.297700223096086e-06, 4.767930200786478e-06, 5.297700223096086e-06, 1.0595400446192173e-05]\n",
      "Current LRs: [0.0001016784047968561, 5.083920239842805e-06, 4.575528215858525e-06, 5.083920239842805e-06, 1.016784047968561e-05]\n",
      "Current LRs: [0.0001000222276275084, 5.00111138137542e-06, 4.501000243237878e-06, 5.00111138137542e-06, 1.000222276275084e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  70%|███████████████████████████████████▍               | 90000/129380 [17:42<2:06:29,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 104916/129380 [17:47<00:06, 3502.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7: images=15000, usable_pairs=28504\n",
      "Current LRs: [0.00022471910112359551, 1.1235955056179776e-05, 1.0112359550561798e-05, 1.1235955056179776e-05, 2.2471910112359552e-05]\n",
      "Current LRs: [0.00044943820224719103, 2.2471910112359552e-05, 2.0224719101123596e-05, 2.2471910112359552e-05, 4.4943820224719104e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 104916/129380 [18:02<00:06, 3502.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0006741573033707865, 3.370786516853933e-05, 3.0337078651685396e-05, 3.370786516853933e-05, 6.741573033707866e-05]\n",
      "Current LRs: [0.0008988764044943821, 4.4943820224719104e-05, 4.044943820224719e-05, 4.4943820224719104e-05, 8.988764044943821e-05]\n",
      "Current LRs: [0.000999582312286147, 4.997911561430736e-05, 4.498120405287662e-05, 4.997911561430736e-05, 9.995823122861471e-05]\n",
      "Current LRs: [0.0009966862238090929, 4.983431119045465e-05, 4.485088007140918e-05, 4.983431119045465e-05, 9.96686223809093e-05]\n",
      "Current LRs: [0.0009910498775867961, 4.955249387933981e-05, 4.459724449140583e-05, 4.955249387933981e-05, 9.910498775867962e-05]\n",
      "Current LRs: [0.0009827078505561919, 4.9135392527809596e-05, 4.422185327502864e-05, 4.9135392527809596e-05, 9.827078505561919e-05]\n",
      "Epoch 1/5: avg loss=1.0651\n",
      "Current LRs: [0.0009594541395226342, 4.797270697613171e-05, 4.317543627851854e-05, 4.797270697613171e-05, 9.594541395226342e-05]\n",
      "Current LRs: [0.0009435883633158757, 4.7179418165793784e-05, 4.246147634921441e-05, 4.7179418165793784e-05, 9.435883633158757e-05]\n",
      "Current LRs: [0.0009253080654526923, 4.626540327263462e-05, 4.1638862945371156e-05, 4.626540327263462e-05, 9.253080654526923e-05]\n",
      "Current LRs: [0.0009047253889210133, 4.523626944605067e-05, 4.07126425014456e-05, 4.523626944605067e-05, 9.047253889210134e-05]\n",
      "Current LRs: [0.0008819666009656687, 4.409833004828344e-05, 3.96884970434551e-05, 4.409833004828344e-05, 8.819666009656688e-05]\n",
      "Current LRs: [0.0008571713184847017, 4.285856592423509e-05, 3.857270933181158e-05, 4.285856592423509e-05, 8.571713184847018e-05]\n",
      "Current LRs: [0.0008304916515304095, 4.1524582576520476e-05, 3.737212431886843e-05, 4.1524582576520476e-05, 8.304916515304095e-05]\n",
      "Current LRs: [0.0008020912701694185, 4.0104563508470925e-05, 3.6094107157623836e-05, 4.0104563508470925e-05, 8.020912701694185e-05]\n",
      "Epoch 2/5: avg loss=0.3446\n",
      "Current LRs: [0.0007437034761125516, 3.718517380562758e-05, 3.3466656425064825e-05, 3.718517380562758e-05, 7.437034761125516e-05]\n",
      "Current LRs: [0.0007113204312086966, 3.556602156043483e-05, 3.200941940439135e-05, 3.556602156043483e-05, 7.113204312086966e-05]\n",
      "Current LRs: [0.0006779477440514587, 3.389738720257294e-05, 3.0507648482315645e-05, 3.389738720257294e-05, 6.779477440514588e-05]\n",
      "Current LRs: [0.0006437901439554836, 3.2189507197774183e-05, 2.8970556477996765e-05, 3.2189507197774183e-05, 6.437901439554837e-05]\n",
      "Current LRs: [0.0006090571753912138, 3.0452858769560688e-05, 2.740757289260462e-05, 3.0452858769560688e-05, 6.0905717539121375e-05]\n",
      "Current LRs: [0.0005739619125056686, 2.8698095625283427e-05, 2.5828286062755085e-05, 2.8698095625283427e-05, 5.7396191250566854e-05]\n",
      "Current LRs: [0.0005387196519899421, 2.6935982599497104e-05, 2.4242384339547397e-05, 2.6935982599497104e-05, 5.387196519899421e-05]\n",
      "Current LRs: [0.0005035465923122, 2.517732961561e-05, 2.2659596654049e-05, 2.517732961561e-05, 5.035465923122e-05]\n",
      "Epoch 3/5: avg loss=0.0899\n",
      "Current LRs: [0.00043733847926675265, 2.1866923963337634e-05, 1.9680231567003868e-05, 2.1866923963337634e-05, 4.373384792667527e-05]\n",
      "Current LRs: [0.0004035869285453927, 2.0179346427269637e-05, 1.8161411784542674e-05, 2.0179346427269637e-05, 4.0358692854539274e-05]\n",
      "Current LRs: [0.00037073356883972374, 1.8536678441986188e-05, 1.668301059778757e-05, 1.8536678441986188e-05, 3.7073356883972375e-05]\n",
      "Current LRs: [0.0003389799435790977, 1.6948997178954885e-05, 1.5254097461059398e-05, 1.6948997178954885e-05, 3.389799435790977e-05]\n",
      "Current LRs: [0.00030852084972147304, 1.5426042486073654e-05, 1.3883438237466287e-05, 1.5426042486073654e-05, 3.085208497214731e-05]\n",
      "Current LRs: [0.00027954314274482864, 1.3977157137241431e-05, 1.2579441423517287e-05, 1.3977157137241431e-05, 2.7954314274482862e-05]\n",
      "Current LRs: [0.0002522245903566728, 1.261122951783364e-05, 1.1350106566050275e-05, 1.261122951783364e-05, 2.522245903566728e-05]\n",
      "Current LRs: [0.00022673278195372367, 1.1336639097686183e-05, 1.0202975187917565e-05, 1.1336639097686183e-05, 2.2673278195372366e-05]\n",
      "Epoch 4/5: avg loss=0.0432\n",
      "Current LRs: [0.0001836764530127955, 9.183822650639775e-06, 8.265440385575798e-06, 9.183822650639775e-06, 1.836764530127955e-05]\n",
      "Current LRs: [0.0001643453452962055, 8.217267264810275e-06, 7.395540538329247e-06, 8.217267264810275e-06, 1.643453452962055e-05]\n",
      "Current LRs: [0.000147380088815253, 7.36900444076265e-06, 6.632103996686386e-06, 7.36900444076265e-06, 1.47380088815253e-05]\n",
      "Current LRs: [0.00013288475925661488, 6.644237962830745e-06, 5.97981416654767e-06, 6.644237962830745e-06, 1.328847592566149e-05]\n",
      "Current LRs: [0.0001209482802023845, 6.047414010119225e-06, 5.442672609107303e-06, 6.047414010119225e-06, 1.209482802023845e-05]\n",
      "Current LRs: [0.00011164387761619321, 5.5821938808096605e-06, 5.023974492728695e-06, 5.5821938808096605e-06, 1.1164387761619321e-05]\n",
      "Current LRs: [0.00010502863062851943, 5.2514315314259715e-06, 4.726288378283375e-06, 5.2514315314259715e-06, 1.0502863062851943e-05]\n",
      "Current LRs: [0.00010114312137695415, 5.057156068847707e-06, 4.551440461962936e-06, 5.057156068847707e-06, 1.0114312137695414e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  81%|████████████████████████████████████████▌         | 105000/129380 [20:34<1:14:52,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: avg loss=0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|██████████████████████████████████████████████▎   | 119925/129380 [20:38<00:02, 3641.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: images=15000, usable_pairs=28894\n",
      "Current LRs: [0.00022172949002217298, 1.1086474501108649e-05, 9.977827050997784e-06, 1.1086474501108649e-05, 2.2172949002217298e-05]\n",
      "Current LRs: [0.00044345898004434595, 2.2172949002217298e-05, 1.9955654101995567e-05, 2.2172949002217298e-05, 4.4345898004434597e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据:  93%|██████████████████████████████████████████████▎   | 119925/129380 [20:52<00:02, 3641.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current LRs: [0.0006651884700665188, 3.325942350332594e-05, 2.993348115299335e-05, 3.325942350332594e-05, 6.651884700665188e-05]\n",
      "Current LRs: [0.0008869179600886919, 4.4345898004434597e-05, 3.9911308203991135e-05, 4.4345898004434597e-05, 8.869179600886919e-05]\n",
      "Current LRs: [0.0009996772136967632, 4.9983860684838164e-05, 4.4985474616354346e-05, 4.9983860684838164e-05, 9.996772136967633e-05]\n",
      "Current LRs: [0.0009970182775954116, 4.985091387977058e-05, 4.486582249179352e-05, 4.985091387977058e-05, 9.970182775954115e-05]\n",
      "Current LRs: [0.0009916894024438153, 4.958447012219076e-05, 4.462602310997169e-05, 4.958447012219076e-05, 9.916894024438152e-05]\n",
      "Current LRs: [0.000983722416407788, 4.9186120820389405e-05, 4.4267508738350466e-05, 4.9186120820389405e-05, 9.837224164077881e-05]\n",
      "Current LRs: [0.0009731649044926787, 4.8658245224633935e-05, 4.3792420702170546e-05, 4.8658245224633935e-05, 9.731649044926787e-05]\n",
      "Epoch 1/5: avg loss=1.0352\n",
      "Current LRs: [0.0009596491049974418, 4.798245524987209e-05, 4.318420972488488e-05, 4.798245524987209e-05, 9.596491049974418e-05]\n",
      "Current LRs: [0.0009440426925065846, 4.720213462532923e-05, 4.2481921162796305e-05, 4.720213462532923e-05, 9.440426925065846e-05]\n",
      "Current LRs: [0.0009260827521513946, 4.630413760756973e-05, 4.1673723846812756e-05, 4.630413760756973e-05, 9.260827521513946e-05]\n",
      "Current LRs: [0.0009058765545928218, 4.529382772964109e-05, 4.0764444956676984e-05, 4.529382772964109e-05, 9.058765545928218e-05]\n",
      "Current LRs: [0.0008835447868779569, 4.4177239343897844e-05, 3.9759515409508065e-05, 4.4177239343897844e-05, 8.835447868779569e-05]\n",
      "Current LRs: [0.0008592208316036002, 4.296104158018001e-05, 3.866493742216201e-05, 4.296104158018001e-05, 8.592208316036002e-05]\n",
      "Current LRs: [0.0008330499702521842, 4.165249851260921e-05, 3.748724866134829e-05, 4.165249851260921e-05, 8.330499702521842e-05]\n",
      "Current LRs: [0.0008051885154583432, 4.0259425772917156e-05, 3.623348319562544e-05, 4.0259425772917156e-05, 8.051885154583431e-05]\n",
      "Current LRs: [0.0007758028773889072, 3.879014386944536e-05, 3.4911129482500824e-05, 3.879014386944536e-05, 7.758028773889072e-05]\n",
      "Epoch 2/5: avg loss=0.3203\n",
      "Current LRs: [0.0007441276040044084, 3.720638020022042e-05, 3.348574218019838e-05, 3.720638020022042e-05, 7.441276040044083e-05]\n",
      "Current LRs: [0.0007121961547152754, 3.560980773576377e-05, 3.2048826962187397e-05, 3.560980773576377e-05, 7.121961547152754e-05]\n",
      "Current LRs: [0.0006792959444850837, 3.3964797224254184e-05, 3.0568317501828766e-05, 3.3964797224254184e-05, 6.792959444850837e-05]\n",
      "Current LRs: [0.0006456234788270468, 3.228117394135234e-05, 2.905305654721711e-05, 3.228117394135234e-05, 6.456234788270468e-05]\n",
      "Current LRs: [0.0006113798757613543, 3.056899378806771e-05, 2.751209440926094e-05, 3.056899378806771e-05, 6.113798757613542e-05]\n",
      "Current LRs: [0.0005767696645827318, 2.8838483229136594e-05, 2.5954634906222932e-05, 2.8838483229136594e-05, 5.767696645827319e-05]\n",
      "Current LRs: [0.0005419995642532321, 2.7099978212661602e-05, 2.4389980391395443e-05, 2.7099978212661602e-05, 5.4199956425323204e-05]\n",
      "Current LRs: [0.0005072772487166365, 2.5363862435831827e-05, 2.2827476192248644e-05, 2.5363862435831827e-05, 5.0727724871663653e-05]\n",
      "Current LRs: [0.00047281010650896825, 2.3640505325448414e-05, 2.1276454792903574e-05, 2.3640505325448414e-05, 4.728101065089683e-05]\n",
      "Epoch 3/5: avg loss=0.0841\n",
      "Current LRs: [0.0004377930742511033, 2.1889653712555164e-05, 1.9700688341299647e-05, 2.1889653712555164e-05, 4.377930742511033e-05]\n",
      "Current LRs: [0.00040447414244672845, 2.0223707122336423e-05, 1.820133641010278e-05, 2.0223707122336423e-05, 4.0447414244672846e-05]\n",
      "Current LRs: [0.00037202440366952005, 1.8601220183476002e-05, 1.6741098165128405e-05, 1.8601220183476002e-05, 3.7202440366952004e-05]\n",
      "Current LRs: [0.00034063767286860613, 1.703188364343031e-05, 1.5328695279087277e-05, 1.703188364343031e-05, 3.406376728686062e-05]\n",
      "Current LRs: [0.00031050141588693977, 1.552507079434699e-05, 1.397256371491229e-05, 1.552507079434699e-05, 3.105014158869398e-05]\n",
      "Current LRs: [0.0002817956297702266, 1.4089781488511331e-05, 1.2680803339660197e-05, 1.4089781488511331e-05, 2.8179562977022662e-05]\n",
      "Current LRs: [0.00025469176768528836, 1.2734588384264417e-05, 1.1461129545837976e-05, 1.2734588384264417e-05, 2.5469176768528834e-05]\n",
      "Current LRs: [0.000229351714869083, 1.146758574345415e-05, 1.0320827169108735e-05, 1.146758574345415e-05, 2.29351714869083e-05]\n",
      "Current LRs: [0.0002059268217248047, 1.0296341086240235e-05, 9.266706977616212e-06, 1.0296341086240235e-05, 2.059268217248047e-05]\n",
      "Epoch 4/5: avg loss=0.0413\n",
      "Current LRs: [0.00018394902274480122, 9.197451137240062e-06, 8.277706023516056e-06, 9.197451137240062e-06, 1.8394902274480123e-05]\n",
      "Current LRs: [0.00016482922799806882, 8.241461399903442e-06, 7.417315259913097e-06, 8.241461399903442e-06, 1.6482922799806884e-05]\n",
      "Current LRs: [0.0001480099711414486, 7.40049855707243e-06, 6.660448701365187e-06, 7.40049855707243e-06, 1.480099711414486e-05]\n",
      "Current LRs: [0.00013359170979139196, 6.679585489569598e-06, 6.011626940612639e-06, 6.679585489569598e-06, 1.3359170979139196e-05]\n",
      "Current LRs: [0.0001216605609613291, 6.083028048066455e-06, 5.47472524325981e-06, 6.083028048066455e-06, 1.216605609613291e-05]\n",
      "Current LRs: [0.00011228778670422798, 5.614389335211399e-06, 5.0529504016902596e-06, 5.614389335211399e-06, 1.1228778670422798e-05]\n",
      "Current LRs: [0.00010552936848047259, 5.27646842402363e-06, 4.748821581621267e-06, 5.27646842402363e-06, 1.055293684804726e-05]\n",
      "Current LRs: [0.0001014256727932641, 5.071283639663205e-06, 4.564155275696885e-06, 5.071283639663205e-06, 1.014256727932641e-05]\n",
      "Current LRs: [0.0001000012100886284, 5.000060504431421e-06, 4.500054453988278e-06, 5.000060504431421e-06, 1.0000121008862841e-05]\n",
      "Epoch 5/5: avg loss=0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "实际加载train图片数据: 100%|████████████████████████████████████████████████████| 129380/129380 [23:29<00:00, 91.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9: images=9380, usable_pairs=18432\n",
      "Current LRs: [0.00034722222222222224, 1.736111111111111e-05, 1.5625e-05, 1.736111111111111e-05, 3.472222222222222e-05]\n",
      "Current LRs: [0.0006944444444444445, 3.472222222222222e-05, 3.125e-05, 3.472222222222222e-05, 6.944444444444444e-05]\n",
      "Current LRs: [0.0009999524043671985, 4.999762021835992e-05, 4.499785819652393e-05, 4.999762021835992e-05, 9.999524043671984e-05]\n",
      "Current LRs: [0.0009958601815141803, 4.979300907570902e-05, 4.4813708168138116e-05, 4.979300907570902e-05, 9.958601815141804e-05]\n",
      "Current LRs: [0.0009852261668953826, 4.9261308344769126e-05, 4.433517751029221e-05, 4.9261308344769126e-05, 9.852261668953825e-05]\n",
      "Epoch 1/5: avg loss=1.0325\n",
      "Current LRs: [0.0009511508380951625, 4.7557541904758126e-05, 4.280178771428231e-05, 4.7557541904758126e-05, 9.511508380951625e-05]\n",
      "Current LRs: [0.0009235541937120755, 4.6177709685603776e-05, 4.15599387170434e-05, 4.6177709685603776e-05, 9.235541937120755e-05]\n",
      "Current LRs: [0.0008904766522903719, 4.45238326145186e-05, 4.0071449353066736e-05, 4.45238326145186e-05, 8.90476652290372e-05]\n",
      "Current LRs: [0.0008524035372499851, 4.262017686249926e-05, 3.835815917624933e-05, 4.262017686249926e-05, 8.524035372499851e-05]\n",
      "Current LRs: [0.0008098934685367982, 4.0494673426839916e-05, 3.644520608415592e-05, 4.0494673426839916e-05, 8.098934685367983e-05]\n",
      "Epoch 2/5: avg loss=0.2471\n",
      "Current LRs: [0.0007262305711138627, 3.6311528555693135e-05, 3.268037570012382e-05, 3.6311528555693135e-05, 7.262305711138627e-05]\n",
      "Current LRs: [0.0006748754354371432, 3.3743771771857155e-05, 3.0369394594671442e-05, 3.3743771771857155e-05, 6.748754354371431e-05]\n",
      "Current LRs: [0.0006216880906303853, 3.108440453151926e-05, 2.7975964078367335e-05, 3.108440453151926e-05, 6.216880906303852e-05]\n",
      "Current LRs: [0.0005674489170655676, 2.8372445853278378e-05, 2.553520126795054e-05, 2.8372445853278378e-05, 5.6744891706556756e-05]\n",
      "Current LRs: [0.00051295372785568, 2.5647686392784004e-05, 2.3082917753505604e-05, 2.5647686392784004e-05, 5.129537278556801e-05]\n",
      "Epoch 3/5: avg loss=0.0601\n",
      "Current LRs: [0.00041885006132375436, 2.094250306618772e-05, 1.8848252759568948e-05, 2.094250306618772e-05, 4.188500613237544e-05]\n",
      "Current LRs: [0.0003677660730852969, 1.8388303654264843e-05, 1.6549473288838358e-05, 1.8388303654264843e-05, 3.6776607308529686e-05]\n",
      "Current LRs: [0.00031935587464006063, 1.5967793732003034e-05, 1.437101435880273e-05, 1.5967793732003034e-05, 3.193558746400607e-05]\n",
      "Current LRs: [0.00027432975466305874, 1.3716487733152938e-05, 1.2344838959837646e-05, 1.3716487733152938e-05, 2.7432975466305876e-05]\n",
      "Current LRs: [0.00023334834963434237, 1.1667417481717118e-05, 1.0500675733545407e-05, 1.1667417481717118e-05, 2.3334834963434236e-05]\n",
      "Epoch 4/5: avg loss=0.0348\n",
      "Current LRs: [0.0001728360654094719, 8.641803270473594e-06, 7.777622943426235e-06, 8.641803270473594e-06, 1.7283606540947188e-05]\n",
      "Current LRs: [0.0001459259904413403, 7.296299522067015e-06, 6.566669569860314e-06, 7.296299522067015e-06, 1.459259904413403e-05]\n",
      "Current LRs: [0.0001249446082295779, 6.247230411478895e-06, 5.6225073703310054e-06, 6.247230411478895e-06, 1.249446082295779e-05]\n",
      "Current LRs: [0.000110199763787214, 5.5099881893607e-06, 4.95898937042463e-06, 5.5099881893607e-06, 1.10199763787214e-05]\n",
      "Current LRs: [0.00010190779781018887, 5.095389890509444e-06, 4.585850901458499e-06, 5.095389890509444e-06, 1.0190779781018887e-05]\n",
      "Epoch 5/5: avg loss=0.0288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    text_extractor.model.encoder.layer[-1].train()\n",
    "    text_extractor.model.encoder.layer[-2].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.train()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.train()\n",
    "\n",
    "    # 为当前大batch构建 Warmup+Cosine 学习率调度器（按总steps）\n",
    "    steps_per_epoch = math.ceil(len(pairs) / max(1, step_bs))\n",
    "    total_steps = epochs * max(1, steps_per_epoch)\n",
    "    scheduler = build_warmup_cosine_scheduler(optim, warmup_ratio=warmup_ratio, min_lr_ratio=min_lr_ratio, total_steps=total_steps)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            optim.zero_grad()\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    loss = info_nce_loss(t_proj, i_proj, temp=temperature)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                loss = info_nce_loss(t_proj, i_proj, temp=temperature)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "            if (steps % 100) == 0:\n",
    "                print('Current LRs:', [pg['lr'] for pg in optim.param_groups])\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载图片与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存：投影层 + 已解冻顶层 + 优化器\n",
    "保存 BERT 的最后2层 + pooler、ResNet50 的 layer4、投影层、优化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_3_1_1.pth\n"
     ]
    }
   ],
   "source": [
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "# 保存一次\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, last_n_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载：恢复解冻顶层与投影层权重，继续训练\n",
    "加载后会自动再次执行顶层解冻，并恢复优化器状态（如提供）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    ln = ckpt.get('last_n_layers', 2)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 测试加载\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 17:12:57,142 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据: 100%|████████████████████████████████████████████████████| 29806/29806 [00:14<00:00, 2007.07it/s]\n",
      "2025-11-09 17:13:14,562 - INFO - 成功创建valid图片映射字典，共29806张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:33<00:00, 149.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0587, Recall@5=0.1979, Recall@10=0.3071, MeanRecall=0.1879 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=valid_imgs_max_samples,\n",
    "    # batch_size=3000,\n",
    "    # max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"step_3_1-1_乾-基线.finishflag\", \"w\") as f:\n",
    "    f.write(\"finish\")\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "kill_current_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
