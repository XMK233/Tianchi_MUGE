{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3-3：改用 Mean-Pooling 文本特征 + LR Schedule (屯-v3)\n",
    "\n",
    "在 `step_2_3-3_屯-mean_pooling.ipynb` 的结构上做最小改动：\n",
    "- 文本使用基于 attention_mask 的 mean-pooling；图像使用 ResNet50 特征\n",
    "- 保留两层投影（本版为单层线性映射保持最小差异）与 InfoNCE 损失、AMP、流式训练\n",
    "- 新增：学习率调度器（warmup + cosine），以进一步降低 loss 并提升稳定性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# # 设置环境变量（与基线一致，按需修改为本地镜像/缓存）\n",
    "# cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "# os.environ['TORCH_HOME'] = cache_dir\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "# os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "import importlib\n",
    "\n",
    "# 可选：Faiss 加速检索（安全导入，避免 NumPy 版本问题导致报错中断）\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = importlib.import_module('faiss')\n",
    "        HAS_FAISS = True\n",
    "except BaseException as e:\n",
    "    print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 设置环境变量（与基线一致，按需修改为本地镜像/缓存）\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器（使用 plan1_1/data_loader.py）\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_2_3_3_v3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型与特征模块\n",
    "文本 mean-pooling 与 ResNet50 图像特征，投影到共享空间。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = BertModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.out_dim = 768\n",
    "        \n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, self.out_dim), dtype=torch.float32, device=self.device)\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        outputs = self.model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # [B, L, 768]\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is None:\n",
    "            return token_embeddings[:, 0, :]\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(token_embeddings)  # [B, L, 1]\n",
    "        summed = (token_embeddings * mask).sum(dim=1)  # [B, 768]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)        # [B, 1]\n",
    "        mean_pooled = summed / lengths\n",
    "        return mean_pooled\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.out_dim = 2048\n",
    "        \n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, self.out_dim), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Linear(768, projection_dim).to(self.device)\n",
    "            self.image_projector = torch.nn.Linear(2048, projection_dim).to(self.device)\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        f = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "        return f\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1) if self.normalize_features else x\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n",
    "仅解冻顶层，降低学习率，控制训练稳定性；新增调度器构造函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n",
    "\n",
    "def build_scheduler(optimizer: torch.optim.Optimizer, num_steps: int, warmup_ratio: float = 0.1, min_lr_ratio: float = 0.1):\n",
    "    warmup_steps = max(1, int(num_steps * warmup_ratio))\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step + 1) / float(max(1, warmup_steps))\n",
    "        # Cosine decay from 1.0 -> min_lr_ratio\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, num_steps - warmup_steps))\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return max(min_lr_ratio, cosine)\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数\n",
    "保持与基线一致的查询数据加载；图片按批次流式以控制显存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 14:09:38,657 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-09 14:09:38,660 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:03, 79533.17it/s] \n",
      "2025-11-09 14:09:41,872 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-09 14:09:41,887 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 145910.12it/s]\n",
      "2025-11-09 14:09:41,927 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# # 训练与流式参数（默认较小，确保可运行）\n",
    "# train_image_batch_size = 500  # 一个大batch的图片数量\n",
    "# max_train_batches = 1         # 加载多少个大batch\n",
    "# epochs_per_batch = 1          # 每个大batch的训练轮数\n",
    "# train_step_batch_size = 32    # 训练时的小 batch size\n",
    "\n",
    "# 训练与流式参数（按需调整）：实际用\n",
    "train_image_batch_size = 5000 ## 一个大batch有这么多图片样本。\n",
    "max_train_batches = 30 ## 总共加载多少个大batch。\n",
    "epochs_per_batch = 5 ## 每个大batch训练几个epoch。\n",
    "train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "\n",
    "\n",
    "use_amp = True\n",
    "temperature = 0.07\n",
    "use_lr_scheduler = True\n",
    "warmup_ratio = 0.1\n",
    "min_lr_ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型并执行顶层解冻\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 14:09:42,233 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-09 14:09:42,627 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_293361/1887567262.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=2)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "optim = build_optimizer(model, text_extractor, image_extractor,\n",
    "                        lr_proj=1e-3, lr_text_top=5e-5, lr_img_top=1e-4, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 训练循环（流式）\n",
    "在每个大批次内构建 warmup+cosine 调度器，并在每步优化后进行 scheduler.step()。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def print_group_lrs(optim: torch.optim.Optimizer):\n",
    "    lrs = [pg['lr'] for pg in optim.param_groups]\n",
    "    print(f'Current LRs: {lrs}')\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "\n",
    "    steps_per_epoch = max(1, (len(pairs) + step_bs - 1) // step_bs)\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    scheduler = None\n",
    "    if use_lr_scheduler:\n",
    "        scheduler = build_scheduler(optim, num_steps=total_steps, warmup_ratio=warmup_ratio, min_lr_ratio=min_lr_ratio)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        print(f'Epoch {e+1}/{epochs} start')\n",
    "        print_group_lrs(optim)\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            optim.zero_grad()\n",
    "            temp = temperature\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    loss = info_nce_loss(t_proj, i_proj, temp)\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                loss = info_nce_loss(t_proj, i_proj, temp)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                optim.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "            if steps % 100 == 0:\n",
    "                print_group_lrs(optim)\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存：投影层 + 已解冻顶层 + 优化器\n",
    "保存 BERT 的最后2层 + pooler、ResNet50 的 layer4、投影层、优化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "# 保存一次\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载：恢复解冻顶层与投影层权重，继续训练\n",
    "加载后会自动再次执行顶层解冻，并恢复优化器状态（如提供）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_2_3_3_v3.pth\n"
     ]
    }
   ],
   "source": [
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    ln = ckpt.get('last_n_layers', 2)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 测试加载\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 14:10:20,098 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据: 100%|████████████████████████████████████████████████████| 29806/29806 [00:13<00:00, 2159.72it/s]\n",
      "2025-11-09 14:10:50,144 - INFO - 成功创建valid图片映射字典，共29806张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=50000,\n",
    "    batch_size=3000,\n",
    "    max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:33<00:00, 147.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0294, Recall@5=0.1020, Recall@10=0.1745, MeanRecall=0.1020 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}