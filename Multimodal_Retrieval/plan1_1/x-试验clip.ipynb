{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63a5769-6394-454d-8885-1e127d52d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/HuggingFaceModels/models--IDEA-CCNL--Taiyi-CLIP-Roberta-102M-Chinese:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--OFA-Sys--chinese-clip-vit-base-patch16:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--Qwen--Qwen1.5-1.8B:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--Qwen--Qwen3-4B:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--bert-base-chinese:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--hfl--chinese-roberta-wwm-ext:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--openai--clip-vit-base-patch32:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--timm--convnext_tiny.in12k_ft_in1k:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--timm--resnet50.a1_in1k:\n",
      "blobs  refs  snapshots\n",
      "\n",
      "/mnt/d/HuggingFaceModels/models--unsloth--Qwen3-4B-Base:\n",
      "blobs  refs  snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/d/HuggingFaceModels/models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17326d88-cc7b-4568-afae-2545c6286f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5518b8-7dde-438b-bee7-3b57f8832619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5885be7-47d4-44fc-b6e8-3202f72aec02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef794dfe-a125-4e6e-925d-de5ab74638e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4b6065-48bd-4087-9682-1d916b3556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/mnt/d/HuggingFaceModels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d775c3-4d66-42ac-ad37-f884f01c1d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (2), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.012 0.    0.987 0.001 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import clip\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "query_texts = [\"一只猫\", \"一只狗\",'两只猫', '两只老虎','一只老虎']  # 这里是输入文本的，可以随意替换。\n",
    "# 加载Taiyi 中文 text encoder\n",
    "text_tokenizer = BertTokenizer.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\", cache_dir=cache_dir)\n",
    "text_encoder = BertForSequenceClassification.from_pretrained(\"IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese\", cache_dir=cache_dir).eval()\n",
    "text = text_tokenizer(query_texts, return_tensors='pt', padding=True)['input_ids']\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # 这里可以换成任意图片的url\n",
    "# 加载CLIP的image encoder\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=cache_dir)  \n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=cache_dir)\n",
    "image = processor(images=Image.open(requests.get(url, stream=True).raw), return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**image)\n",
    "    text_features = text_encoder(text).logits\n",
    "    # 归一化\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "    # 计算余弦相似度 logit_scale是尺度系数\n",
    "    logit_scale = clip_model.logit_scale.exp()\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    print(np.around(probs, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795ebed-a07e-49f7-97c2-b9e5cce8c071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83142477-418d-4cf2-ae51-4094039c18eb",
   "metadata": {},
   "source": [
    "## 评估：Recall@1/5/10 与 MeanRecall\n",
    "- 在验证集上构建图像索引（特征）\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62851d71-662c-46a5-8f7d-158b993bbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建验证集图像索引\n",
    "clip_model.eval()\n",
    "tokenizer_eval = AutoTokenizer.from_pretrained('IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese', cache_dir=cache_dir, local_files_only=True)\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "valid_imgs = loader.create_img_id_to_image_dict(split='valid')\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "# 图像特征提取（批量）\n",
    "eval_image_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "all_image_ids = list(valid_imgs.keys())\n",
    "image_tensors, ids_kept = [], []\n",
    "for k in all_image_ids:\n",
    "    img = valid_imgs[k]\n",
    "    if img is None:\n",
    "        continue\n",
    "    ids_kept.append(k)\n",
    "    image_tensors.append(eval_image_tf(img.convert('RGB')))\n",
    "\n",
    "with torch.no_grad():\n",
    "    if len(image_tensors) == 0:\n",
    "        all_image_feats = torch.empty((0, 512))\n",
    "    else:\n",
    "        feats = []\n",
    "        bs = 128\n",
    "        for s in tqdm(range(0, len(image_tensors), bs), desc='Build image index'):\n",
    "            batch = torch.stack(image_tensors[s:s+bs]).to(device)\n",
    "            with autocast(enabled=(device.type=='cuda')):\n",
    "                img_feats = clip_model.get_image_features(pixel_values=batch)\n",
    "            img_feats = torch.nn.functional.normalize(img_feats, p=2, dim=1)\n",
    "            feats.append(img_feats.detach().cpu())\n",
    "        all_image_feats = torch.cat(feats, dim=0)\n",
    "all_image_ids = ids_kept\n",
    "\n",
    "# FAISS 索引（可选）\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    faiss_index.add(all_image_feats.numpy().astype('float32'))\n",
    "\n",
    "# 评估召回\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        tok = tokenizer_eval(q_text, return_tensors='pt', padding='max_length', truncation=True, max_length=32).to(device)\n",
    "        with torch.no_grad():\n",
    "            with autocast(enabled=(device.type=='cuda')):\n",
    "                q_feat = clip_model.get_text_features(**tok)\n",
    "            q_feat = torch.nn.functional.normalize(q_feat, p=2, dim=1)\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = torch.matmul(q_feat, all_image_feats.t())\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / max(total, 1)) if total > 0 else 0.0 for k in k_values}\n",
    "\n",
    "metrics = compute_recall_at_k([1, 5, 10], valid_queries)\n",
    "mean_recall = sum(metrics.values()) / len(metrics) if len(metrics) > 0 else 0.0\n",
    "print(f\"Recall@1={metrics[1]:.4f}, Recall@5={metrics[5]:.4f}, Recall@10={metrics[10]:.4f}, MeanRecall={mean_recall:.4f} (N={len(valid_queries)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406e490-3676-4d58-8fd5-578d647f6f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
