{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3-4：两层MLP投影头 + 可学习温度 + AMP/FAISS (蒙)\n",
    "\n",
    "依据 `2.3的改进方案.md` 的第三优先：\n",
    "- 将投影头升级为两层 MLP（Linear → GELU → Dropout → Linear）\n",
    "- 引入可学习温度（logit_scale），替代固定常数 temperature\n",
    "- 保留 AMP 训练加速与 FAISS 检索回退逻辑，保证可运行且稳定\n",
    "\n",
    "在 `step_2_3-3_屯-mean_pooling.ipynb` 基础上实现，文本特征使用 attention_mask 的 mean-pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现loss到0.04就不好下降了。做一些改动：\n",
    "* 优化器改用adamw。\n",
    "* projector看上去优化了一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AMP\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 安全导入 FAISS（不可用则回退）\n",
    "HAS_FAISS = False\n",
    "try:\n",
    "    import faiss\n",
    "    HAS_FAISS = True\n",
    "except Exception:\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 环境与缓存\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型模块\n",
    "- 文本特征：attention_mask mean-pooling\n",
    "- 投影头：两层 MLP（含 GELU + Dropout）\n",
    "- 可学习温度：logit_scale 参数，训练中联合优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "# 1. 优化版两层MLP投影头（核心组件）\n",
    "class OptimizedMLPProjector(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim) if use_bn else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_dim)\n",
    "        # Kaiming初始化\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='linear')\n",
    "        nn.init.zeros_(self.layer2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    # 类作用：封装 BERT 文本编码，支持带梯度的特征提取\n",
    "    # 参数:\n",
    "    # - model_name: 预训练文本模型名称（默认 'bert-base-chinese'）\n",
    "    # - device: 设备（'cpu' 或 'cuda'），用于放置模型与张量\n",
    "    # - cache_dir: 本地模型缓存目录，优先本地加载，失败再远程\n",
    "    def __init__(self, model_name='bert-base-chinese', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        # 优先本地加载，失败则远程镜像加载\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "            self.model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        except Exception:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=False)\n",
    "            self.model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir, local_files_only=False).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        # 函数作用：对一批文本进行编码并做 attention 掩码的 mean-pooling\n",
    "        # 参数：texts 文本字符串列表\n",
    "        # 返回：形状 [B, 768] 的句向量（带注意力掩码的均值池化）\n",
    "        if not texts:\n",
    "            return torch.empty((0, 768), dtype=torch.float32, device=self.device)\n",
    "        inputs = self.tokenizer(\n",
    "            texts, padding=True, truncation=True, max_length=32, return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        token_embeddings = outputs.last_hidden_state  # [B, L, 768]\n",
    "        attention_mask = inputs['attention_mask']     # [B, L]\n",
    "        # mean-pooling with attention mask\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(token_embeddings)  # [B, L, 1]\n",
    "        summed = (token_embeddings * mask).sum(dim=1)  # [B, 768]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)        # [B, 1]\n",
    "        mean_pooled = summed / lengths\n",
    "        return mean_pooled\n",
    "\n",
    "# from safetensors.torch import load_file\n",
    "# class ImageFeatureExtractor:\n",
    "#     '''\n",
    "#     改进版，使得timm不要每次都去连huggingface。\n",
    "#     '''\n",
    "#     def __init__(self, model_name='resnet50', device='cpu', weights_path=None, cache_dir=None):\n",
    "#         self.device = device\n",
    "#         self.model = timm.create_model(model_name, pretrained=False, num_classes=0, cache_dir=cache_dir)\n",
    "\n",
    "#         if weights_path is not None:\n",
    "#             if weights_path.endswith('.safetensors'):\n",
    "#                 state_dict = load_file(weights_path)\n",
    "#             else:\n",
    "#                 state_dict = torch.load(weights_path, map_location='cpu')\n",
    "#             self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "#         self.model = self.model.to(device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "\n",
    "#     def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "#         if not images:\n",
    "#             in_dim = getattr(self.model, 'num_features', 2048)\n",
    "#             return torch.empty((0, in_dim), dtype=torch.float32, device=self.device)\n",
    "#         tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "#         feats = self.model(tensors)\n",
    "#         return feats\n",
    "# image_extractor = ImageFeatureExtractor(\n",
    "#     device=device, \n",
    "#     weights_path=\"/mnt/d/HuggingFaceModels/models--timm--resnet50.a1_in1k/snapshots/767268603ca0cb0bfe326fa87277f19c419566ef/model.safetensors\"\n",
    "# )\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    # 类作用：使用 timm 的图像模型提取图像特征，num_classes=0 输出特征向量\n",
    "    # 参数:\n",
    "    # - model_name: timm 模型名称（默认 'resnet50'）\n",
    "    # - device: 设备（'cpu' 或 'cuda'）\n",
    "    # - cache_dir: timm 模型缓存目录\n",
    "    def __init__(self, model_name='resnet50', device='cpu', cache_dir=None):\n",
    "        self.device = device\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=True, num_classes=0,\n",
    "            cache_dir=cache_dir\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        # 函数作用：对图像列表进行预处理并前向提取特征\n",
    "        # 参数：images PIL.Image 列表\n",
    "        # 返回：形状 [B, 2048] 的图像特征（以 ResNet50 为例）\n",
    "        if not images:\n",
    "            return torch.empty((0, 2048), dtype=torch.float32, device=self.device)\n",
    "        tensors = torch.stack([self.transform(img.convert('RGB')) for img in images]).to(self.device)\n",
    "        feats = self.model(tensors)\n",
    "        return feats\n",
    "\n",
    "class FeatureFusion:\n",
    "    # 类作用：将原始文本/图像特征投影到共同的子空间（projection_dim）\n",
    "    # 参数:\n",
    "    # - fusion_method: 融合方式，当前支持 'projection'\n",
    "    # - projection_dim: 目标投影维度\n",
    "    # - device: 设备\n",
    "    # - hidden_dim: 两层 MLP 的中间隐藏层维度\n",
    "    # - dropout: Dropout 概率\n",
    "    # - text_in_dim/image_in_dim: 输入维度（默认文本768/图像2048）\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None, hidden_dim=1024, dropout=0.1, text_in_dim=768, image_in_dim=2048):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout\n",
    "        if fusion_method == 'projection':\n",
    "            # self.text_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(text_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            # self.image_projector = torch.nn.Sequential(\n",
    "            #     torch.nn.Linear(image_in_dim, hidden_dim),\n",
    "            #     torch.nn.GELU(),\n",
    "            #     torch.nn.Dropout(p=dropout),\n",
    "            #     torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            # ).to(self.device)\n",
    "            self.text_projector = OptimizedMLPProjector(text_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "            self.image_projector = OptimizedMLPProjector(image_in_dim, hidden_dim, projection_dim, dropout=dropout).to(self.device)\n",
    "            \n",
    "\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        # 将文本特征通过两层 MLP 投影到 projection_dim，并返回\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        # 将图像特征通过两层 MLP 投影到 projection_dim，并返回\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    # 类作用：提供特征归一化与相似度计算（默认余弦相似度）\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # 功能：数值稳健化（去 NaN/Inf）后做 L2 归一化\n",
    "        # 数值稳健化：去除 NaN/Inf 并在归一化中使用 eps\n",
    "        f = torch.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(f, p=2, dim=1, eps=1e-6)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        # 功能：计算文本与图像特征之间的相似度矩阵\n",
    "        # 当 similarity_type='cosine' 时，先归一化再矩阵乘法；否则直接点积\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    # 类作用：跨模态检索模型，封装提取、投影、相似度与可学习温度\n",
    "    # 参数:\n",
    "    # - text_extractor/image_extractor: 文本/图像特征提取器实例\n",
    "    # - fusion_method/projection_dim/similarity_type: 融合与相似度配置\n",
    "    # - normalize_features: 是否在融合后做归一化\n",
    "    # - device: 模型设备\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        # 动态获取输入维度，避免环境差异导致维度不符\n",
    "        text_in_dim = getattr(text_extractor.model.config, 'hidden_size', 768)\n",
    "        image_in_dim = getattr(image_extractor.model, 'num_features', 2048)\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device, text_in_dim=text_in_dim, image_in_dim=image_in_dim)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # 可学习温度：logit_scale，取值经 exp 后再取倒数作为 temperature\n",
    "        init_temp = 0.07\n",
    "        self.logit_scale = torch.nn.Parameter(torch.tensor(math.log(1.0 / init_temp), dtype=torch.float32))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 将特征数值稳健化并做 L2 归一化（可关闭）\n",
    "        if not self.normalize_features:\n",
    "            return x\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "    def current_temperature(self) -> torch.Tensor:\n",
    "        # 将可学习参数 logit_scale 转换为正温度（1/exp），并裁剪范围\n",
    "        # 将 logit_scale 转换为正的温度，并进行合理范围裁剪\n",
    "        temp = 1.0 / torch.exp(self.logit_scale)\n",
    "        return torch.clamp(temp, min=1e-3, max=10.0)\n",
    "\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        # 用文本提取器编码文本，随后通过 MLP 投影并归一化\n",
    "        # 评估阶段需禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.text_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        # 用图像提取器编码图像，随后通过 MLP 投影并归一化\n",
    "        # 评估阶段需禁用 BN/Dropout 的训练行为\n",
    "        self.fusion.image_projector.eval()\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        # 功能：批量构建图像检索索引（id -> 特征），按 batch 处理\n",
    "        # 参数：images_dict 图像字典（id -> PIL.Image）；batch_size 批大小\n",
    "        # 返回：每个图像 id 对应融合后的特征张量（在 CPU 上）\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temperature: torch.Tensor) -> torch.Tensor:\n",
    "    # 损失作用：对齐文本-图像，通过双向对比学习的 InfoNCE 损失\n",
    "    # 参数:\n",
    "    # - text_feats: 文本投影后的特征 [B, D]\n",
    "    # - image_feats: 图像投影后的特征 [B, D]\n",
    "    # - temperature: 温度（正值），用于缩放 logits 稳定训练\n",
    "    # 原理：计算 t·i^T 的相似度，分别以行/列为正样本做交叉熵，最后取平均\n",
    "    # 数值防护：去除 NaN/Inf 并在半精度下转 float32 防溢出\n",
    "    t = torch.nan_to_num(text_feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    i = torch.nan_to_num(image_feats, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    logits = torch.mm(t, i.t()).float() / temperature.float()\n",
    "    # 限幅，避免极端值导致 softmax 溢出或 NaN\n",
    "    logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    logits = torch.clamp(logits, -100.0, 100.0)\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顶层解冻与优化器分组\n",
    "- 解冻 BERT 顶层（最后2层 + pooler）与 ResNet layer4\n",
    "- 优化器包含：两层MLP投影头参数、已解冻的顶层参数、logit_scale 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_text_top_layers(text_extractor: TextFeatureExtractor, last_n_layers: int = 2):\n",
    "    \"\"\"\n",
    "    冻结 BERT 主体，仅解冻最后 N 层与 pooler，以便轻量微调上层表示。\n",
    "    参数：\n",
    "    - text_extractor: 文本特征提取器实例\n",
    "    - last_n_layers: 需要解冻的最后层数（默认2）\n",
    "    原理：设置 requires_grad=True 并将对应模块置为 train()；其余部分仍保持 eval()。\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    功能：冻结 BERT 主体参数，仅解冻最后 N 层与 pooler 以进行轻量微调。\n",
    "    参数：\n",
    "    - text_extractor: 文本特征提取器实例\n",
    "    - last_n_layers: 解冻的最后层数（默认2）\n",
    "    过程：设置 requires_grad 并将解冻层设为 train()，其余仍 eval()。\n",
    "    \"\"\"\n",
    "    for p in text_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    for i in range(total_layers - last_n_layers, total_layers):\n",
    "        for p in enc.layer[i].parameters():\n",
    "            p.requires_grad = True\n",
    "        enc.layer[i].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        for p in text_extractor.model.pooler.parameters():\n",
    "            p.requires_grad = True\n",
    "        text_extractor.model.pooler.train()\n",
    "    text_extractor.model.eval()\n",
    "\n",
    "def unfreeze_image_top_block(image_extractor: ImageFeatureExtractor, unfreeze_layer4: bool = True):\n",
    "    \"\"\"\n",
    "    冻结图像模型主体，仅解冻顶层 block（如 ResNet 的 layer4）以微调。\n",
    "    参数：\n",
    "    - image_extractor: 图像特征提取器实例\n",
    "    - unfreeze_layer4: 是否解冻 layer4（默认 True）\n",
    "    原理：设置 requires_grad=True 并将对应模块置为 train()；其余部分仍保持 eval()。\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    功能：冻结图像模型参数，仅解冻最顶层 block（如 ResNet layer4）。\n",
    "    参数：\n",
    "    - image_extractor: 图像特征提取器实例\n",
    "    - unfreeze_layer4: 是否解冻 layer4（默认 True）\n",
    "    过程：设置 requires_grad 并将解冻层设为 train()，其余仍 eval()。\n",
    "    \"\"\"\n",
    "    for p in image_extractor.model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if unfreeze_layer4 and hasattr(image_extractor.model, 'layer4'):\n",
    "        for p in image_extractor.model.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "        image_extractor.model.layer4.train()\n",
    "    image_extractor.model.eval()\n",
    "\n",
    "def build_optimizer(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                   lr_proj: float = 1e-3, lr_text_top: float = 5e-5, lr_img_top: float = 1e-4, lr_logit_scale: float = 1e-3, weight_decay: float = 1e-4):\n",
    "    \"\"\"\n",
    "    构建分组 Adam 优化器，为不同模块设置差异化学习率与权重衰减。\n",
    "    参数：\n",
    "    - model: 跨模态检索模型（含两层 MLP 与可学习温度参数 logit_scale）\n",
    "    - text_extractor/image_extractor: 提供已解冻顶层参数的提取器\n",
    "    - lr_proj/lr_text_top/lr_img_top/lr_logit_scale: 各参数组学习率\n",
    "    - weight_decay: 投影头参数的权重衰减（其他组设为 0）\n",
    "    返回：torch.optim.Adam 实例\n",
    "    原理：参数组包括：两层 MLP、文本顶层（最后两层+pooler）、图像 top block、logit_scale。\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    功能：按模块分组构建 Adam 优化器，设置不同学习率与权重衰减。\n",
    "    参数：\n",
    "    - model: 跨模态检索模型（包含两层MLP与 logit_scale）\n",
    "    - text_extractor/image_extractor: 提供顶层可训练参数的提取器\n",
    "    - lr_proj/lr_text_top/lr_img_top/lr_logit_scale: 各参数组的学习率\n",
    "    - weight_decay: 投影头参数的权重衰减\n",
    "    返回：torch.optim.Adam 实例\n",
    "    原理：将投影头、文本顶层、图像顶层、logit_scale 分别作为参数组，以便差异化优化。\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    # 两层MLP投影头\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    # 文本顶层\n",
    "    text_top_params = []\n",
    "    enc = text_extractor.model.encoder\n",
    "    for mod in enc.layer[-2:]:\n",
    "        text_top_params += list(mod.parameters())\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_top_params += list(text_extractor.model.pooler.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in text_top_params if p.requires_grad],\n",
    "        'lr': lr_text_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    # 图像顶层\n",
    "    img_top_params = []\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        img_top_params += list(image_extractor.model.layer4.parameters())\n",
    "    params.append({\n",
    "        'params': [p for p in img_top_params if p.requires_grad],\n",
    "        'lr': lr_img_top,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    # 可学习温度参数\n",
    "    params.append({\n",
    "        'params': [model.logit_scale],\n",
    "        'lr': lr_logit_scale,\n",
    "        'weight_decay': 0.0\n",
    "    })\n",
    "    optimizer = torch.optim.AdamW(params)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:10:07,009 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-08 17:10:07,011 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:01, 222502.29it/s]\n",
      "2025-11-08 17:10:08,198 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-08 17:10:08,209 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 302466.30it/s]\n",
      "2025-11-08 17:10:08,231 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# 训练与流式参数（默认较小，确保顺利运行；可按需增大）\n",
    "train_image_batch_size = 500\n",
    "max_train_batches = 1\n",
    "epochs_per_batch = 1\n",
    "train_step_batch_size = 32\n",
    "valid_imgs_max_samples = 100\n",
    "\n",
    "# # 训练与流式参数（按需调整）：实际用\n",
    "# train_image_batch_size = 15000 ## 一个大batch有这么多图片样本。\n",
    "# max_train_batches = 10 ## 总共加载多少个大batch。\n",
    "# epochs_per_batch = 10 ## 每个大batch训练几个epoch。\n",
    "# train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "# valid_imgs_max_samples = 30000\n",
    "\n",
    "use_amp = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型与优化器，并进行顶层解冻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:10:08,368 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2025-11-08 17:10:08,949 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_841876/4183273530.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "unfreeze_text_top_layers(text_extractor, last_n_layers=2)\n",
    "unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "\n",
    "optim = build_optimizer(model, text_extractor, image_extractor,\n",
    "                        lr_proj=1e-3, lr_text_top=5e-5, lr_img_top=1e-4, lr_logit_scale=1e-3, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环（流式）\n",
    "- 构建 (query, image) 配对\n",
    "- AMP 加速与梯度裁剪\n",
    "- 使用可学习温度 model.current_temperature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:10:11,546 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:   0%|                                                     | 260/129380 [00:00<00:50, 2579.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images=500, usable_pairs=997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_841876/100774040.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "实际加载train图片数据:   0%|▏                                                      | 499/129380 [00:06<29:13, 73.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: avg loss=2.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    \"\"\"\n",
    "    功能：从训练 DataFrame 按 query_text 和 item_ids 选择第一张可用图片，形成 (文本, 图像, 图像id) 三元组列表。\n",
    "    参数：\n",
    "    - train_df: 包含列 'query_text' 与 'item_ids' 的 DataFrame，其中 'item_ids' 为候选图片 id 列表\n",
    "    - img_dict: 图片字典（id -> PIL.Image），不可用或缺失的 id 映射为 None\n",
    "    返回：List[Tuple[str, Image.Image, str]]\n",
    "    原理：遍历每行，从 item_ids 中挑第一张在 img_dict 中存在的图片，作为该 query 的正样本。\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    \"\"\"\n",
    "    功能：对一个 (文本, 图像) 对列表进行多轮小批训练。\n",
    "    参数：\n",
    "    - pairs: 训练三元组列表 (text, image, image_id)\n",
    "    - epochs: 在该批数据上迭代的轮数\n",
    "    - step_bs: 每个优化步的小批大小\n",
    "    过程：\n",
    "    - 训练模式开启指定的投影头与顶层模块\n",
    "    - 每个小批：编码文本与图像 -> 通过两层 MLP 投影 -> 计算 InfoNCE 损失\n",
    "    - AMP 分支使用 GradScaler 缩放与步进；非 AMP 分支直接 backward + step\n",
    "    - 对投影头参数进行梯度裁剪（max_norm=5.0）以提升稳定性\n",
    "    \"\"\"\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "    text_extractor.model.encoder.layer[-1].train()\n",
    "    text_extractor.model.encoder.layer[-2].train()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.train()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            optim.zero_grad()\n",
    "            temp = model.current_temperature()\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    loss = info_nce_loss(t_proj, i_proj, temperature=temp)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                loss = info_nce_loss(t_proj, i_proj, temperature=temp)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                optim.step()\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载检查点\n",
    "保存两层MLP投影头、解冻顶层与优化器，并记录 logit_scale（可学习温度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_2_3_4_mlp_temp_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_2_3_4_mlp_temp_checkpoint.pth')\n",
    "\n",
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        'logit_scale': model.logit_scale.detach().cpu(),\n",
    "        'text_unfrozen': {},\n",
    "        'image_unfrozen': {},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    enc = text_extractor.model.encoder\n",
    "    total_layers = len(enc.layer)\n",
    "    start_idx = max(0, total_layers - last_n_layers)\n",
    "    for i in range(start_idx, total_layers):\n",
    "        ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    if hasattr(image_extractor.model, 'layer4'):\n",
    "        ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    if 'logit_scale' in ckpt:\n",
    "        model.logit_scale.data = ckpt['logit_scale'].to(model.logit_scale.device)\n",
    "    ln = ckpt.get('last_n_layers', 2)\n",
    "    unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    enc = text_extractor.model.encoder\n",
    "    for k, v in ckpt['text_unfrozen'].items():\n",
    "        if k.startswith('encoder_layer_'):\n",
    "            idx = int(k.split('_')[-1])\n",
    "            if 0 <= idx < len(enc.layer):\n",
    "                enc.layer[idx].load_state_dict(v)\n",
    "    if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "        text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "        image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    if optimizer is not None and 'optimizer' in ckpt:\n",
    "        optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 保存一次并测试加载\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, 2)\n",
    "# load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "优先使用 FAISS；不可用则回退到 Torch 相似度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:10:38,681 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据:  99%|████████████████████████████████████████████████████████▍| 99/100 [00:00<00:00, 2207.32it/s]\n",
      "2025-11-08 17:10:43,482 - INFO - 成功创建valid图片映射字典，共100张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:36<00:00, 137.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0020, Recall@5=0.0074, Recall@10=0.0104, MeanRecall=0.0066 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=valid_imgs_max_samples, \n",
    "    batch_size=3000, max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
