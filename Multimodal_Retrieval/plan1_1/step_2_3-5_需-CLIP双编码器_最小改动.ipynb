{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2.3-3：CLIP 双编码器（需）\n",
    "\n",
    "在 `step_2_3-3_屯-mean_pooling.ipynb` 的结构上做最小改动：\n",
    "- 使用 CLIP (`openai/clip-vit-base-patch32`) 同时作为文本与图像编码器\n",
    "- 保持两层投影头、InfoNCE 损失、AMP 与训练循环不变（除编码器来源外）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import math\n",
    "# from typing import List, Dict, Tuple\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# # 可选：Faiss 加速检索（安全导入，避免 NumPy 版本问题导致报错中断）\n",
    "# HAS_FAISS = False\n",
    "# faiss = None\n",
    "# try:\n",
    "#     spec = importlib.util.find_spec('faiss')\n",
    "#     if spec is not None:\n",
    "#         faiss = importlib.import_module('faiss')\n",
    "#         HAS_FAISS = True\n",
    "# except BaseException as e:\n",
    "#     print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "#     HAS_FAISS = False\n",
    "\n",
    "\n",
    "# # 环境变量与缓存\n",
    "# cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "# os.environ['TORCH_HOME'] = cache_dir\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "# os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "# os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# # 导入数据加载器\n",
    "# sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "# from data_loader import DataLoader\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# clip_model_name = 'openai/clip-vit-base-patch32'\n",
    "# print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/xiuminke/miniconda3/envs/ml12/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import importlib\n",
    "\n",
    "# 可选：Faiss 加速检索（安全导入，避免 NumPy 版本问题导致报错中断）\n",
    "HAS_FAISS = False\n",
    "faiss = None\n",
    "try:\n",
    "    spec = importlib.util.find_spec('faiss')\n",
    "    if spec is not None:\n",
    "        faiss = importlib.import_module('faiss')\n",
    "        HAS_FAISS = True\n",
    "except BaseException as e:\n",
    "    print(f'Faiss unavailable: {e.__class__.__name__}')\n",
    "    HAS_FAISS = False\n",
    "\n",
    "# 设置环境变量（与基线一致，按需修改为本地镜像/缓存）\n",
    "cache_dir = \"/mnt/d/HuggingFaceModels/\"\n",
    "os.environ['TORCH_HOME'] = cache_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\"\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 导入数据加载器（使用 plan1_1/data_loader.py）\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'Multimodal_Retrieval', 'plan1_1')))\n",
    "from data_loader import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "clip_model_name = 'openai/clip-vit-base-patch32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型与特征模块\n",
    "保持原有类名与结构，内部实现改为使用 CLIP 的 `get_text_features` 与 `get_image_features`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, device='cpu', cache_dir=None, clip_model_name='openai/clip-vit-base-patch32', clip_model=None, clip_processor=None):\n",
    "        self.device = device\n",
    "        self.processor = clip_processor or CLIPProcessor.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = clip_model or CLIPModel.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        # 输出维度（CLIP 投影维度）\n",
    "        self.out_dim = getattr(self.model.config, 'projection_dim', 512)\n",
    "\n",
    "    def encode_with_grad(self, texts: List[str]) -> torch.Tensor:\n",
    "        if not texts:\n",
    "            return torch.empty((0, self.out_dim), dtype=torch.float32, device=self.device)\n",
    "        inputs = self.processor(text=texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        feats = self.model.get_text_features(**inputs)  # [B, D]\n",
    "        return feats\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, device='cpu', cache_dir=None, clip_model_name='openai/clip-vit-base-patch32', clip_model=None, clip_processor=None):\n",
    "        self.device = device\n",
    "        self.processor = clip_processor or CLIPProcessor.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.model = clip_model or CLIPModel.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.out_dim = getattr(self.model.config, 'projection_dim', 512)\n",
    "\n",
    "    def encode_with_grad(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        if not images:\n",
    "            return torch.empty((0, self.out_dim), dtype=torch.float32, device=self.device)\n",
    "        inputs = self.processor(images=images, return_tensors='pt')\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        feats = self.model.get_image_features(**inputs)  # [B, D]\n",
    "        return feats\n",
    "\n",
    "class FeatureFusion:\n",
    "    def __init__(self, fusion_method='projection', projection_dim=512, device=None, hidden_dim=1024, dropout=0.1, text_in_dim=512, image_in_dim=512):\n",
    "        self.fusion_method = fusion_method\n",
    "        self.projection_dim = projection_dim\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout\n",
    "        if fusion_method == 'projection':\n",
    "            self.text_projector = torch.nn.Sequential(\n",
    "                torch.nn.Linear(text_in_dim, hidden_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Dropout(p=dropout),\n",
    "                torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            ).to(self.device)\n",
    "            self.image_projector = torch.nn.Sequential(\n",
    "                torch.nn.Linear(image_in_dim, hidden_dim),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Dropout(p=dropout),\n",
    "                torch.nn.Linear(hidden_dim, projection_dim)\n",
    "            ).to(self.device)\n",
    "            # self.text_projector = torch.nn.Linear(text_in_dim, projection_dim).to(self.device)\n",
    "            # self.image_projector = torch.nn.Linear(image_in_dim, projection_dim).to(self.device)\n",
    "\n",
    "    def fuse_text_features(self, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_projector(text_features) if self.fusion_method == 'projection' else text_features\n",
    "\n",
    "    def fuse_image_features(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_projector(image_features) if self.fusion_method == 'projection' else image_features\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, similarity_type='cosine'):\n",
    "        self.similarity_type = similarity_type\n",
    "    def normalize_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        f = torch.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(f, p=2, dim=1, eps=1e-6)\n",
    "    def calculate_similarity(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.similarity_type == 'cosine':\n",
    "            t_n = self.normalize_features(text_features)\n",
    "            i_n = self.normalize_features(image_features)\n",
    "            return torch.mm(t_n, i_n.t())\n",
    "        return torch.mm(text_features, image_features.t())\n",
    "\n",
    "class CrossModalRetrievalModel:\n",
    "    def __init__(self, text_extractor, image_extractor, fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=None):\n",
    "        self.text_extractor = text_extractor\n",
    "        self.image_extractor = image_extractor\n",
    "        text_in_dim = getattr(text_extractor, 'out_dim', 512)\n",
    "        image_in_dim = getattr(image_extractor, 'out_dim', 512)\n",
    "        self.fusion = FeatureFusion(fusion_method, projection_dim, device, text_in_dim=text_in_dim, image_in_dim=image_in_dim)\n",
    "        self.sim = SimilarityCalculator(similarity_type)\n",
    "        self.normalize_features = normalize_features\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.normalize_features:\n",
    "            return x\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return torch.nn.functional.normalize(x, p=2, dim=1, eps=1e-6)\n",
    "    def extract_and_fuse_text_features(self, texts: List[str]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            t = self.text_extractor.encode_with_grad(texts)\n",
    "        return self._norm(self.fusion.fuse_text_features(t))\n",
    "    def extract_and_fuse_image_features(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            i = self.image_extractor.encode_with_grad(images)\n",
    "        return self._norm(self.fusion.fuse_image_features(i))\n",
    "    def build_image_index(self, images_dict: Dict[str, Image.Image], batch_size: int = 32) -> Dict[str, torch.Tensor]:\n",
    "        feats = {}\n",
    "        keys = list(images_dict.keys())\n",
    "        for s in range(0, len(keys), batch_size):\n",
    "            batch_ids = keys[s:s+batch_size]\n",
    "            batch_imgs = [images_dict[k] for k in batch_ids if images_dict[k] is not None]\n",
    "            valid_ids = [k for k in batch_ids if images_dict[k] is not None]\n",
    "            if not batch_imgs:\n",
    "                continue\n",
    "            bf = self.extract_and_fuse_image_features(batch_imgs)\n",
    "            for j, img_id in enumerate(valid_ids):\n",
    "                feats[img_id] = bf[j].detach().cpu()\n",
    "        return feats\n",
    "\n",
    "def info_nce_loss(text_feats: torch.Tensor, image_feats: torch.Tensor, temp: float) -> torch.Tensor:\n",
    "    logits = torch.mm(text_feats, image_feats.t()).float() / float(temp)\n",
    "    logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "    logits = torch.clamp(logits, -100.0, 100.0)\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_t + loss_i) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器（最小改动）\n",
    "仅优化两层 MLP 投影头参数，保持与原 notebook 相同的接口与结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model: CrossModalRetrievalModel, lr_proj: float = 1e-3, weight_decay: float = 1e-4):\n",
    "    params = []\n",
    "    params.append({\n",
    "        'params': list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "        'lr': lr_proj,\n",
    "        'weight_decay': weight_decay\n",
    "    })\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与训练参数\n",
    "保持与原 notebook 一致的流式图片加载与训练参数接口。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:30:20,099 - INFO - 初始化数据加载器，数据目录: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval\n",
      "2025-11-09 16:30:20,100 - INFO - 加载train查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_queries.jsonl\n",
      "加载train查询数据: 248786it [00:01, 221168.70it/s]\n",
      "2025-11-09 16:30:21,297 - INFO - 成功加载train查询数据，共248786条\n",
      "2025-11-09 16:30:21,307 - INFO - 加载valid查询数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_queries.jsonl\n",
      "加载valid查询数据: 5008it [00:00, 213757.30it/s]\n",
      "2025-11-09 16:30:21,335 - INFO - 成功加载valid查询数据，共5008条\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "train_df = loader.load_queries(split='train')\n",
    "valid_df = loader.load_queries(split='valid')\n",
    "\n",
    "# 训练与流式参数（默认较小，确保顺利运行；可按需增大）\n",
    "train_image_batch_size = 500\n",
    "max_train_batches = 1\n",
    "epochs_per_batch = 1\n",
    "train_step_batch_size = 32\n",
    "valid_imgs_max_samples = 100\n",
    "\n",
    "# # 训练与流式参数（按需调整）：实际用\n",
    "# train_image_batch_size = 15000 ## 一个大batch有这么多图片样本。\n",
    "# max_train_batches = 10 ## 总共加载多少个大batch。\n",
    "# epochs_per_batch = 5 ## 每个大batch训练几个epoch。\n",
    "# train_step_batch_size = 32 ## 每个大batch里面训练的时候的小batch_size是多少。\n",
    "# valid_imgs_max_samples = 30000\n",
    "\n",
    "use_amp = True\n",
    "temperature = 0.07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optim groups: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26205/2543662652.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n"
     ]
    }
   ],
   "source": [
    "# 共享一个 CLIP 模型与处理器，减少显存与加载开销\n",
    "shared_processor = CLIPProcessor.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True)\n",
    "shared_model = CLIPModel.from_pretrained(clip_model_name, cache_dir=cache_dir, local_files_only=True).to(device)\n",
    "text_extractor = TextFeatureExtractor(device=device, cache_dir=cache_dir, clip_model_name=clip_model_name, clip_model=shared_model, clip_processor=shared_processor)\n",
    "image_extractor = ImageFeatureExtractor(device=device, cache_dir=cache_dir, clip_model_name=clip_model_name, clip_model=shared_model, clip_processor=shared_processor)\n",
    "model = CrossModalRetrievalModel(\n",
    "    text_extractor, image_extractor, \n",
    "    fusion_method='projection', projection_dim=512, similarity_type='cosine', normalize_features=True, device=device\n",
    ")\n",
    "\n",
    "optim = build_optimizer(model, lr_proj=1e-3, weight_decay=1e-4)\n",
    "scaler = GradScaler(enabled=(device.type == 'cuda' and use_amp))\n",
    "print('Optim groups:', len(optim.param_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环（流式）\n",
    "结构与原 notebook 保持一致：构建 (query, image) 配对，AMP 与裁剪保持。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:30:28,672 - INFO - 批量加载train图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_train_imgs.tsv\n",
      "实际加载train图片数据:   0%|▏                                                    | 488/129380 [00:00<00:57, 2248.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images=500, usable_pairs=997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26205/3346926532.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "实际加载train图片数据:   0%|▏                                                      | 499/129380 [00:07<33:41, 63.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: avg loss=3.4173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_batch_pairs(train_df, img_dict: Dict[str, Image.Image]) -> List[Tuple[str, Image.Image, str]]:\n",
    "    pairs = []\n",
    "    if 'item_ids' in train_df.columns:\n",
    "        for _, row in train_df.iterrows():\n",
    "            q = row.get('query_text', None)\n",
    "            ids = row.get('item_ids', [])\n",
    "            if not q or not isinstance(ids, list) or not ids:\n",
    "                continue\n",
    "            chosen_img = None\n",
    "            chosen_id = None\n",
    "            for iid in ids:\n",
    "                sid = str(iid)\n",
    "                if sid in img_dict and img_dict[sid] is not None:\n",
    "                    chosen_img = img_dict[sid]\n",
    "                    chosen_id = sid\n",
    "                    break\n",
    "            if chosen_img is not None:\n",
    "                pairs.append((q, chosen_img, chosen_id))\n",
    "    return pairs\n",
    "\n",
    "def train_one_batch(pairs: List[Tuple[str, Image.Image, str]], epochs: int, step_bs: int):\n",
    "    model.fusion.text_projector.train()\n",
    "    model.fusion.image_projector.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for s in range(0, len(pairs), step_bs):\n",
    "            batch = pairs[s:s+step_bs]\n",
    "            if not batch:\n",
    "                continue\n",
    "            texts = [t for (t, _, _) in batch]\n",
    "            imgs = [im for (_, im, _) in batch]\n",
    "\n",
    "            optim.zero_grad()\n",
    "            temp = temperature\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                with autocast(enabled=True):\n",
    "                    t_feats = text_extractor.encode_with_grad(texts)\n",
    "                    i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                    t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                    i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                    loss = info_nce_loss(t_proj, i_proj, temp)\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                t_feats = text_extractor.encode_with_grad(texts)\n",
    "                i_feats = image_extractor.encode_with_grad(imgs)\n",
    "                t_proj = model._norm(model.fusion.fuse_text_features(t_feats))\n",
    "                i_proj = model._norm(model.fusion.fuse_image_features(i_feats))\n",
    "                loss = info_nce_loss(t_proj, i_proj, temp)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.fusion.text_projector.parameters()) + list(model.fusion.image_projector.parameters()),\n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                optim.step()\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "        print(f\"Epoch {e+1}/{epochs}: avg loss={running_loss/max(steps,1):.4f}\")\n",
    "\n",
    "# 流式加载与训练\n",
    "batch_idx = 0\n",
    "for image_batch in loader.load_images_batch(split='train', batch_size=train_image_batch_size, max_batches=max_train_batches):\n",
    "    batch_idx += 1\n",
    "    img_map = {item['img_id']: item['image'] for item in image_batch}\n",
    "    pairs = build_batch_pairs(train_df, img_map)\n",
    "    print(f\"Batch {batch_idx}: images={len(img_map)}, usable_pairs={len(pairs)}\")\n",
    "    if not pairs:\n",
    "        del img_map\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "    train_one_batch(pairs, epochs=epochs_per_batch, step_bs=train_step_batch_size)\n",
    "    del img_map\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存：投影层 + 已解冻顶层 + 优化器\n",
    "保存 BERT 的最后2层 + pooler、ResNet50 的 layer4、投影层、优化器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to: /mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights/step_2_3_5.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/mnt/d/forCoding_data/Tianchi_MUGE/trained_models/weights'\n",
    "save_path = os.path.join(save_dir, 'step_2_3_5.pth')\n",
    "\n",
    "def save_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, save_path: str, last_n_layers: int):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        'projection_dim': model.fusion.projection_dim,\n",
    "        # 'last_n_layers': last_n_layers,\n",
    "        'fusion': {\n",
    "            'text_projector': model.fusion.text_projector.state_dict(),\n",
    "            'image_projector': model.fusion.image_projector.state_dict(),\n",
    "        },\n",
    "        # 'text_unfrozen': {},\n",
    "        # 'image_unfrozen': {},\n",
    "        # 'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    # enc = text_extractor.model.encoder\n",
    "    # total_layers = len(enc.layer)\n",
    "    # start_idx = max(0, total_layers - last_n_layers)\n",
    "    # for i in range(start_idx, total_layers):\n",
    "    #     ckpt['text_unfrozen'][f'encoder_layer_{i}'] = enc.layer[i].state_dict()\n",
    "    # if hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "    #     ckpt['text_unfrozen']['pooler'] = text_extractor.model.pooler.state_dict()\n",
    "    # if hasattr(image_extractor.model, 'layer4'):\n",
    "    #     ckpt['image_unfrozen']['layer4'] = image_extractor.model.layer4.state_dict()\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(f\"Checkpoint saved to: {save_path}\")\n",
    "\n",
    "# 保存一次\n",
    "save_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载：恢复解冻顶层与投影层权重，继续训练\n",
    "加载后会自动再次执行顶层解冻，并恢复优化器状态（如提供）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unfreeze_checkpoint(model: CrossModalRetrievalModel, text_extractor: TextFeatureExtractor, image_extractor: ImageFeatureExtractor,\n",
    "                             optimizer: torch.optim.Optimizer, load_path: str):\n",
    "    ckpt = torch.load(load_path, map_location='cpu')\n",
    "    model.fusion.text_projector.load_state_dict(ckpt['fusion']['text_projector'])\n",
    "    model.fusion.image_projector.load_state_dict(ckpt['fusion']['image_projector'])\n",
    "    # ln = ckpt.get('last_n_layers', 2)\n",
    "    # unfreeze_text_top_layers(text_extractor, last_n_layers=ln)\n",
    "    # unfreeze_image_top_block(image_extractor, unfreeze_layer4=True)\n",
    "    # enc = text_extractor.model.encoder\n",
    "    # for k, v in ckpt['text_unfrozen'].items():\n",
    "    #     if k.startswith('encoder_layer_'):\n",
    "    #         idx = int(k.split('_')[-1])\n",
    "    #         if 0 <= idx < len(enc.layer):\n",
    "    #             enc.layer[idx].load_state_dict(v)\n",
    "    # if 'pooler' in ckpt['text_unfrozen'] and hasattr(text_extractor.model, 'pooler') and text_extractor.model.pooler is not None:\n",
    "    #     text_extractor.model.pooler.load_state_dict(ckpt['text_unfrozen']['pooler'])\n",
    "    # if 'layer4' in ckpt['image_unfrozen'] and hasattr(image_extractor.model, 'layer4'):\n",
    "    #     image_extractor.model.layer4.load_state_dict(ckpt['image_unfrozen']['layer4'])\n",
    "    # if optimizer is not None and 'optimizer' in ckpt:\n",
    "    #     optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    # print(f\"Checkpoint loaded from: {load_path}\")\n",
    "\n",
    "# 测试加载\n",
    "load_unfreeze_checkpoint(model, text_extractor, image_extractor, optim, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证评估：Recall@1/5/10 与 MeanRecall\n",
    "- 基于验证集构建图像索引\n",
    "- 对每条查询计算相似度并统计召回（优先使用 FAISS；不可用则 Torch 回退）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_imgs = loader.create_img_id_to_image_dict(\n",
    "#     split='valid', \n",
    "#     max_samples=valid_imgs_max_samples,\n",
    "#     # batch_size=3000,\n",
    "#     # max_batches=10\n",
    "# )\n",
    "\n",
    "# valid_queries = []\n",
    "# if 'item_ids' in valid_df.columns:\n",
    "#     for _, row in valid_df.iterrows():\n",
    "#         q = row.get('query_text', None)\n",
    "#         ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "#         if q and ids:\n",
    "#             valid_queries.append((q, ids))\n",
    "# print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "# image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "# all_image_ids = list(image_index.keys())\n",
    "# all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "# faiss_index = None\n",
    "# if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "#     d = all_image_feats.size(1)\n",
    "#     faiss_index = faiss.IndexFlatIP(d)\n",
    "#     feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "#     faiss_index.add(feats_np)\n",
    "\n",
    "# all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "# def compute_recall_at_k(k_values, queries):\n",
    "#     recalls = {k: 0 for k in k_values}\n",
    "#     total = 0\n",
    "#     for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "#         if all_image_feats.size(0) == 0:\n",
    "#             continue\n",
    "#         q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "#         if faiss_index is not None:\n",
    "#             q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "#             _, I = faiss_index.search(q_np, max(k_values))\n",
    "#             top_idx = I[0].tolist()\n",
    "#             top_ids = [all_image_ids[i] for i in top_idx]\n",
    "#         else:\n",
    "#             sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "#             _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "#             top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "#         total += 1\n",
    "#         for k in k_values:\n",
    "#             if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "#                 recalls[k] += 1\n",
    "#     return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "# rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "# mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "# print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:30:55,470 - INFO - 批量加载valid图片数据: /mnt/d/forCoding_data/Tianchi_MUGE/originalData/Multimodal_Retrieval/MR_valid_imgs.tsv\n",
      "实际加载valid图片数据:  99%|████████████████████████████████████████████████████████▍| 99/100 [00:00<00:00, 2396.00it/s]\n",
      "2025-11-09 16:31:00,020 - INFO - 成功创建valid图片映射字典，共100张图片\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable valid queries: 5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|████████████████████████████████████████████████████████████████████| 5008/5008 [00:32<00:00, 152.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1=0.0000, Recall@5=0.0008, Recall@10=0.0020, MeanRecall=0.0009 (N=5008)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_imgs = loader.create_img_id_to_image_dict(\n",
    "    split='valid', \n",
    "    max_samples=valid_imgs_max_samples,\n",
    "    # batch_size=3000,\n",
    "    # max_batches=10\n",
    ")\n",
    "\n",
    "valid_queries = []\n",
    "if 'item_ids' in valid_df.columns:\n",
    "    for _, row in valid_df.iterrows():\n",
    "        q = row.get('query_text', None)\n",
    "        ids = [str(i) for i in row.get('item_ids', [])] if isinstance(row.get('item_ids', []), list) else []\n",
    "        if q and ids:\n",
    "            valid_queries.append((q, ids))\n",
    "print(f'Usable valid queries: {len(valid_queries)}')\n",
    "\n",
    "image_index = model.build_image_index(valid_imgs, batch_size=32)\n",
    "all_image_ids = list(image_index.keys())\n",
    "all_image_feats = torch.stack([image_index[i] for i in all_image_ids]) if all_image_ids else torch.empty((0, 512))\n",
    "faiss_index = None\n",
    "if HAS_FAISS and all_image_feats.size(0) > 0:\n",
    "    d = all_image_feats.size(1)\n",
    "    faiss_index = faiss.IndexFlatIP(d)\n",
    "    feats_np = all_image_feats.detach().cpu().numpy().astype('float32')\n",
    "    faiss_index.add(feats_np)\n",
    "\n",
    "all_image_feats = all_image_feats.to(device)\n",
    "\n",
    "def compute_recall_at_k(k_values, queries):\n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    total = 0\n",
    "    for q_text, gt_ids in tqdm(queries, desc='Evaluate'):\n",
    "        if all_image_feats.size(0) == 0:\n",
    "            continue\n",
    "        q_feat = model.extract_and_fuse_text_features([q_text])\n",
    "        if faiss_index is not None:\n",
    "            q_np = q_feat.detach().cpu().numpy().astype('float32')\n",
    "            _, I = faiss_index.search(q_np, max(k_values))\n",
    "            top_idx = I[0].tolist()\n",
    "            top_ids = [all_image_ids[i] for i in top_idx]\n",
    "        else:\n",
    "            sims = model.sim.calculate_similarity(q_feat, all_image_feats)\n",
    "            _, top_idx = torch.topk(sims[0], k=max(k_values))\n",
    "            top_ids = [all_image_ids[i] for i in top_idx.tolist()]\n",
    "        total += 1\n",
    "        for k in k_values:\n",
    "            if any(g in set(top_ids[:k]) for g in gt_ids):\n",
    "                recalls[k] += 1\n",
    "    return {k: (recalls[k] / total if total > 0 else 0.0) for k in k_values}, total\n",
    "\n",
    "rec, total_q = compute_recall_at_k([1,5,10], valid_queries)\n",
    "mean_recall = (rec.get(1,0)+rec.get(5,0)+rec.get(10,0))/3 if total_q>0 else 0.0\n",
    "print(f'Recall@1={rec.get(1,0):.4f}, Recall@5={rec.get(5,0):.4f}, Recall@10={rec.get(10,0):.4f}, MeanRecall={mean_recall:.4f} (N={total_q})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"step_2_3-5_需-CLIP双编码器_最小改动.finishflag\", \"w\") as f:\n",
    "    f.write(\"finish\")\n",
    "\n",
    "import IPython\n",
    "def kill_current_kernel():\n",
    "    '''杀死当前的kernel释放内存空间。'''\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "kill_current_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
